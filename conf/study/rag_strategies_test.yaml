# RAG Strategies Test - Focused Ablation Study
# 
# Run: uv run ragicamp run conf/study/rag_strategies_test.yaml
#
# Purpose: Systematic ablation study of RAG components:
#   - Retriever types: Dense, Hybrid, Hierarchical
#   - Query transforms: None, HyDE, Multi-query  
#   - Reranking: With and without cross-encoder
#   - Top-k variations: 3, 5, 10
#
# Design principle: Fix variables, vary one at a time for clear insights.
# Focus on BGE (SOTA) for retrievers, test multiple top_k and advanced features.
#
# Hardware: Any GPU with 8GB+ VRAM (SLMs fit easily)

name: rag_strategies_test
description: "Systematic ablation study of RAG strategies with SLMs"

# 100 examples for fast iteration
num_questions: 100

# Test on 2 diverse datasets
datasets:
  - nq          # Natural Questions (Wikipedia-based, open-domain)
  #- hotpotqa    # Multi-hop reasoning

# ============================================================================
# SLM Models - Use ONE model to reduce experiment matrix
# ============================================================================
# NOTE: For initial testing, use just Llama 3.2 3B (best SLM).
# Expand to other models after validating best RAG strategies.
models: &slm_models
  - hf:meta-llama/Llama-3.2-3B-Instruct  # Best SLM

# Uncomment for full model comparison after finding best strategies:
# models_full: &slm_models_full
  - hf:google/gemma-2b-it
#   - hf:Qwen/Qwen2.5-1.5B-Instruct
#   - hf:meta-llama/Llama-3.2-3B-Instruct

# ============================================================================
# DirectLLM Baseline (for comparison)
# ============================================================================
direct:
  enabled: true
  models: *slm_models
  prompts:
    - concise
  quantization:
    - none

# ============================================================================
# RAG Experiments - Systematic Ablation
# ============================================================================
rag:
  enabled: true
  models: *slm_models

  # --------------------------------------------------------------------------
  # Corpus Configuration (Wikipedia)
  # --------------------------------------------------------------------------
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.en
    max_docs: 150000

  # --------------------------------------------------------------------------
  # Retriever Configurations
  # 
  # Strategy: Focus on BGE (SOTA), test retrieval variations
  # --------------------------------------------------------------------------
  retrievers:
    # --- Core retriever: Dense BGE (baseline) ---
    - type: dense
      name: dense_bge_large
      embedding_model: BAAI/bge-large-en-v1.5
      chunk_size: 512
      chunk_overlap: 50

    # --- Hybrid: BM25 + Dense (tests keyword matching value) ---
    - type: hybrid
      name: hybrid_bge
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.5  # Equal weight
      chunk_size: 512
      chunk_overlap: 50

    # --- Hierarchical: Parent-child chunks (tests context vs precision) ---
    - type: hierarchical
      name: hierarchical_bge_3072_768
      embedding_model: BAAI/bge-large-en-v1.5
      parent_chunk_size: 3072  # Return large context
      child_chunk_size: 768    # Search precise chunks
      parent_overlap: 300
      child_overlap: 150

  # --------------------------------------------------------------------------
  # Query Transformation Strategies
  # 
  # NOTE: These add latency (LLM call before retrieval). Test to see if
  # the retrieval improvement justifies the cost.
  # --------------------------------------------------------------------------
  query_transform:
    - none        # Baseline: no transformation
    - hyde        # HyDE: LLM generates hypothetical answer
    - multiquery  # Multi-query: LLM rewrites query 3 ways

  # --------------------------------------------------------------------------
  # Reranking - Tests two-stage retrieval
  # --------------------------------------------------------------------------
  reranker:
    configs:
      - enabled: false
        name: no_rerank
      - enabled: true
        name: rerank_bge
        model: bge  # BAAI/bge-reranker-large (best quality)
      - enabled: true
        name: rerank_ms_marco
        model: ms-marco  # cross-encoder/ms-marco-MiniLM-L-6-v2 (faster)

  # --------------------------------------------------------------------------
  # Top-k Values - Key ablation parameter
  # 
  # More context = more info but also more noise for the LLM
  # --------------------------------------------------------------------------
  top_k_values:
    - 3   # Minimal context (less noise)
    - 5   # Standard
    - 10  # More context (test if SLMs can handle it)

  prompts:
    - concise
    - fewshot_3

  quantization:
    - none

# ============================================================================
# Metrics
# ============================================================================
metrics:
  - f1
  - exact_match
  - bertscore
  - llm_judge

# LLM Judge configuration
llm_judge:
  model: openai:gpt-4o-mini
  judgment_type: binary

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/rag_strategies_test

# ============================================================================
# Execution options
# ============================================================================
batch_size: 64
min_batch_size: 1

options:
  save_intermediate: true
  skip_existing: true
  build_index_if_missing: true
  cache_embeddings: true
