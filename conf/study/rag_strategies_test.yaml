# RAG Strategies Test
# Fast iteration testing for new RAG capabilities
#
# Run: uv run ragicamp run conf/study/rag_strategies_test.yaml
#
# Purpose: Test different RAG strategies with minimal compute:
#   - Retriever types: Dense, Hybrid, Hierarchical
#   - Query transforms: None, HyDE, Multi-query  
#   - Reranking: With and without cross-encoder
#   - Chunking: Recursive, Semantic
#
# Uses SLMs (1-3B) for fast iteration and Wikipedia corpus for retrieval
#
# Hardware: Any GPU with 8GB+ VRAM (SLMs fit easily)

name: rag_strategies_test
description: "Test RAG strategies (indexers, retrievers, chunking) with SLMs"

# Quick iteration: 100 examples per dataset
num_questions: 100

# Test on 2 diverse datasets
datasets:
  - nq          # Natural Questions (Wikipedia-based, open-domain)
  - hotpotqa    # Multi-hop reasoning

# ============================================================================
# SLM Models (Small, fast, fits on any GPU)
# ============================================================================
models: &slm_models
  - hf:google/gemma-2b-it           # Gemma 2B - fast baseline
  - hf:Qwen/Qwen2.5-1.5B-Instruct   # Qwen 1.5B - very fast
  - hf:meta-llama/Llama-3.2-3B-Instruct  # Llama 3.2 3B - best SLM

# ============================================================================
# DirectLLM Baseline (for comparison)
# ============================================================================
direct:
  enabled: true
  models: *slm_models
  prompts:
    - concise
  quantization:
    - none  # SLMs fit without quantization

# ============================================================================
# RAG Experiments - Testing all new strategies
# ============================================================================
rag:
  enabled: true
  models: *slm_models

  # --------------------------------------------------------------------------
  # Corpus Configuration (Wikipedia)
  # --------------------------------------------------------------------------
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.simple  # Simple English Wikipedia (~200k articles)
    max_docs: 50000           # Limit for faster indexing

  # --------------------------------------------------------------------------
  # Retriever Configurations - Test all types
  # --------------------------------------------------------------------------
  retrievers:
    # --- Dense Retrievers (SOTA embedding models) ---
    - type: dense
      name: dense_bge_large
      embedding_model: BAAI/bge-large-en-v1.5
      chunk_size: 512
      chunk_overlap: 50

    - type: dense
      name: dense_e5_large
      embedding_model: intfloat/e5-large-v2
      chunk_size: 512
      chunk_overlap: 50

    - type: dense
      name: dense_gte_large
      embedding_model: thenlper/gte-large
      chunk_size: 512
      chunk_overlap: 50

    # --- Hybrid Retriever (BM25 + Dense) ---
    - type: hybrid
      name: hybrid_bge
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.5  # Equal weight BM25 and dense
      chunk_size: 512
      chunk_overlap: 50

    - type: hybrid
      name: hybrid_bge_dense_heavy
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.7  # More weight on dense
      chunk_size: 512
      chunk_overlap: 50

    # --- Hierarchical Retriever (parent-child chunks) ---
    - type: hierarchical
      name: hierarchical_bge_1024_256
      embedding_model: BAAI/bge-large-en-v1.5
      parent_chunk_size: 1024
      child_chunk_size: 256
      parent_overlap: 100
      child_overlap: 50

    - type: hierarchical
      name: hierarchical_bge_2048_512
      embedding_model: BAAI/bge-large-en-v1.5
      parent_chunk_size: 2048
      child_chunk_size: 512
      parent_overlap: 200
      child_overlap: 100

  # --------------------------------------------------------------------------
  # Query Transformation Strategies
  # --------------------------------------------------------------------------
  query_transform:
    - none        # Baseline: no transformation
    - hyde        # HyDE: LLM generates hypothetical answer
    - multiquery  # Multi-query: LLM rewrites query 3 ways

  # --------------------------------------------------------------------------
  # Reranking Configurations
  # --------------------------------------------------------------------------
  reranker:
    # Test with and without reranking
    configs:
      - enabled: false
        name: no_rerank
      - enabled: true
        name: rerank_bge
        model: bge  # BAAI/bge-reranker-large
      - enabled: true
        name: rerank_ms_marco
        model: ms-marco  # cross-encoder/ms-marco-MiniLM-L-6-v2

  # --------------------------------------------------------------------------
  # Other RAG parameters
  # --------------------------------------------------------------------------
  top_k_values:
    - 3
    - 5

  prompts:
    - concise

  quantization:
    - none  # SLMs don't need quantization

# ============================================================================
# Metrics
# ============================================================================
metrics:
  # Core metrics (fast)
  - f1
  - exact_match
  
  # Semantic metrics
  - bertscore

  # Skip slow metrics for fast iteration
  # - bleurt
  # - llm_judge

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/rag_strategies_test

# ============================================================================
# Execution options
# ============================================================================
batch_size: 16    # Smaller batch for SLMs
min_batch_size: 1

options:
  save_intermediate: true
  skip_existing: true
  
  # RAG-specific options
  build_index_if_missing: true  # Auto-build retriever indices
  cache_embeddings: true        # Cache document embeddings
