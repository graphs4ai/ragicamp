# Smart Retrieval with Small Language Models
# 
# Run: uv run ragicamp run conf/study/smart_retrieval_slm.yaml --dry-run
#
# =============================================================================
# HYPOTHESIS
# =============================================================================
# "High-quality retrieval can compensate for smaller, faster LLMs."
#
# We invest computational budget in retrieval (good embeddings, reranking,
# advanced agents) rather than in large generators. This tests whether
# RAG quality is bottlenecked by retrieval or generation.
#
# =============================================================================
# DESIGN
# =============================================================================
# - Uses SINGLETON experiments (explicit, hypothesis-driven)
# - Each experiment tests ONE specific configuration
# - Premium embeddings: BGE-M3, BGE-large
# - Advanced retrieval: Hybrid (TF-IDF/BM25), reranking, iterative/self-RAG
# - Small generators: Llama-3.2-3B, Phi-3-mini, Qwen2.5-3B
#
# Hardware: Single GPU with 16GB+ VRAM
# Expected runtime: 6-8 hours (3 SLMs x 3 datasets)

name: smart_retrieval_slm
description: "Test if premium retrieval compensates for smaller generators"

# Sample size
num_questions: 1000

# Default dataset (used when experiments don't specify)
datasets:
  - nq
  - triviaqa
  - hotpotqa

# =============================================================================
# Metrics
# =============================================================================
metrics:
  - f1
  - exact_match
  - bertscore
  - bleurt

llm_judge:
  model: openai:gpt-4o-mini
  judgment_type: binary

# =============================================================================
# DirectLLM Baseline (grid search for comparison)
# =============================================================================
# Grid search auto-generates: model x prompt x quantization combinations
# These provide "no retrieval" baselines to measure RAG improvement.
#
# SLMs tested (based on 2024/2025 benchmarks):
# - Llama-3.2-3B: Solid baseline, good instruction following
# - Phi-3-mini-4k: Strong reasoning (3.8B), punches above weight class
# - Qwen2.5-3B: Best MMLU scores in 3B class
direct:
  enabled: true
  models:
    - vllm:meta-llama/Llama-3.2-3B-Instruct
    - vllm:microsoft/Phi-3-mini-4k-instruct
    - vllm:Qwen/Qwen2.5-3B-Instruct
  prompts:
    - concise
  quantization:
    - none

# =============================================================================
# RAG Configuration
# =============================================================================
# We set enabled: true so indexes get built, but don't specify models/top_k
# so no grid search experiments are generated. Actual experiments come from
# the 'experiments:' section below.

rag:
  enabled: true
  
  # Empty models list = no grid search experiments generated
  models: []
  
  # --------------------------------------------------------------------------
  # Corpus: Wikipedia (filtered by importance)
  # --------------------------------------------------------------------------
  # Using WikiRank to filter to top 200k most important articles.
  # This ensures we index high-quality content (major topics, countries, etc.)
  # instead of arbitrary first-200k articles from the dataset stream.
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.en
    max_docs: 200000
    wikirank_top_k: 200000  # Filter to top 200k by WikiRank importance score
    streaming: false        # Load full dataset into RAM (faster, requires ~20GB)
    num_proc: 128           # Parallel workers for dataset loading & WikiRank filtering
    # Index building batch sizes (tuned for high-RAM/VRAM machines)
    doc_batch_size: 50000      # Documents to chunk per batch
    embedding_batch_size: 4096 # Texts to embed per GPU batch
  
  # --------------------------------------------------------------------------
  # Embedding Backend Configuration
  # --------------------------------------------------------------------------
  # Options:
  # - sentence_transformers: Default, works with any HuggingFace model
  # - vllm: Uses vLLM continuous batching (faster, but limited model support)
  #
  # vLLM-compatible embedding models:
  # - intfloat/e5-mistral-7b-instruct (recommended, top MTEB)
  # - Alibaba-NLP/gte-Qwen2-7B-instruct (top overall on MTEB)
  # - BAAI/bge-en-icl (instruction-following BGE)
  # - Salesforce/SFR-Embedding-Mistral
  #
  # NOT compatible with vLLM (use sentence_transformers):
  # - BAAI/bge-large-en-v1.5, BAAI/bge-m3 (BERT-based, not decoder)
  # - intfloat/e5-large-v2 (BERT-based)
  #
  # See: https://docs.vllm.ai/en/latest/getting_started/examples/embedding.html
  embedding:
    backend: vllm  # 'vllm' (faster) or 'sentence_transformers' (more compatible)
    # vLLM-specific settings (only used when backend: vllm)
    vllm_gpu_memory_fraction: 0.9  # Index building runs before generation
    vllm_enforce_eager: false      # Use CUDA graphs for better performance
  
  # --------------------------------------------------------------------------
  # Retriever Definitions (for index building)
  # --------------------------------------------------------------------------
  # Only the two indexes that are already built:
  # - en_bge_large_en_v1.5_c512_o50 (dense_bge_large_512)
  # - en_bge_m3_c512_o50 (dense_bge_m3_512)
  retrievers:
    # Dense retrievers with different embeddings (BUILT)
    - type: dense
      name: dense_bge_large_512
      embedding_model: BAAI/bge-large-en-v1.5
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw  # Fast CPU search (50-100x faster than flat)
    
    - type: dense
      name: dense_bge_m3_512
      embedding_model: BAAI/bge-m3
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # -------------------------------------------------------------------------
    # FUTURE: Top MTEB Retrieval Models (uncomment to build)
    # -------------------------------------------------------------------------
    # Rankings from MTEB retrieval leaderboard (2024/2025):
    # 1. gte-Qwen2-7B-instruct - Top overall but 7B params (expensive)
    # 2. NV-Embed-v1 - NVIDIA, excellent quality
    # 3. GTE-large-en-v1.5 - Strong retrieval, reasonable size
    # 4. E5-large-v2 - Microsoft, battle-tested
    # 5. Nomic-embed-text-v1.5 - Open source, good quality/size ratio
    
    # GTE-large: Alibaba, top-tier retrieval performance
    # - type: dense
    #   name: dense_gte_large_512
    #   embedding_model: Alibaba-NLP/gte-large-en-v1.5
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # E5-large-v2: Microsoft, very stable and well-tested
    # - type: dense
    #   name: dense_e5_large_512
    #   embedding_model: intfloat/e5-large-v2
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # Nomic: 137M params - tests if smaller model is sufficient
    # - type: dense
    #   name: dense_nomic_512
    #   embedding_model: nomic-ai/nomic-embed-text-v1.5
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # Stella: 1.5B params - tests if bigger model improves quality
    # - type: dense
    #   name: dense_stella_512
    #   embedding_model: dunzhang/stella_en_1.5B_v5
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # -------------------------------------------------------------------------
    # Hybrid retrievers (dense + sparse) - reuses BGE-large embeddings
    # -------------------------------------------------------------------------
    # sparse_method: 'tfidf' (default) or 'bm25'
    # Multiple hybrid retrievers with same embedding_model share sparse index
    
    # TF-IDF hybrids
    - type: hybrid
      name: hybrid_bge_large_tfidf_05
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: tfidf
      alpha: 0.5  # 50% dense, 50% sparse
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    - type: hybrid
      name: hybrid_bge_large_tfidf_07
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: tfidf
      alpha: 0.7  # 70% dense, 30% sparse
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # BM25 hybrids (more sophisticated sparse ranking)
    - type: hybrid
      name: hybrid_bge_large_bm25_05
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: bm25
      alpha: 0.5  # 50% dense, 50% sparse
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    - type: hybrid
      name: hybrid_bge_large_bm25_07
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: bm25
      alpha: 0.7  # 70% dense, 30% sparse
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw

# =============================================================================
# SINGLETON EXPERIMENTS (hypothesis-driven)
# =============================================================================
# Each group tests ONE retrieval strategy with ALL 3 SLMs for fair comparison.
# Direct baselines (no RAG) are auto-generated by the direct: section above.
#
# SLMs tested in each group:
# - Llama-3.2-3B: Meta, solid baseline
# - Phi-3-mini-4k: Microsoft, strong reasoning (3.8B)
# - Qwen2.5-3B: Alibaba, best MMLU in 3B class

experiments:
  # ========================================================================
  # GROUP A: Simple Dense RAG (baseline for all SLMs)
  # ========================================================================
  # Tests basic dense retrieval with BGE-large embeddings.
  # This is the simplest RAG setup - establishes baseline for each model.
  
  - name: a1_llama_dense
    hypothesis: "Llama + simple dense retrieval"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: a2_phi3_dense
    hypothesis: "Phi-3 + simple dense retrieval"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: a3_qwen_dense
    hypothesis: "Qwen + simple dense retrieval"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP B: Hybrid Retrieval (Dense + BM25) - all SLMs
  # ========================================================================
  # Tests hybrid retrieval combining dense embeddings with BM25 sparse.
  # Uses 70/30 dense-heavy blend which typically works best.
  
  - name: b1_llama_hybrid
    hypothesis: "Llama + hybrid (70/30) catches keywords"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    prompt: concise
  
  - name: b2_phi3_hybrid
    hypothesis: "Phi-3 + hybrid retrieval"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    prompt: concise
  
  - name: b3_qwen_hybrid
    hypothesis: "Qwen + hybrid retrieval"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP C: Reranking (Two-Stage) - all SLMs
  # ========================================================================
  # Tests cross-encoder reranking: overfetch 25, rerank to top 5.
  # Reranking typically gives biggest single improvement.
  
  - name: c1_llama_rerank
    hypothesis: "Llama + reranking for precision"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: c2_phi3_rerank
    hypothesis: "Phi-3 + reranking"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: dense_bge_large_512
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: c3_qwen_rerank
    hypothesis: "Qwen + reranking"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise

  # ========================================================================
  # GROUP D: Query Transform (HyDE) - all SLMs
  # ========================================================================
  # Tests HyDE query transformation for better retrieval.
  # Note: HyDE uses the LLM itself, so model quality matters more here.
  
  - name: d1_llama_hyde
    hypothesis: "Llama + HyDE query transform"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: hyde
    top_k: 5
    prompt: concise
  
  - name: d2_phi3_hyde
    hypothesis: "Phi-3 + HyDE - better hypotheticals?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: dense_bge_large_512
    query_transform: hyde
    top_k: 5
    prompt: concise
  
  - name: d3_qwen_hyde
    hypothesis: "Qwen + HyDE"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: hyde
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP E: Iterative RAG - all SLMs
  # ========================================================================
  # Tests query refinement with one iteration.
  # Model quality affects refinement ability.
  
  - name: e1_llama_iterative
    hypothesis: "Llama + iterative refinement"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: e2_phi3_iterative
    hypothesis: "Phi-3 + iterative - better reasoning?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: e3_qwen_iterative
    hypothesis: "Qwen + iterative refinement"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise

  # ========================================================================
  # GROUP F: Premium Stack (Hybrid + Rerank) - all SLMs
  # ========================================================================
  # Tests best combined retrieval: hybrid + reranking.
  # This should show maximum retrieval quality.
  
  - name: f1_llama_premium
    hypothesis: "Llama + hybrid + rerank (premium)"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: f2_phi3_premium
    hypothesis: "Phi-3 + premium stack"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: f3_qwen_premium
    hypothesis: "Qwen + premium stack - best overall?"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise

# =============================================================================
# Output Configuration
# =============================================================================
output_dir: outputs/smart_retrieval_slm

# =============================================================================
# Execution Settings
# =============================================================================
batch_size: 128
min_batch_size: 1
