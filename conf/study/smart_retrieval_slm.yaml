# Smart Retrieval with Small Language Models
# 
# Run: uv run ragicamp run conf/study/smart_retrieval_slm.yaml --dry-run
#
# =============================================================================
# HYPOTHESIS
# =============================================================================
# "High-quality retrieval can compensate for smaller, faster LLMs."
#
# We invest computational budget in retrieval (good embeddings, reranking,
# advanced agents) rather than in large generators. This tests whether
# RAG quality is bottlenecked by retrieval or generation.
#
# =============================================================================
# DESIGN
# =============================================================================
# - Uses GRID SEARCH for systematic coverage of combinations
# - Uses SINGLETON experiments for agent-based strategies (iterative, self-rag)
# - Supports RANDOM SEARCH mode for efficient exploration (see sampling: section)
#
# Embeddings: BGE-large (BERT), BGE-M3 (multilingual), GTE-Qwen2-1.5B (fast + strong)
# Advanced retrieval: Hybrid (TF-IDF/BM25), Hierarchical, reranking
# Query transforms: HyDE, MultiQuery
# Small generators: Llama-3.2-3B, Phi-3-mini, Qwen2.5-3B
#
# Hardware: Single GPU with 48GB+ VRAM (B200 recommended)

name: smart_retrieval_slm
description: "Test if premium retrieval compensates for smaller generators"

# Sample size per experiment
num_questions: 1000

# Datasets to test
datasets:
  - nq
  - triviaqa
  - hotpotqa

# =============================================================================
# Metrics
# =============================================================================
metrics:
  - f1
  - exact_match
  - bertscore
  - bleurt

llm_judge:
  model: openai:gpt-4o-mini
  judgment_type: binary

# =============================================================================
# SLM Models (shared across Direct and RAG)
# =============================================================================
# Using YAML anchors to ensure both Direct and RAG use identical models
models: &slm_models
  - vllm:meta-llama/Llama-3.2-3B-Instruct   # Meta, solid baseline
  - vllm:microsoft/Phi-3-mini-4k-instruct   # Microsoft, strong reasoning (3.8B)
  - vllm:Qwen/Qwen2.5-3B-Instruct           # Alibaba, best MMLU in 3B class

# =============================================================================
# DirectLLM Baseline (grid search for comparison)
# =============================================================================
# These provide "no retrieval" baselines to measure RAG improvement.
direct:
  enabled: true
  models: *slm_models
  prompts:
    - concise

# =============================================================================
# RAG Configuration (Grid Search)
# =============================================================================
# Grid search generates: Models × Retrievers × TopK × QueryTransforms × Rerankers × Prompts × Datasets
#
# To enable random search mode, uncomment the sampling section below.

rag:
  enabled: true
  models: *slm_models
  
  # --------------------------------------------------------------------------
  # Sampling Mode (Random Search)
  # --------------------------------------------------------------------------
  # Sample ~300 RAG experiments from the grid to keep total around 350.
  # Grid: 3 models × 6 retrievers × 3 top_k × 3 query_transform × 3 reranker × 3 datasets
  #     = 1,458 possible combinations
  # Fixed experiments (always included):
  #   - Direct baselines: 9 (3 models × 3 datasets)
  #   - Singleton experiments: 30 (iterative_rag, self_rag, premium × 3 datasets)
  # Total: 9 + 30 + 300 = ~350 experiments
  sampling:
    mode: stratified        # Ensures coverage across all models × retrievers
    n_experiments: 300      # Sample from 1,458 RAG grid combinations
    seed: 42                # Reproducible sampling
    stratify_by: [model, retriever]  # At least one experiment per model×retriever
  
  # --------------------------------------------------------------------------
  # Corpus: Wikipedia (filtered by importance)
  # --------------------------------------------------------------------------
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.en
    max_docs: 200000
    wikirank_top_k: 200000
    streaming: false
    num_proc: 128
    # doc_batch_size: Number of documents per embedding batch.
    doc_batch_size: 5000
    embedding_batch_size: 4096  # Only used for sentence_transformers, vLLM ignores
    # Embedding backend configuration (used during index building)
    embedding:
      backend: vllm
      vllm_gpu_memory_fraction: 0.9  # High fraction for index building (no generator running)
      vllm_enforce_eager: false
  
  # --------------------------------------------------------------------------
  # Retrievers (referenced by grid search)
  # --------------------------------------------------------------------------
  # The grid search will use the 'name' field to reference these configs.
  # Index building uses full config, experiments use name reference.
  retrievers:
    # =========================================================================
    # DENSE RETRIEVERS
    # =========================================================================
    # NOTE: embedding_index maps logical names to actual index directories
    # Index naming convention: {lang}_{model_shortname}_c{chunk}_o{overlap}
    
    # BGE-large: BERT-based, proven MTEB performer
    # vLLM supports BERT-based embedding models via BertEmbeddingModel
    # See: https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/
    - type: dense
      name: dense_bge_large_512
      embedding_index: en_bge_large_en_v1.5_c512_o50  # Actual index on disk
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # BGE-M3: Multilingual, good for diverse queries
    - type: dense
      name: dense_bge_m3_512
      embedding_index: en_bge_m3_c512_o50  # Actual index on disk
      embedding_model: BAAI/bge-m3
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # =========================================================================
    # HIGH-PERFORMANCE EMBEDDING MODELS (vLLM 0.15+)
    # =========================================================================
    
    # GTE-Qwen2-1.5B: Fast with strong MTEB scores (~67)
    # 1.5B params, good balance of speed and quality
    - type: dense
      name: dense_gte_qwen2_1.5b_512
      embedding_index: en_gte_qwen2_1.5b_instruct_c512_o50  # Actual index on disk
      embedding_model: Alibaba-NLP/gte-Qwen2-1.5B-instruct
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # E5-Mistral: 7B decoder, top MTEB retrieval scores
    - type: dense
      name: dense_e5_mistral_512
      embedding_index: en_e5_mistral_7b_instruct_c512_o50  # Actual index on disk
      embedding_model: intfloat/e5-mistral-7b-instruct
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # =========================================================================
    # HYBRID RETRIEVERS (Dense + Sparse)
    # =========================================================================
    # Hybrid retrievers reuse the dense index + add sparse index
    
    # Hybrid BM25 - 50/50 blend
    - type: hybrid
      name: hybrid_bge_large_bm25_05
      embedding_index: en_bge_large_en_v1.5_c512_o50  # Reuses dense index
      sparse_index: en_bge_large_en_v1.5_c512_o50_sparse_bm25
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      sparse_method: bm25
      alpha: 0.5
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # Hybrid BM25 - 70/30 dense-heavy (typically best)
    - type: hybrid
      name: hybrid_bge_large_bm25_07
      embedding_index: en_bge_large_en_v1.5_c512_o50  # Reuses dense index
      sparse_index: en_bge_large_en_v1.5_c512_o50_sparse_bm25
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      sparse_method: bm25
      alpha: 0.7
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # Hybrid TF-IDF - alternative sparse method
    - type: hybrid
      name: hybrid_bge_large_tfidf_07
      embedding_index: en_bge_large_en_v1.5_c512_o50  # Reuses dense index
      sparse_index: en_bge_large_en_v1.5_c512_o50_sparse_tfidf
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      sparse_method: tfidf
      alpha: 0.7
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # Hybrid GTE-Qwen2 + BM25 - strong hybrid with fast embeddings
    - type: hybrid
      name: hybrid_gte_qwen2_bm25_07
      embedding_index: en_gte_qwen2_1.5b_instruct_c512_o50  # Reuses GTE dense index
      embedding_model: Alibaba-NLP/gte-Qwen2-1.5B-instruct
      embedding_backend: vllm
      sparse_method: bm25
      alpha: 0.7
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # =========================================================================
    # HIERARCHICAL RETRIEVERS (Parent-Child Chunks)
    # =========================================================================
    
    # Hierarchical: Search small chunks, return larger context
    - type: hierarchical
      name: hierarchical_bge_large
      embedding_index: hierarchical_bge_large  # Hierarchical uses its own index
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      parent_chunk_size: 2048
      child_chunk_size: 448
      index_type: hnsw
  
  # --------------------------------------------------------------------------
  # Grid Search Dimensions
  # --------------------------------------------------------------------------
  # Which retrievers to test in grid search (by name)
  # 
  # STRATEGY: Compare embedding models with different characteristics
  # - BGE Large: proven baseline, fast inference
  # - BGE M3: multilingual capability  
  # - GTE-Qwen2-1.5B: fast with strong MTEB scores (requires vLLM 0.15+)
  # - Hybrid: combines dense + BM25
  #
  retriever_names:
    - dense_bge_large_512           # Built - proven baseline
    - dense_bge_m3_512              # Built - multilingual
    - dense_gte_qwen2_1.5b_512      # Built - fast, strong (1.5B params)
    - dense_e5_mistral_512          # Built - 7B decoder, top MTEB scores
    - hybrid_bge_large_bm25_07      # Built - uses BGE Large index + BM25
    - hybrid_gte_qwen2_bm25_07      # Uses GTE index + BM25 (needs sparse)
    - hierarchical_bge_large        # Hierarchical retrieval
  
  top_k_values:
    - 3
    - 5
    - 10
  
  # Query transformation strategies
  query_transform:
    - none         # Baseline
    - hyde         # Hypothetical document embeddings
    - multiquery   # Multiple query variations
  
  # Reranker configurations
  reranker:
    configs:
      - enabled: false
        name: none
      - enabled: true
        name: bge
      - enabled: true
        name: bge-v2
  
  # Fetch K multiplier (documents to retrieve before reranking)
  fetch_k_multiplier: 5  # fetch_k = top_k * 5 when reranking
  
  # --------------------------------------------------------------------------
  # Prompt Strategies
  # --------------------------------------------------------------------------
  # Testing different prompt formats to find what works best with SLMs
  #
  # Available prompts:
  # - concise: Minimal instructions, just answer (baseline)
  # - structured: Clear ### delimiters, explicit sections
  # - extractive: Strict context-only, no model knowledge
  # - cot: Chain-of-thought reasoning (good for HotpotQA)
  # - cited: Answers with passage citations [1], [2]
  # - fewshot_3: 3 in-context examples
  #
  prompts:
    - concise       # Baseline - minimal prompt
    - structured    # Clear delimiters, better for instruction-tuned models
    - extractive    # Strict extraction, reduces hallucination

# =============================================================================
# SINGLETON EXPERIMENTS (Agent-Based Strategies)
# =============================================================================
# These require agent_type which can't be expressed in grid search.
# Each tests ONE agent strategy with each SLM for comparison.

experiments:
  # ========================================================================
  # GROUP: Iterative RAG (Query Refinement)
  # ========================================================================
  # Tests query refinement over multiple iterations.
  
  - name: iterative_llama_1iter
    hypothesis: "Llama + 1 iteration refinement"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: iterative_phi3_1iter
    hypothesis: "Phi-3 + 1 iteration - better reasoning?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: iterative_qwen_1iter
    hypothesis: "Qwen + 1 iteration refinement"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  # 2 iterations - does more help?
  - name: iterative_qwen_2iter
    hypothesis: "Qwen + 2 iterations - more refinement helps?"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 2
    stop_on_sufficient: true
    prompt: concise

  # ========================================================================
  # GROUP: Self-RAG (Adaptive Retrieval)
  # ========================================================================
  # Tests adaptive retrieval - model decides when to retrieve.
  
  - name: selfrag_llama
    hypothesis: "Llama + Self-RAG adaptive retrieval"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: selfrag_phi3
    hypothesis: "Phi-3 + Self-RAG - better self-assessment?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: selfrag_qwen
    hypothesis: "Qwen + Self-RAG adaptive retrieval"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP: Premium Combo (Hybrid + Rerank + HyDE) - Best of Everything
  # ========================================================================
  # Tests maximum retrieval investment per SLM.
  
  - name: premium_llama
    hypothesis: "Llama + hybrid + rerank + hyde (maximum retrieval)"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise
  
  - name: premium_phi3
    hypothesis: "Phi-3 + premium stack"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise
  
  - name: premium_qwen
    hypothesis: "Qwen + premium stack - best overall?"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise

# =============================================================================
# Output Configuration
# =============================================================================
output_dir: outputs/smart_retrieval_slm

# =============================================================================
# Execution Settings
# =============================================================================
batch_size: 128
min_batch_size: 1
