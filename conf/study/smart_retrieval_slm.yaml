# Smart Retrieval with Small Language Models
# 
# Run: uv run ragicamp run conf/study/smart_retrieval_slm.yaml --dry-run
#
# =============================================================================
# HYPOTHESIS
# =============================================================================
# "High-quality retrieval can compensate for smaller, faster LLMs."
#
# We invest computational budget in retrieval (good embeddings, reranking,
# advanced agents) rather than in large generators. This tests whether
# RAG quality is bottlenecked by retrieval or generation.
#
# =============================================================================
# DESIGN
# =============================================================================
# - Uses SINGLETON experiments (explicit, hypothesis-driven)
# - Each experiment tests ONE specific configuration
# - Premium embeddings: BGE-M3, BGE-large, Nomic
# - Advanced retrieval: Hybrid, hierarchical, reranking
# - Small generator: Llama 3.2 3B
#
# Hardware: Single GPU with 16GB+ VRAM
# Expected runtime: 4-6 hours

name: smart_retrieval_slm
description: "Test if premium retrieval compensates for smaller generators"

# Sample size
num_questions: 1000

# Default dataset (used when experiments don't specify)
datasets:
  - nq
  - triviaqa
  - hotpotqa

# =============================================================================
# Metrics
# =============================================================================
metrics:
  - f1
  - exact_match
  - bertscore
  - bleurt

llm_judge:
  model: openai:gpt-4o-mini
  judgment_type: binary

# =============================================================================
# DirectLLM Baseline (grid search for comparison)
# =============================================================================
direct:
  enabled: true
  models:
    - vllm:meta-llama/Llama-3.2-3B-Instruct
  prompts:
    - concise
  quantization:
    - none

# =============================================================================
# RAG Configuration
# =============================================================================
# We set enabled: true so indexes get built, but don't specify models/top_k
# so no grid search experiments are generated. Actual experiments come from
# the 'experiments:' section below.

rag:
  enabled: true
  
  # Empty models list = no grid search experiments generated
  models: []
  
  # --------------------------------------------------------------------------
  # Corpus: Wikipedia (filtered by importance)
  # --------------------------------------------------------------------------
  # Using WikiRank to filter to top 200k most important articles.
  # This ensures we index high-quality content (major topics, countries, etc.)
  # instead of arbitrary first-200k articles from the dataset stream.
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.en
    max_docs: 200000
    wikirank_top_k: 200000  # Filter to top 200k by WikiRank importance score
    streaming: false        # Load full dataset into RAM (faster, requires ~20GB)
    num_proc: 128           # Parallel workers for dataset loading & WikiRank filtering
    # Index building batch sizes (tuned for high-RAM/VRAM machines)
    doc_batch_size: 50000      # Documents to chunk per batch
    embedding_batch_size: 4096 # Texts to embed per GPU batch
  
  # --------------------------------------------------------------------------
  # Retriever Definitions (for index building)
  # --------------------------------------------------------------------------
  # Only the two indexes that are already built:
  # - en_bge_large_en_v1.5_c512_o50 (dense_bge_large_512)
  # - en_bge_m3_c512_o50 (dense_bge_m3_512)
  retrievers:
    # Dense retrievers with different embeddings (BUILT)
    - type: dense
      name: dense_bge_large_512
      embedding_model: BAAI/bge-large-en-v1.5
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw  # Fast CPU search (50-100x faster than flat)
    
    - type: dense
      name: dense_bge_m3_512
      embedding_model: BAAI/bge-m3
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # -------------------------------------------------------------------------
    # FUTURE: Additional embedding models (uncomment to build)
    # -------------------------------------------------------------------------
    # Nomic: 137M params - tests if smaller model is sufficient
    # - type: dense
    #   name: dense_nomic_512
    #   embedding_model: nomic-ai/nomic-embed-text-v1.5
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # Stella: 1.5B params - tests if bigger model improves quality
    # - type: dense
    #   name: dense_stella_512
    #   embedding_model: dunzhang/stella_en_1.5B_v5
    #   chunk_size: 512
    #   chunk_overlap: 50
    #   chunking_strategy: recursive
    #   index_type: hnsw
    
    # Hybrid retrievers (dense + BM25) - uses existing BGE-large embeddings
    - type: hybrid
      name: hybrid_bge_large_05
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.5  # 50% dense, 50% BM25
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    - type: hybrid
      name: hybrid_bge_large_07
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.7  # 70% dense, 30% BM25
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw

# =============================================================================
# SINGLETON EXPERIMENTS (hypothesis-driven)
# =============================================================================
# These are at TOP LEVEL, not under rag:
# Each experiment is explicit and tests one hypothesis.

experiments:
  # ========================================================================
  # GROUP A: Embedding Model Comparison
  # ========================================================================
  
  - name: a1_bge_large_baseline
    hypothesis: "BGE-large as dense baseline"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: a2_bge_m3_dense
    hypothesis: "BGE-M3 captures richer semantics"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_m3_512
    top_k: 5
    prompt: concise
  
  # FUTURE: Additional embedding models (uncomment when indexes are built)
  # - name: a3_nomic_embed
  #   hypothesis: "Nomic (137M) tests if smaller model is sufficient"
  #   model: vllm:meta-llama/Llama-3.2-3B-Instruct
  #   retriever: dense_nomic_512
  #   top_k: 5
  #   prompt: concise
  
  # - name: a4_stella_embed
  #   hypothesis: "Stella (1.5B) tests if bigger model improves quality"
  #   model: vllm:meta-llama/Llama-3.2-3B-Instruct
  #   retriever: dense_stella_512
  #   top_k: 5
  #   prompt: concise

  # ========================================================================
  # GROUP B: Hybrid Retrieval (Dense + Sparse)
  # ========================================================================
  
  - name: b1_hybrid_balanced
    hypothesis: "Hybrid catches keywords dense misses"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_05
    top_k: 5
    prompt: concise
  
  - name: b2_hybrid_dense_heavy
    hypothesis: "70% dense, 30% sparse optimal balance"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_07
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP D: Reranking (Two-Stage Retrieval)
  # ========================================================================
  
  - name: d1_rerank_bge_top3
    hypothesis: "Overfetch 20, rerank to 3 for precision"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 3
    fetch_k: 20
    reranker: bge
    prompt: concise
  
  - name: d2_rerank_bge_top5
    hypothesis: "Overfetch 25, rerank to 5 for balance"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: d3_hybrid_rerank
    hypothesis: "Hybrid + rerank = best of both worlds"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_05
    top_k: 5
    fetch_k: 20
    reranker: bge
    prompt: concise

  # ========================================================================
  # GROUP E: Query Transformation
  # ========================================================================
  
  - name: e1_hyde
    hypothesis: "HyDE helps match answer vocabulary"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: hyde
    top_k: 5
    prompt: concise
  
  - name: e2_multiquery
    hypothesis: "Multiple query variants improve recall"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: multiquery
    top_k: 5
    prompt: concise
  
  - name: e3_hyde_rerank
    hypothesis: "HyDE + rerank = transform then filter"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: hyde
    top_k: 3
    fetch_k: 15
    reranker: bge
    prompt: concise
  
  - name: e4_multiquery_rerank
    hypothesis: "MultiQuery expands recall, rerank filters precision"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: dense_bge_large_512
    query_transform: multiquery
    top_k: 3
    fetch_k: 20
    reranker: bge
    prompt: concise

  # ========================================================================
  # GROUP F: Advanced Agents
  # ========================================================================
  
  - name: f1_iterative_1round
    hypothesis: "One refinement round helps complex queries"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: f2_iterative_2rounds
    hypothesis: "Two refinement rounds for stubborn queries"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 2
    stop_on_sufficient: true
    prompt: concise
  
  - name: f3_selfrag_balanced
    hypothesis: "Adaptive retrieval skips when unnecessary"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    retrieval_threshold: 0.5
    verify_answer: false
    prompt: concise
  
  - name: f4_selfrag_verified
    hypothesis: "Verification ensures grounding"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    retrieval_threshold: 0.5
    verify_answer: true
    fallback_to_direct: true
    prompt: concise
  
  - name: f5_iterative_rerank
    hypothesis: "Query refinement + reranking for precision"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 3
    fetch_k: 15
    reranker: bge
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise

  # ========================================================================
  # GROUP G: Combined Best Practices
  # ========================================================================
  
  - name: g1_premium_stack
    hypothesis: "Best-in-class: hybrid + rerank"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_07
    top_k: 5
    fetch_k: 25
    reranker: bge
    prompt: concise
  
  - name: g2_iterative_premium
    hypothesis: "Iterative + hybrid + rerank"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: hybrid_bge_large_07
    top_k: 5
    fetch_k: 20
    reranker: bge
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise

# =============================================================================
# Output Configuration
# =============================================================================
output_dir: outputs/smart_retrieval_slm

# =============================================================================
# Execution Settings
# =============================================================================
batch_size: 128
min_batch_size: 1
