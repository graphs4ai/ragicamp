# Smart Retrieval with Small Language Models
# 
# Run: uv run ragicamp run conf/study/smart_retrieval_slm.yaml --dry-run
#
# =============================================================================
# HYPOTHESIS
# =============================================================================
# "High-quality retrieval can compensate for smaller, faster LLMs."
#
# We invest computational budget in retrieval (good embeddings, reranking,
# advanced agents) rather than in large generators. This tests whether
# RAG quality is bottlenecked by retrieval or generation.
#
# =============================================================================
# DESIGN
# =============================================================================
# - Uses GRID SEARCH for systematic coverage of combinations
# - Uses SINGLETON experiments for agent-based strategies (iterative, self-rag)
# - Supports RANDOM SEARCH mode for efficient exploration (see sampling: section)
#
# Embeddings: BGE-large (BERT), BGE-M3 (multilingual), E5-Mistral-7B (decoder)
# Advanced retrieval: Hybrid (TF-IDF/BM25), Hierarchical, reranking
# Query transforms: HyDE, MultiQuery
# Small generators: Llama-3.2-3B, Phi-3-mini, Qwen2.5-3B
#
# Hardware: Single GPU with 48GB+ VRAM (B200 recommended)

name: smart_retrieval_slm
description: "Test if premium retrieval compensates for smaller generators"

# Sample size per experiment
num_questions: 1000

# Datasets to test
datasets:
  - nq
  - triviaqa
  - hotpotqa

# =============================================================================
# Metrics
# =============================================================================
metrics:
  - f1
  - exact_match
  - bertscore
  - bleurt

llm_judge:
  model: openai:gpt-4o-mini
  judgment_type: binary

# =============================================================================
# SLM Models (shared across Direct and RAG)
# =============================================================================
# Using YAML anchors to ensure both Direct and RAG use identical models
models: &slm_models
  - vllm:meta-llama/Llama-3.2-3B-Instruct   # Meta, solid baseline
  - vllm:microsoft/Phi-3-mini-4k-instruct   # Microsoft, strong reasoning (3.8B)
  - vllm:Qwen/Qwen2.5-3B-Instruct           # Alibaba, best MMLU in 3B class

# =============================================================================
# DirectLLM Baseline (grid search for comparison)
# =============================================================================
# These provide "no retrieval" baselines to measure RAG improvement.
direct:
  enabled: true
  models: *slm_models
  prompts:
    - concise
  quantization:
    - none

# =============================================================================
# RAG Configuration (Grid Search)
# =============================================================================
# Grid search generates: Models × Retrievers × TopK × QueryTransforms × Rerankers × Prompts × Datasets
#
# To enable random search mode, uncomment the sampling section below.

rag:
  enabled: true
  models: *slm_models
  
  # --------------------------------------------------------------------------
  # Sampling Mode (Random Search)
  # --------------------------------------------------------------------------
  # Sample ~260 RAG experiments from the grid to keep total around 300.
  # Fixed experiments (always included):
  #   - Direct baselines: 9 (3 models × 3 datasets)
  #   - Singleton experiments: 30 (iterative_rag, self_rag, premium × 3 datasets)
  # Total: 9 + 30 + 260 = ~300 experiments
  sampling:
    mode: stratified        # Ensures coverage across all models × retrievers
    n_experiments: 260      # Sample from 1,215 RAG grid combinations
    seed: 42                # Reproducible sampling
    stratify_by: [model, retriever]  # At least one experiment per model×retriever
  
  # --------------------------------------------------------------------------
  # Corpus: Wikipedia (filtered by importance)
  # --------------------------------------------------------------------------
  corpus:
    source: wikimedia/wikipedia
    version: 20231101.en
    max_docs: 200000
    wikirank_top_k: 200000
    streaming: false
    num_proc: 128
    # doc_batch_size: Number of documents per embedding batch.
    doc_batch_size: 5000
    embedding_batch_size: 4096
  
  # --------------------------------------------------------------------------
  # Embedding Backend Configuration
  # --------------------------------------------------------------------------
  embedding:
    backend: vllm
    vllm_gpu_memory_fraction: 0.9
    vllm_enforce_eager: false
  
  # --------------------------------------------------------------------------
  # Retrievers (referenced by grid search)
  # --------------------------------------------------------------------------
  # The grid search will use the 'name' field to reference these configs.
  # Index building uses full config, experiments use name reference.
  retrievers:
    # =========================================================================
    # DENSE RETRIEVERS
    # =========================================================================
    
    # BGE-large: BERT-based, proven MTEB performer
    # vLLM supports BERT-based embedding models via BertEmbeddingModel
    # See: https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/
    - type: dense
      name: dense_bge_large_512
      embedding_model: BAAI/bge-large-en-v1.5
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # BGE-M3: Multilingual, good for diverse queries
    - type: dense
      name: dense_bge_m3_512
      embedding_model: BAAI/bge-m3
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # E5-Mistral: 7B decoder, top MTEB retrieval scores
    - type: dense
      name: dense_e5_mistral_512
      embedding_model: intfloat/e5-mistral-7b-instruct
      embedding_backend: vllm
      chunk_size: 512
      chunk_overlap: 50
      chunking_strategy: recursive
      index_type: hnsw
    
    # =========================================================================
    # HYBRID RETRIEVERS (Dense + Sparse)
    # =========================================================================
    
    # Hybrid BM25 - 50/50 blend
    - type: hybrid
      name: hybrid_bge_large_bm25_05
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: bm25
      alpha: 0.5
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # Hybrid BM25 - 70/30 dense-heavy (typically best)
    - type: hybrid
      name: hybrid_bge_large_bm25_07
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: bm25
      alpha: 0.7
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # Hybrid TF-IDF - alternative sparse method
    - type: hybrid
      name: hybrid_bge_large_tfidf_07
      embedding_model: BAAI/bge-large-en-v1.5
      sparse_method: tfidf
      alpha: 0.7
      chunk_size: 512
      chunk_overlap: 50
      index_type: hnsw
    
    # =========================================================================
    # HIERARCHICAL RETRIEVERS (Parent-Child Chunks)
    # =========================================================================
    
    # Hierarchical: Search small chunks, return larger context
    - type: hierarchical
      name: hierarchical_bge_large
      embedding_model: BAAI/bge-large-en-v1.5
      parent_chunk_size: 1024
      child_chunk_size: 256
      index_type: hnsw
  
  # --------------------------------------------------------------------------
  # Grid Search Dimensions
  # --------------------------------------------------------------------------
  # Which retrievers to test in grid search (by name)
  retriever_names:
    - dense_bge_large_512
    - dense_bge_m3_512
    - dense_e5_mistral_512
    - hybrid_bge_large_bm25_07
    - hierarchical_bge_large
  
  top_k_values:
    - 3
    - 5
    - 10
  
  # Query transformation strategies
  query_transform:
    - none         # Baseline
    - hyde         # Hypothetical document embeddings
    - multiquery   # Multiple query variations
  
  # Reranker configurations
  reranker:
    configs:
      - enabled: false
        name: none
      - enabled: true
        name: bge
      - enabled: true
        name: bge-v2
  
  # Fetch K multiplier (documents to retrieve before reranking)
  fetch_k_multiplier: 5  # fetch_k = top_k * 5 when reranking
  
  prompts:
    - concise
  
  quantization:
    - none

# =============================================================================
# SINGLETON EXPERIMENTS (Agent-Based Strategies)
# =============================================================================
# These require agent_type which can't be expressed in grid search.
# Each tests ONE agent strategy with each SLM for comparison.

experiments:
  # ========================================================================
  # GROUP: Iterative RAG (Query Refinement)
  # ========================================================================
  # Tests query refinement over multiple iterations.
  
  - name: iterative_llama_1iter
    hypothesis: "Llama + 1 iteration refinement"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: iterative_phi3_1iter
    hypothesis: "Phi-3 + 1 iteration - better reasoning?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  - name: iterative_qwen_1iter
    hypothesis: "Qwen + 1 iteration refinement"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 1
    stop_on_sufficient: true
    prompt: concise
  
  # 2 iterations - does more help?
  - name: iterative_qwen_2iter
    hypothesis: "Qwen + 2 iterations - more refinement helps?"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: iterative_rag
    retriever: dense_bge_large_512
    top_k: 5
    max_iterations: 2
    stop_on_sufficient: true
    prompt: concise

  # ========================================================================
  # GROUP: Self-RAG (Adaptive Retrieval)
  # ========================================================================
  # Tests adaptive retrieval - model decides when to retrieve.
  
  - name: selfrag_llama
    hypothesis: "Llama + Self-RAG adaptive retrieval"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: selfrag_phi3
    hypothesis: "Phi-3 + Self-RAG - better self-assessment?"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise
  
  - name: selfrag_qwen
    hypothesis: "Qwen + Self-RAG adaptive retrieval"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    agent_type: self_rag
    retriever: dense_bge_large_512
    top_k: 5
    prompt: concise

  # ========================================================================
  # GROUP: Premium Combo (Hybrid + Rerank + HyDE) - Best of Everything
  # ========================================================================
  # Tests maximum retrieval investment per SLM.
  
  - name: premium_llama
    hypothesis: "Llama + hybrid + rerank + hyde (maximum retrieval)"
    model: vllm:meta-llama/Llama-3.2-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise
  
  - name: premium_phi3
    hypothesis: "Phi-3 + premium stack"
    model: vllm:microsoft/Phi-3-mini-4k-instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise
  
  - name: premium_qwen
    hypothesis: "Qwen + premium stack - best overall?"
    model: vllm:Qwen/Qwen2.5-3B-Instruct
    retriever: hybrid_bge_large_bm25_07
    query_transform: hyde
    top_k: 5
    fetch_k: 25
    reranker: bge-v2
    prompt: concise

# =============================================================================
# Output Configuration
# =============================================================================
output_dir: outputs/smart_retrieval_slm

# =============================================================================
# Execution Settings
# =============================================================================
batch_size: 128
min_batch_size: 1
