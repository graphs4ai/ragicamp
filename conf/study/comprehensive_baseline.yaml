# Comprehensive Baseline Study
# Full baseline with multiple HF models, OpenAI, and corpus variations
#
# Run: python scripts/experiments/run_study.py conf/study/comprehensive_baseline.yaml
#
# Experiment matrix:
#   DirectLLM: 6 models × 3 prompts × 3 datasets = 54 experiments
#   RAG: 2 models × 6 retrievers × 3 top_k × 3 datasets = 108 experiments
#   Total: ~162 experiments
#
# Estimated time: 10-20 hours (depending on models/hardware)

name: comprehensive_baseline
description: "Full baseline study with HF + OpenAI models and multiple retrieval configurations"

# Full datasets (set to null for complete dataset, or cap for testing)
num_questions: null  # null = full dataset

# All three QA datasets
datasets:
  - nq
  - triviaqa
  - hotpotqa

# ============================================================================
# DirectLLM Experiments (no retrieval)
# ============================================================================
direct:
  enabled: true
  models:
    # --- OpenAI (high quality, costs money) ---
    - openai:gpt-4o-mini       # Fast, cheap, good baseline
    - openai:gpt-4o            # SOTA, expensive but comprehensive
    
    # --- HuggingFace models (local, free) ---
    # Small models (fast, good for baseline)
    - hf:google/gemma-2b-it           # 2B params, good quality
    - hf:microsoft/phi-3-mini-4k-instruct  # 3.8B params, strong for size
    
    # Medium models (need more VRAM, better quality)
    - hf:meta-llama/Llama-3.2-3B-Instruct  # 3B Llama, good instruction following
    - hf:mistralai/Mistral-7B-Instruct-v0.3  # 7B, excellent quality
    
    # Large models (optional, need 24GB+ VRAM)
    # - hf:meta-llama/Llama-3.1-8B-Instruct  # 8B, very strong
    # - hf:Qwen/Qwen2.5-7B-Instruct          # 7B, multilingual
    
  prompts:
    - default    # Standard QA prompt
    - concise    # "Answer briefly"
    - detailed   # "Explain your reasoning"

# ============================================================================
# RAG Experiments (with retrieval)
# ============================================================================
rag:
  enabled: true
  
  # Models for RAG (keep this smaller - focus on best models)
  models:
    - openai:gpt-4o-mini   # Good balance of quality/cost
    - hf:google/gemma-2b-it  # Representative local model
  
  # Retriever variations (6 indexes)
  # Format: {corpus}_{embedding}_{strategy}_{chunk_size}
  retrievers:
    # Simple Wikipedia corpus (faster, good for dev)
    - simple_minilm_recursive_512      # MiniLM, small chunks
    - simple_minilm_recursive_1024     # MiniLM, larger chunks
    - simple_mpnet_recursive_512       # MPNet (better embeddings)
    
    # Full English Wikipedia (production, higher coverage)
    - en_minilm_recursive_512
    - en_mpnet_recursive_512
    - en_e5_recursive_512              # E5 embeddings (SOTA)
  
  # How many docs to retrieve
  top_k_values:
    - 3    # Minimal context
    - 5    # Balanced
    - 10   # Rich context
  
  # RAG-specific prompts
  prompts:
    - default

# ============================================================================
# Metrics
# ============================================================================
metrics:
  # Core metrics (always run)
  - f1
  - exact_match
  
  # Semantic metrics (run after generation)
  - bertscore
  - bleurt
  
  # LLM-as-judge (final evaluation)
  - llm_judge

llm_judge:
  model: openai:gpt-4o-mini
  type: binary  # CORRECT/INCORRECT

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/comprehensive_baseline

# ============================================================================
# Execution options
# ============================================================================
options:
  # Batch size for HF models
  batch_size: 4
  
  # Save predictions after each experiment (for recovery)
  save_intermediate: true
  
  # Skip if output already exists
  skip_existing: true
  
  # GPU memory optimization for HF models
  quantization: 4bit  # 4bit, 8bit, or none

