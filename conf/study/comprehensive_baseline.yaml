# Comprehensive Baseline Study
# Comparing models across families, sizes, and generations
#
# Run: uv run ragicamp run conf/study/comprehensive_baseline.yaml
#
# Comparison Axes:
#   - Model sizes: SLM (1-3B) vs Medium (7-9B) vs Large (32-70B)
#   - Providers: Google, Meta, Mistral, Qwen, DeepSeek
#   - RAG vs Direct: With and without retrieval
#   - Retrieval strategies: Dense, Hybrid, Hierarchical
#   - Query transformation: None, HyDE, Multi-query
#
# Hardware: A100 40GB - can run 70B with 4-bit quantization
#
# Prompt types:
#   - concise: Short answer format
#   - fewshot: 5 dataset-specific in-context examples

name: comprehensive_baseline
description: "Baseline study comparing model families, sizes (SLM to 70B), and RAG strategies"

# 1000 examples per dataset for statistical significance
num_questions: 1000

# All QA datasets (including domain-specific)
datasets:
  - nq          # Natural Questions (Wikipedia-based)
  - triviaqa    # Trivia QA (web documents)
  - hotpotqa    # HotpotQA (multi-hop reasoning)
  - techqa      # TechQA (technical support - IBM)
  - pubmedqa    # PubMedQA (biomedical research)

# ============================================================================
# Unified model list for fair Direct vs RAG comparison
# ============================================================================
# Using YAML anchors to ensure both Direct and RAG use identical models
models: &models
  # --------------------------------------------------------------------------
  # SLMs (1-3B) - Fast, lightweight
  # --------------------------------------------------------------------------
  - hf:google/gemma-2b-it           # Gemma 1, 2B (SLM baseline)
  - hf:meta-llama/Llama-3.2-3B-Instruct  # Llama 3.2, 3B (SLM)
  - hf:Qwen/Qwen2.5-1.5B-Instruct   # Qwen 2.5, 1.5B (SLM)

  # --------------------------------------------------------------------------
  # Medium (7-9B) - Best balance of quality and speed
  # --------------------------------------------------------------------------
  - hf:google/gemma-2-9b-it         # Gemma 2, 9B (much improved)
  - hf:meta-llama/Llama-3.1-8B-Instruct  # Llama 3.1, 8B
  - hf:Qwen/Qwen2.5-7B-Instruct     # Qwen 2.5, 7B
  - hf:mistralai/Mistral-7B-Instruct-v0.3  # Mistral v0.3
  - hf:deepseek-ai/deepseek-llm-7b-chat    # DeepSeek 7B

  # --------------------------------------------------------------------------
  # LARGE MODELS (32-70B) - Uncomment for SOTA comparison
  # Requires 4-bit quantization on A100 40GB
  # --------------------------------------------------------------------------
  #- hf:meta-llama/Llama-3.1-70B-Instruct   # 70B SOTA (~35GB 4-bit)
  #- hf:Qwen/Qwen2.5-32B-Instruct           # 32B (~18GB 4-bit)
  #- hf:mistralai/Mixtral-8x7B-Instruct-v0.1  # MoE 8x7B (~25GB 4-bit)

# ============================================================================
# DirectLLM Experiments (no retrieval - knowledge from pretraining)
# ============================================================================
direct:
  enabled: true
  models: *models  # Same models as RAG for fair comparison

  prompts:
    - concise    # Short answers
    - fewshot    # 5 in-context examples

  quantization:
    - 4bit       # For larger models
    - none       # Full precision for smaller models

# ============================================================================
# RAG Experiments (with retrieval)
# ============================================================================
rag:
  enabled: true
  models: *models  # Same models as Direct for fair comparison

  # --------------------------------------------------------------------------
  # Retriever configurations
  # Testing different retrieval strategies with SOTA embedding models
  # --------------------------------------------------------------------------
  retrievers:
    # Hybrid retrieval (BM25 + Dense) - Best for mixed content
    - type: hybrid
      name: hybrid_bge_large
      embedding_model: BAAI/bge-large-en-v1.5
      alpha: 0.5  # Equal weight to dense and sparse

    # Hierarchical retrieval - Best for long documents
    - type: hierarchical
      name: hierarchical_e5_large
      embedding_model: intfloat/e5-large-v2
      parent_chunk_size: 1024
      child_chunk_size: 256

    # Dense retrieval baseline - Standard approach
    - type: dense
      name: dense_gte_large
      embedding_model: thenlper/gte-large

  # --------------------------------------------------------------------------
  # Query transformation strategies
  # --------------------------------------------------------------------------
  query_transform:
    - none        # Baseline (no transformation)
    - hyde        # HyDE: Generate hypothetical answer, search with that
    - multiquery  # Multi-query: LLM rewrites query into 3 variations

  # --------------------------------------------------------------------------
  # Reranking (cross-encoder for higher accuracy)
  # --------------------------------------------------------------------------
  reranker:
    enabled: true
    model: bge  # BAAI/bge-reranker-large

  top_k_values:
    - 3    # Focused context
    - 5    # More context

  prompts:
    - concise    # Short answers
    - fewshot    # 5 in-context examples

  quantization:
    - 4bit       # For large models (70B, 32B)
    - none       # For medium models (7-9B)

# ============================================================================
# Metrics
# ============================================================================
metrics:
  # Core metrics
  - f1
  - exact_match

  # Semantic metrics
  - bertscore
  - bleurt

  # LLM-as-judge
  - llm_judge

llm_judge:
  model: openai:gpt-4o-mini
  type: binary

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/comprehensive_baseline

# ============================================================================
# Execution options
# ============================================================================
batch_size: 32
min_batch_size: 1

options:
  save_intermediate: true
  skip_existing: true
