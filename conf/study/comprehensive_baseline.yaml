# Comprehensive Baseline Study
# Comparing models across families, sizes, and generations
#
# Run: uv run ragicamp run conf/study/comprehensive_baseline.yaml
#
# Comparison Axes:
#   - Model sizes: SLM (1-3B) vs Medium (7-9B) vs Large (32-70B)
#   - Providers: Google, Meta, Mistral, Qwen, DeepSeek
#   - RAG vs Direct: With and without retrieval
#   - Architectures: Dense vs MoE (Mixtral)
#
# Hardware: A100 40GB - can run 70B with 4-bit quantization
#
# Prompt types:
#   - concise: Short answer format
#   - fewshot: 5 dataset-specific in-context examples

name: comprehensive_baseline
description: "Baseline study comparing model families, sizes (SLM to 70B), and generations"

# Limit to 100 examples per dataset for faster iteration
num_questions: 100

# All three QA datasets
datasets:
  - nq
  - triviaqa
  - hotpotqa

# ============================================================================
# DirectLLM Experiments (no retrieval)
# ============================================================================
direct:
  enabled: true
  models:
    # ----------------------------------------------------------------------
    # OpenAI (API) - Commercial baseline
    # ----------------------------------------------------------------------
    #- openai:gpt-4o-mini          # Fast, cheap, good baseline

    # ----------------------------------------------------------------------
    # Google Gemma - Compare generations (1 vs 2)
    # ----------------------------------------------------------------------
    - hf:google/gemma-2b-it       # Gemma 1, 2B (SLM baseline)
    - hf:google/gemma-2-9b-it     # Gemma 2, 9B (LLM, much improved)

    # ----------------------------------------------------------------------
    # Meta LLaMA - Compare sizes (3B vs 8B, same generation)
    # ----------------------------------------------------------------------
    - hf:meta-llama/Llama-3.2-3B-Instruct # Llama 3.2, 3B (SLM)
    - hf:meta-llama/Llama-3.1-8B-Instruct # Llama 3.1, 8B (LLM)

    # ----------------------------------------------------------------------
    # Mistral - Compare versions
    # ----------------------------------------------------------------------
    - hf:mistralai/Mistral-7B-Instruct-v0.2  # v0.2 (older)
    - hf:mistralai/Mistral-7B-Instruct-v0.3  # v0.3 (current)

    # ----------------------------------------------------------------------
    # Qwen - Compare sizes (SLM vs LLM)
    # ----------------------------------------------------------------------
    - hf:Qwen/Qwen2.5-1.5B-Instruct  # 1.5B (SLM)
    - hf:Qwen/Qwen2.5-7B-Instruct    # 7B (LLM)

    # ----------------------------------------------------------------------
    # DeepSeek - Strong reasoning models
    # ----------------------------------------------------------------------
    - hf:deepseek-ai/deepseek-llm-7b-chat  # 7B chat model

    # ----------------------------------------------------------------------
    # Falcon - Historical reference (older architecture)
    # ----------------------------------------------------------------------
    - hf:tiiuae/falcon-7b-instruct  # 7B, older but interesting comparison

    # ----------------------------------------------------------------------
    # LARGE MODELS (32-70B) - Requires 4-bit quantization
    # These run on A100 40GB with 4-bit
    # ----------------------------------------------------------------------
    - hf:meta-llama/Llama-3.1-70B-Instruct   # 70B, SOTA open model (~35GB 4-bit)
    - hf:Qwen/Qwen2.5-32B-Instruct           # 32B, very strong (~18GB 4-bit)
    - hf:mistralai/Mixtral-8x7B-Instruct-v0.1  # MoE 8x7B (~25GB 4-bit)

  prompts:
    - concise    # Short answers
    - fewshot    # 5 in-context examples

  quantization:
    - 4bit       # Required for large models (70B, 32B)
    - none       # Full precision for smaller models

# ============================================================================
# RAG Experiments (with retrieval)
# ============================================================================
rag:
  enabled: true

  models:
    # API baseline
    #- openai:gpt-4o-mini

    # Medium models (7-9B) - Best from each family
    - hf:google/gemma-2-9b-it             # Google - best Gemma
    - hf:meta-llama/Llama-3.1-8B-Instruct # Meta - best Llama  
    - hf:Qwen/Qwen2.5-7B-Instruct         # Alibaba - best Qwen
    - hf:mistralai/Mistral-7B-Instruct-v0.3  # Mistral - latest
    - hf:deepseek-ai/deepseek-llm-7b-chat    # DeepSeek - reasoning

    # Large models (for RAG quality comparison)
    - hf:meta-llama/Llama-3.1-70B-Instruct   # 70B SOTA (4-bit)
    - hf:Qwen/Qwen2.5-32B-Instruct           # 32B strong (4-bit)

  retrievers:
    # One representative from each embedding family
    - simple_minilm_recursive_1024   # MiniLM (fast, 384d)
    - simple_mpnet_recursive_512     # MPNet (balanced, 768d)
    - simple_e5_recursive_512        # E5 (SOTA, 384d)

  top_k_values:
    - 3    # Focused context

  prompts:
    - concise    # Short answers
    - fewshot    # 5 in-context examples

  quantization:
    - 4bit       # For large models (70B, 32B)
    - none       # For medium models (7-9B)

# ============================================================================
# Metrics
# ============================================================================
metrics:
  # Core metrics
  - f1
  - exact_match

  # Semantic metrics
  - bertscore
  - bleurt

  # LLM-as-judge
  - llm_judge

llm_judge:
  model: openai:gpt-5-mini
  type: binary

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/comprehensive_baseline

# ============================================================================
# Execution options
# ============================================================================
batch_size: 32
min_batch_size: 1

options:
  save_intermediate: true
  skip_existing: true
