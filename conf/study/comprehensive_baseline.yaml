# Comprehensive Baseline Study
# Full baseline with multiple HF models, OpenAI, and corpus variations
#
# Run: python scripts/experiments/run_study.py conf/study/comprehensive_baseline.yaml
#
# Experiment matrix (with quantization variations):
#   DirectLLM: 4 HF models × 4 prompts × 2 quant × 3 datasets = 96 experiments
#   RAG: 1 HF model × 6 retrievers × 3 top_k × 2 prompts × 2 quant × 3 datasets = 216 experiments
#        + 1 OpenAI × 6 retrievers × 3 top_k × 2 prompts × 3 datasets = 108 experiments
#   Total: ~420 experiments
#
# Prompt types:
#   - default: Standard QA prompt
#   - concise: Short answer format
#   - fewshot: 5 dataset-specific in-context examples
#   - fewshot_3: 3 dataset-specific in-context examples
#
# Quantization options (HF models only):
#   - 4bit: Faster, less VRAM (~6GB for 7B model)
#   - none: Full precision, slower, more VRAM (~14GB for 7B model)
#
# With 100 questions per dataset:
#   - 100 questions × ~420 experiments = ~42,000 total inferences
#   - Estimated time: 4-8 hours (depending on models/hardware)

name: comprehensive_baseline
description: "Full baseline study with HF + OpenAI models and multiple retrieval configurations"

# Limit to 100 examples per dataset for faster iteration
num_questions: 100  # null = full dataset, or set a cap for testing

# All three QA datasets
datasets:
  - nq
  #- triviaqa
  - hotpotqa

# ============================================================================
# DirectLLM Experiments (no retrieval)
# ============================================================================
direct:
  enabled: true
  models:
    # --- OpenAI (high quality, costs money) ---
    #- openai:gpt-4o-mini       # Fast, cheap, good baseline
    #- openai:gpt-4o            # SOTA, expensive but comprehensive
    
    # --- HuggingFace models (local, free) ---
    # Small models (fast, good for baseline)
    - hf:google/gemma-2b-it           # 2B params, good quality
    - hf:microsoft/phi-3-mini-4k-instruct  # 3.8B params, strong for size
    
    # Medium models (need more VRAM, better quality)
    - hf:meta-llama/Llama-3.2-3B-Instruct  # 3B Llama, good instruction following
    - hf:mistralai/Mistral-7B-Instruct-v0.3  # 7B, excellent quality
    
    # Large models (optional, need 24GB+ VRAM)
    # - hf:meta-llama/Llama-3.1-8B-Instruct  # 8B, very strong
    # - hf:Qwen/Qwen2.5-7B-Instruct          # 7B, multilingual
    
  prompts:
    - default    # Standard QA prompt
    - concise    # "Answer briefly"
    - fewshot    # 5 dataset-specific examples (few-shot learning)
    #- fewshot_3  # 3 dataset-specific examples
  
  # Quantization options for HF models (OpenAI is unaffected)
  quantization:
    - 4bit       # 4-bit quantization (faster, less VRAM)
    - none       # Full precision (slower, more VRAM, potentially better quality)

# ============================================================================
# RAG Experiments (with retrieval)
# ============================================================================
rag:
  enabled: true
  
  # Models for RAG (keep this smaller - focus on best models)
  models:
    #- openai:gpt-4o-mini   # Good balance of quality/cost
    - hf:google/gemma-2b-it  # Representative local model
  
  # Retriever variations (6 indexes on Simple Wikipedia)
  # Format: {corpus}_{embedding}_{strategy}_{chunk_size}
  retrievers:
    # MiniLM embeddings (fast, 384 dims)
    #- simple_minilm_recursive_512      # Small chunks
    - simple_minilm_recursive_1024     # Larger chunks
    
    # MPNet embeddings (better quality, 768 dims)
    - simple_mpnet_recursive_512       # Small chunks
    #- simple_mpnet_recursive_1024      # Larger chunks
    
    # E5 embeddings (SOTA, 384 dims)
    - simple_e5_recursive_512          # Small chunks
    #- simple_e5_recursive_1024         # Larger chunks
  
  # How many docs to retrieve
  top_k_values:
    - 3    # Minimal context
    - 5    # Balanced
    #- 10   # Rich context
  
  # RAG-specific prompts
  prompts:
    - default
    - fewshot    # 5 dataset-specific examples
  
  # Quantization for RAG (only affects HF models)
  quantization:
    - 4bit       # 4-bit quantization
    - none       # Full precision

# ============================================================================
# Metrics
# ============================================================================
metrics:
  # Core metrics (always run)
  - f1
  - exact_match
  
  # Semantic metrics (run after generation)
  - bertscore
  - bleurt
  
  # LLM-as-judge (final evaluation)
  - llm_judge

llm_judge:
  model: openai:gpt-4o-mini
  type: binary  # CORRECT/INCORRECT

# ============================================================================
# Output
# ============================================================================
output_dir: outputs/comprehensive_baseline

# ============================================================================
# Execution options
# ============================================================================
options:
  # Batch size for HF models (not yet implemented in runner - TODO)
  # Keep low for non-quantized models which use more VRAM
  batch_size: 8
  
  # Save predictions after each experiment (for recovery)
  save_intermediate: true
  
  # Skip if output already exists
  skip_existing: true
  
  # GPU memory optimization for HF models
  quantization: 4bit  # 4bit, 8bit, or none

# Note: To actually use batch inference, run_study.py needs to be updated
# to use model.batch_generate() instead of processing one question at a time.

