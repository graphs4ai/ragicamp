# @package _global_
# Comprehensive DirectLLM Study
# 
# Tests DirectLLM (no retrieval) across:
# - Multiple models (Gemma 2B, Llama 3 8B, OpenAI GPT-4o-mini)
# - Multiple prompts (concise, sentence, explained)
# - All datasets (NQ, TriviaQA, HotpotQA)
#
# Usage:
#   python -m ragicamp.cli.run experiment=comprehensive_direct
#
# Full sweep (18 runs = 2 models × 3 datasets × 3 prompts):
#   python scripts/experiments/run_comprehensive_study.py --direct-only

defaults:
  - override /model: gemma_2b_4bit
  - override /dataset: nq
  - override /agent: direct_llm
  - override /metrics: study_core
  - override /evaluation: study_100
  - override /prompt: concise
  - override /judge: gpt4_mini
  - override /mlflow: default

# Limit to 100 examples
dataset:
  num_examples: 100

# Apply prompt to agent
agent:
  system_prompt: ${prompt.system_prompt}

# Experiment metadata for MLflow
experiment:
  name: comprehensive_direct_${model.model_name}_${dataset.name}_${prompt.style}
  description: "DirectLLM: ${model.model_name} on ${dataset.name} with ${prompt.style} prompt"
  tags:
    - comprehensive_study
    - direct_llm
    - ${dataset.name}
    - ${model.model_name}
    - ${prompt.style}

# Reproducibility
seed: 42
