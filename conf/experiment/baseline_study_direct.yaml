# @package _global_
# Baseline Study: DirectLLM across datasets
# 
# Usage (single run):
#   python -m ragicamp.cli.run experiment=baseline_study_direct
#
# Usage (full sweep):
#   python -m ragicamp.cli.run --multirun \
#     experiment=baseline_study_direct \
#     model=gemma_2b_4bit,phi3,llama3_8b \
#     dataset=nq,triviaqa,hotpotqa \
#     prompt=concise,sentence,explained

defaults:
  - override /model: gemma_2b_4bit
  - override /dataset: nq
  - override /agent: direct_llm
  - override /metrics: baseline
  - override /evaluation: standard
  - override /prompt: concise
  - override /judge: gpt4_mini
  - override /mlflow: default

# Apply prompt to agent
agent:
  system_prompt: ${prompt.system_prompt}

# Experiment metadata
experiment:
  name: baseline_study_direct
  description: "DirectLLM baseline across datasets"
  tags:
    - baseline_study
    - direct_llm
    - ${dataset.name}
    - ${model.model_name}
    - ${prompt.style}

# Reproducibility
seed: 42
