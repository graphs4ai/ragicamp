{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB03: Component Analysis\n",
    "\n",
    "**Question:** Which RAG knobs matter most? Optimal values? Interactions?\n",
    "\n",
    "This notebook analyzes RAG component effects:\n",
    "- Variance decomposition (which factors explain the most performance variance)\n",
    "- Marginal effects of each component\n",
    "- Prompt and top-K deep dives\n",
    "- Interaction effects between components\n",
    "- Optimal configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as scipy_stats\n\nfrom analysis_utils import (\n    load_all_results, setup_plotting, identify_bottlenecks,\n    compute_marginal_means, plot_component_effects,\n    plot_interaction_heatmap, find_synergistic_combinations,\n    weighted_mean_with_ci, multi_metric_bottlenecks_df,\n    metric_correlation_matrix,\n    PRIMARY_METRIC, BROKEN_MODELS,\n    MULTI_METRIC_SET, CORRECTNESS_METRICS, CONTEXT_METRICS,\n    GROUNDEDNESS_METRICS,\n)\n\nsetup_plotting()\nSTUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n\ndf_all = load_all_results(STUDY_PATH)\ndf = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n\n# Focus on RAG experiments only\nrag = df[df['exp_type'] == 'rag'].copy()\nprint(f\"RAG experiments: {len(rag)} (from {len(df)} total, {len(df_all)} before broken-model filter)\")\n\n# Check which metrics have data\navailable_metrics = [m for m in MULTI_METRIC_SET if m in rag.columns and rag[m].notna().sum() >= 10]\nprint(f\"\\nMetrics with sufficient data for multi-metric analysis:\")\nfor m in available_metrics:\n    n = rag[m].notna().sum()\n    print(f\"  {m:<20s}: {n:>4d}/{len(rag)} ({n/len(rag)*100:.0f}%)\")\n\n# Context-aware metric availability flags (used across multiple sections)\nhas_faith = 'faithfulness' in rag.columns and rag['faithfulness'].notna().sum() >= 10\nhas_halluc = 'hallucination' in rag.columns and rag['hallucination'].notna().sum() >= 10\nhas_ctx_recall = 'context_recall' in rag.columns and rag['context_recall'].notna().sum() >= 10\nhas_aic = 'answer_in_context' in rag.columns and rag['answer_in_context'].notna().sum() >= 10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variance Decomposition\n",
    "\n",
    "The single most important thesis figure: which factors explain the most F1 variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottlenecks = identify_bottlenecks(df, PRIMARY_METRIC)\n",
    "\n",
    "if bottlenecks:\n",
    "    print(\"Variance Explained by Factor (%)\")\n",
    "    print(\"=\" * 50)\n",
    "    for factor, pct in bottlenecks.items():\n",
    "        bar = '#' * int(pct / 2)\n",
    "        print(f\"  {factor:<20s}: {pct:5.1f}%  {bar}\")\n",
    "\n",
    "    # Horizontal bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    factors = list(bottlenecks.keys())\n",
    "    values = list(bottlenecks.values())\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(factors)))\n",
    "    ax.barh(factors[::-1], values[::-1], color=colors[::-1], edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Variance Explained (%)')\n",
    "    ax.set_title('RAG Component Variance Decomposition')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    for i, v in enumerate(values[::-1]):\n",
    "        ax.text(v + 0.3, i, f'{v:.1f}%', va='center', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Multi-Metric Variance Decomposition\n\n**Key thesis question:** Do different quality dimensions depend on the same components?\n\nIf reranker dominates F1 but not faithfulness, it means reranking improves correctness\nwithout improving groundedness. If model_short dominates everything,\nretrieval investment is less impactful regardless of what we measure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multi-metric variance decomposition\nmm_df = multi_metric_bottlenecks_df(df, metrics=available_metrics)\n\nif not mm_df.empty:\n    # Pivot for grouped bar chart: factors as rows, metrics as columns\n    mm_pivot = mm_df.pivot(index='factor', columns='metric', values='variance_pct').fillna(0)\n\n    # Sort factors by F1 variance (or first available metric)\n    sort_metric = PRIMARY_METRIC if PRIMARY_METRIC in mm_pivot.columns else mm_pivot.columns[0]\n    mm_pivot = mm_pivot.sort_values(sort_metric, ascending=True)\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    mm_pivot.plot(kind='barh', ax=ax, width=0.8, edgecolor='black', linewidth=0.3)\n    ax.set_xlabel('Variance Explained (%)')\n    ax.set_title('Which Components Matter — By Metric')\n    ax.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left')\n    ax.grid(axis='x', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    # Print the table\n    print(\"\\nVariance Explained (%) by Factor and Metric:\")\n    print(\"=\" * 70)\n    display(mm_pivot.round(1))\n\n    # Highlight divergences: where do metrics disagree about what matters?\n    print(\"\\nKey divergences (factor importance differs >5pp across metrics):\")\n    for factor in mm_pivot.index:\n        vals = mm_pivot.loc[factor]\n        spread = vals.max() - vals.min()\n        if spread > 5:\n            top_metric = vals.idxmax()\n            low_metric = vals.idxmin()\n            print(f\"  {factor}: {top_metric}={vals[top_metric]:.1f}% vs \"\n                  f\"{low_metric}={vals[low_metric]:.1f}% (spread={spread:.1f}pp)\")\nelse:\n    print(\"Not enough metric data for multi-metric variance decomposition.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Marginal Effects\n",
    "\n",
    "Marginal mean of each factor level, controlling for model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n",
    "           'query_transform', 'top_k']\n",
    "# Filter to factors present with > 1 level\n",
    "factors = [f for f in factors if f in rag.columns and rag[f].nunique() > 1]\n",
    "\n",
    "n_factors = len(factors)\n",
    "ncols = min(3, n_factors)\n",
    "nrows = (n_factors + ncols - 1) // ncols\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 5 * nrows))\n",
    "if n_factors == 1:\n",
    "    axes = np.array([axes])\n",
    "axes = np.atleast_2d(axes)\n",
    "\n",
    "for idx, factor in enumerate(factors):\n",
    "    r, c = divmod(idx, ncols)\n",
    "    ax = axes[r, c]\n",
    "    plot_component_effects(df, factor, PRIMARY_METRIC, ax=ax)\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(n_factors, nrows * ncols):\n",
    "    r, c = divmod(idx, ncols)\n",
    "    axes[r, c].set_visible(False)\n",
    "\n",
    "plt.suptitle('Marginal Effects of RAG Components on F1', y=1.01, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt marginal means with CI\n",
    "prompt_stats = weighted_mean_with_ci(rag, 'prompt', PRIMARY_METRIC)\n",
    "print(\"Prompt Performance (mean F1 with 95% CI):\")\n",
    "display(prompt_stats.round(4))\n",
    "\n",
    "# Prompt x Dataset heatmap\n",
    "prompt_ds = rag.groupby(['prompt', 'dataset'])[PRIMARY_METRIC].mean().unstack()\n",
    "if not prompt_ds.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar chart\n",
    "    x = range(len(prompt_stats))\n",
    "    yerr_low = np.maximum(prompt_stats['mean'] - prompt_stats['ci_low'], 0)\n",
    "    yerr_high = np.maximum(prompt_stats['ci_high'] - prompt_stats['mean'], 0)\n",
    "    axes[0].bar(x, prompt_stats['mean'], yerr=[yerr_low, yerr_high],\n",
    "               capsize=4, alpha=0.8, color='steelblue')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(prompt_stats['prompt'], rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Mean F1')\n",
    "    axes[0].set_title('Prompt Performance')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Heatmap\n",
    "    sns.heatmap(prompt_ds, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('Prompt x Dataset (Mean F1)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-K Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'top_k' in rag.columns and rag['top_k'].nunique() > 1:\n",
    "    topk_data = rag.dropna(subset=['top_k', PRIMARY_METRIC])\n",
    "    retriever_types = sorted(topk_data['retriever_type'].dropna().unique())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for rt in retriever_types:\n",
    "        sub = topk_data[topk_data['retriever_type'] == rt]\n",
    "        means = sub.groupby('top_k')[PRIMARY_METRIC].agg(['mean', 'std', 'count'])\n",
    "        means = means.sort_index()\n",
    "        ci = 1.96 * means['std'] / np.sqrt(means['count'])\n",
    "        ax.plot(means.index, means['mean'], marker='o', label=rt)\n",
    "        ax.fill_between(means.index, means['mean'] - ci, means['mean'] + ci, alpha=0.15)\n",
    "\n",
    "    ax.set_xlabel('Top-K')\n",
    "    ax.set_ylabel('Mean F1')\n",
    "    ax.set_title('F1 vs Top-K by Retriever Type')\n",
    "    ax.legend(title='Retriever')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Top-K has <= 1 unique value; skipping sensitivity plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_pairs = [\n",
    "    ('retriever_type', 'reranker'),\n",
    "    ('prompt', 'query_transform'),\n",
    "    ('retriever_type', 'embedding_model'),\n",
    "    ('reranker', 'query_transform'),\n",
    "]\n",
    "# Filter to pairs where both factors have > 1 level\n",
    "interaction_pairs = [(f1, f2) for f1, f2 in interaction_pairs\n",
    "                     if f1 in rag.columns and f2 in rag.columns\n",
    "                     and rag[f1].nunique() > 1 and rag[f2].nunique() > 1]\n",
    "\n",
    "n_pairs = len(interaction_pairs)\n",
    "if n_pairs > 0:\n",
    "    ncols = min(2, n_pairs)\n",
    "    nrows = (n_pairs + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 5 * nrows))\n",
    "    if n_pairs == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes_flat = np.atleast_1d(axes).flatten()\n",
    "\n",
    "    for idx, (f1, f2) in enumerate(interaction_pairs):\n",
    "        plot_interaction_heatmap(df, f1, f2, PRIMARY_METRIC, ax=axes_flat[idx])\n",
    "\n",
    "    for idx in range(n_pairs, len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Component Interaction Heatmaps', y=1.01, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough factor pairs with multiple levels for interaction analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synergistic / redundant combos\n",
    "for f1, f2 in interaction_pairs:\n",
    "    combos = find_synergistic_combinations(df, f1, f2, PRIMARY_METRIC)\n",
    "    if combos:\n",
    "        combo_df = pd.DataFrame(combos)\n",
    "        syn = combo_df[combo_df['synergy'] == 'Synergistic']\n",
    "        red = combo_df[combo_df['synergy'] == 'Redundant']\n",
    "        print(f\"\\n{f1} x {f2}:\")\n",
    "        if len(syn) > 0:\n",
    "            print(f\"  Synergistic ({len(syn)}):\")\n",
    "            for _, r in syn.head(3).iterrows():\n",
    "                print(f\"    {r[f1]} + {r[f2]}: interaction = +{r['interaction_effect']:.4f}\")\n",
    "        if len(red) > 0:\n",
    "            print(f\"  Redundant ({len(red)}):\")\n",
    "            for _, r in red.head(3).iterrows():\n",
    "                print(f\"    {r[f1]} + {r[f2]}: interaction = {r['interaction_effect']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 5a. Dataset-Conditional Interaction Effects\n\n**Key question:** Do interaction patterns hold across all datasets, or are synergies dataset-specific?\n\nIf reranker x retriever synergy only appears on HotpotQA (multi-hop), it suggests\nthe synergy is driven by question complexity rather than being universal.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dataset-conditional interaction effects\n# Show the top 2 most important interaction pairs broken out by dataset\nds_list = sorted(rag['dataset'].unique())\ntop_pairs = interaction_pairs[:2]  # Use same pairs from section 5\n\nif len(ds_list) >= 2 and len(top_pairs) >= 1:\n    for f1, f2 in top_pairs:\n        n_ds = len(ds_list)\n        fig, axes = plt.subplots(1, n_ds, figsize=(6 * n_ds, 5))\n        if n_ds == 1:\n            axes = [axes]\n\n        for i, ds in enumerate(ds_list):\n            ds_rag = rag[rag['dataset'] == ds]\n            if ds_rag[f1].nunique() > 1 and ds_rag[f2].nunique() > 1:\n                pivot = ds_rag.groupby([f1, f2])[PRIMARY_METRIC].mean().unstack()\n                sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn',\n                            ax=axes[i], linewidths=0.5)\n            axes[i].set_title(f'{ds}')\n\n        plt.suptitle(f'{f1} x {f2} — Interaction by Dataset', y=1.02, fontsize=13)\n        plt.tight_layout()\n        plt.show()\n\n    # Quantify: does the interaction effect change sign or magnitude across datasets?\n    print(\"\\nInteraction Stability Across Datasets:\")\n    print(\"=\" * 60)\n    for f1, f2 in top_pairs:\n        print(f\"\\n  {f1} x {f2}:\")\n        ds_effects = {}\n        for ds in ds_list:\n            ds_rag = rag[rag['dataset'] == ds]\n            combos = find_synergistic_combinations(ds_rag, f1, f2, PRIMARY_METRIC)\n            if combos:\n                combo_df = pd.DataFrame(combos)\n                ds_effects[ds] = combo_df.set_index([f1, f2])['interaction_effect']\n\n        if len(ds_effects) >= 2:\n            # Find combos present in all datasets\n            all_idx = set.intersection(*[set(e.index) for e in ds_effects.values()])\n            if all_idx:\n                effect_df = pd.DataFrame({ds: e.loc[list(all_idx)]\n                                          for ds, e in ds_effects.items()})\n                effect_df['sign_stable'] = (effect_df[ds_list] > 0).all(axis=1) | \\\n                                           (effect_df[ds_list] < 0).all(axis=1)\n                n_stable = effect_df['sign_stable'].sum()\n                n_total = len(effect_df)\n                print(f\"    {n_stable}/{n_total} combos have same-sign interaction across all datasets\")\n\n                # Show top combos where sign FLIPS\n                unstable = effect_df[~effect_df['sign_stable']].copy()\n                if not unstable.empty:\n                    unstable['spread'] = unstable[ds_list].max(axis=1) - unstable[ds_list].min(axis=1)\n                    print(f\"    Dataset-sensitive combos (sign flips):\")\n                    for idx, row in unstable.nlargest(3, 'spread').iterrows():\n                        vals = ', '.join(f\"{ds}={row[ds]:+.4f}\" for ds in ds_list)\n                        print(f\"      {idx[0]}+{idx[1]}: {vals}\")\n                else:\n                    print(f\"    All interaction effects are sign-stable across datasets!\")\nelse:\n    print(\"Need >= 2 datasets and >= 1 interaction pair for conditional analysis.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5c. Retrieval Quality Pipeline: Context Recall → F1\n\n**Key question:** Is retrieval quality a sufficient predictor of downstream answer quality?\n\nIf context_recall strongly predicts F1, the pipeline is retrieval-bottlenecked.\nIf the correlation is weak, generation quality matters independently.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if has_ctx_recall or has_aic:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Context Recall vs F1\n    if has_ctx_recall:\n        cr_df = rag.dropna(subset=[PRIMARY_METRIC, 'context_recall'])\n        ax = axes[0]\n        for ds in sorted(cr_df['dataset'].unique()):\n            sub = cr_df[cr_df['dataset'] == ds]\n            ax.scatter(sub['context_recall'], sub[PRIMARY_METRIC],\n                       s=20, alpha=0.4, label=ds)\n        rho, p = scipy_stats.spearmanr(cr_df['context_recall'], cr_df[PRIMARY_METRIC])\n        # Regression line\n        z = np.polyfit(cr_df['context_recall'], cr_df[PRIMARY_METRIC], 1)\n        x_line = np.linspace(cr_df['context_recall'].min(), cr_df['context_recall'].max(), 50)\n        ax.plot(x_line, np.polyval(z, x_line), 'k--', alpha=0.5)\n        ax.set_xlabel('Context Recall')\n        ax.set_ylabel('F1')\n        ax.set_title(f'Context Recall → F1 (rho={rho:.3f}, p={p:.2e})')\n        ax.legend()\n        ax.grid(alpha=0.2)\n\n    # Answer-in-Context vs F1\n    if has_aic:\n        aic_df = rag.dropna(subset=[PRIMARY_METRIC, 'answer_in_context'])\n        ax = axes[1]\n        # Box plots: F1 distribution when answer IS vs IS NOT in context\n        ctx_yes = aic_df[aic_df['answer_in_context'] >= 0.5][PRIMARY_METRIC]\n        ctx_no = aic_df[aic_df['answer_in_context'] < 0.5][PRIMARY_METRIC]\n        bp = ax.boxplot([ctx_no.values, ctx_yes.values],\n                        labels=['No answer\\nin context', 'Answer\\nin context'],\n                        patch_artist=True)\n        bp['boxes'][0].set_facecolor('#e74c3c')\n        bp['boxes'][1].set_facecolor('#2ecc71')\n        ax.set_ylabel('F1')\n        ax.set_title(f'F1 by Answer-in-Context\\n'\n                     f'(yes: {ctx_yes.mean():.3f}, no: {ctx_no.mean():.3f}, '\n                     f'gap: {ctx_yes.mean() - ctx_no.mean():.3f})')\n        ax.grid(axis='y', alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Component impact on context_recall specifically\n    if has_ctx_recall:\n        print(\"\\nContext Recall by RAG Component (marginal means):\")\n        print(\"=\" * 60)\n        for factor in ['retriever_type', 'reranker', 'top_k', 'embedding_model']:\n            if factor in cr_df.columns and cr_df[factor].nunique() > 1:\n                means = cr_df.groupby(factor)['context_recall'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n                print(f\"\\n  {factor}:\")\n                for level, row in means.iterrows():\n                    print(f\"    {level:<20s}: {row['mean']:.3f} (n={int(row['count'])})\")\nelse:\n    print(\"Context-aware metrics not available. Run:\")\n    print(\"  uv run ragicamp compute-metrics outputs/smart_retrieval_slm -m answer_in_context,context_recall\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Faithfulness vs Correctness Trade-off\n\n**Key thesis insight:** Are correct answers grounded in context, or are models hallucinating right answers?\n\n- **Top-right** (high F1 + high faithfulness): Ideal — correct AND grounded\n- **Bottom-right** (high F1 + low faithfulness): Correct but ungrounded — lucky hallucination\n- **Top-left** (low F1 + high faithfulness): Faithful but wrong — retrieval failure",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if has_faith:\n    faith_df = rag.dropna(subset=[PRIMARY_METRIC, 'faithfulness'])\n\n    fig, axes = plt.subplots(1, 2 if has_halluc else 1,\n                             figsize=(14 if has_halluc else 8, 6))\n    if not has_halluc:\n        axes = [axes]\n\n    # Faithfulness vs F1 scatter\n    ax = axes[0]\n    for model in sorted(faith_df['model_short'].unique()):\n        sub = faith_df[faith_df['model_short'] == model]\n        ax.scatter(sub[PRIMARY_METRIC], sub['faithfulness'],\n                   s=25, alpha=0.5, label=model)\n\n    ax.axhline(y=0.5, color='grey', ls='--', alpha=0.4)\n    ax.axvline(x=faith_df[PRIMARY_METRIC].median(), color='grey', ls='--', alpha=0.4)\n    ax.set_xlabel('F1 (Correctness)')\n    ax.set_ylabel('Faithfulness (Groundedness)')\n    ax.grid(alpha=0.2)\n\n    # Annotate quadrants\n    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n    ax.text(xlim[1] * 0.95, ylim[1] * 0.95, 'Ideal', ha='right', va='top',\n            fontsize=9, color='green', alpha=0.7)\n    ax.text(xlim[0] + 0.01, ylim[1] * 0.95, 'Faithful\\nbut wrong', ha='left', va='top',\n            fontsize=9, color='orange', alpha=0.7)\n    ax.text(xlim[1] * 0.95, ylim[0] + 0.02, 'Lucky\\nhallucination', ha='right', va='bottom',\n            fontsize=9, color='red', alpha=0.7)\n\n    rho, p = scipy_stats.spearmanr(faith_df[PRIMARY_METRIC], faith_df['faithfulness'])\n    ax.set_title(f'Correctness vs Faithfulness (rho={rho:.3f}, p={p:.2e})')\n    ax.legend(fontsize=8, loc='lower right')\n\n    if has_halluc:\n        ax2 = axes[1]\n        halluc_df = rag.dropna(subset=[PRIMARY_METRIC, 'hallucination'])\n        for model in sorted(halluc_df['model_short'].unique()):\n            sub = halluc_df[halluc_df['model_short'] == model]\n            ax2.scatter(sub[PRIMARY_METRIC], sub['hallucination'],\n                       s=25, alpha=0.5, label=model)\n        rho2, _ = scipy_stats.spearmanr(halluc_df[PRIMARY_METRIC], halluc_df['hallucination'])\n        ax2.set_xlabel('F1 (Correctness)')\n        ax2.set_ylabel('Hallucination Score (lower is better)')\n        ax2.set_title(f'Correctness vs Hallucination (rho={rho2:.3f})')\n        ax2.legend(fontsize=8, loc='upper right')\n        ax2.grid(alpha=0.2)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Per-component faithfulness breakdown\n    print(\"\\nFaithfulness by RAG Component (mean):\")\n    print(\"=\" * 60)\n    for factor in ['reranker', 'retriever_type', 'prompt', 'query_transform', 'agent_type']:\n        if factor in faith_df.columns and faith_df[factor].nunique() > 1:\n            means = faith_df.groupby(factor)['faithfulness'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n            print(f\"\\n  {factor}:\")\n            for level, row in means.iterrows():\n                print(f\"    {level:<20s}: {row['mean']:.3f} (n={int(row['count'])})\")\nelse:\n    print(\"Faithfulness metric not available. Run:\")\n    print(\"  uv run ragicamp compute-metrics outputs/smart_retrieval_slm -m faithfulness,hallucination\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimal Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cols = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n",
    "               'query_transform', 'top_k', 'agent_type']\n",
    "config_cols = [c for c in config_cols if c in rag.columns]\n",
    "\n",
    "# Best config per dataset\n",
    "print(\"Best Configuration per Dataset:\")\n",
    "print(\"=\" * 60)\n",
    "for ds in sorted(rag['dataset'].unique()):\n",
    "    ds_df = rag[rag['dataset'] == ds]\n",
    "    if ds_df[PRIMARY_METRIC].notna().sum() == 0:\n",
    "        continue\n",
    "    best_idx = ds_df[PRIMARY_METRIC].idxmax()\n",
    "    best = ds_df.loc[best_idx]\n",
    "    print(f\"\\n  {ds} (F1={best[PRIMARY_METRIC]:.4f}):\")\n",
    "    for c in config_cols:\n",
    "        print(f\"    {c:<20s}: {best.get(c, 'n/a')}\")\n",
    "\n",
    "# Best config per model\n",
    "print(\"\\n\\nBest Configuration per Model:\")\n",
    "print(\"=\" * 60)\n",
    "for model in sorted(rag['model_short'].unique()):\n",
    "    m_df = rag[rag['model_short'] == model]\n",
    "    if m_df[PRIMARY_METRIC].notna().sum() == 0:\n",
    "        continue\n",
    "    best_idx = m_df[PRIMARY_METRIC].idxmax()\n",
    "    best = m_df.loc[best_idx]\n",
    "    print(f\"\\n  {model} (F1={best[PRIMARY_METRIC]:.4f}):\")\n",
    "    for c in config_cols:\n",
    "        print(f\"    {c:<20s}: {best.get(c, 'n/a')}\")\n",
    "\n",
    "# \"Universal recipe\" — most common values in top-10% of experiments\n",
    "top_pct = rag.nlargest(max(1, len(rag) // 10), PRIMARY_METRIC)\n",
    "print(\"\\n\\nUniversal Recipe (mode of top-10% experiments):\")\n",
    "print(\"=\" * 60)\n",
    "for c in config_cols:\n",
    "    if c in top_pct.columns:\n",
    "        mode = top_pct[c].mode()\n",
    "        print(f\"  {c:<20s}: {mode.iloc[0] if len(mode) > 0 else 'n/a'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Cross-Dataset Generalization\n\n**Key thesis question:** Does the best configuration on one dataset work well on others?\n\nIf configs generalize, we can recommend a \"universal recipe.\"\nIf they don't, dataset-specific tuning is essential — and the search space\neffectively multiplies by the number of datasets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cross-dataset generalization: do configs that rank well on one dataset rank well on others?\ndatasets = sorted(rag['dataset'].unique())\n\nif len(datasets) >= 2:\n    # Build a config fingerprint for matching configs across datasets\n    config_cols_gen = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n                       'query_transform', 'top_k', 'agent_type']\n    config_cols_gen = [c for c in config_cols_gen if c in rag.columns]\n\n    # Create config key (string hash of component values)\n    rag_gen = rag.copy()\n    rag_gen['config_key'] = rag_gen[config_cols_gen].astype(str).agg('|'.join, axis=1)\n\n    # For each model, compute per-config mean F1 within each dataset\n    # Then correlate ranks across datasets\n    print(\"Cross-Dataset Rank Correlation (Spearman rho)\")\n    print(\"=\" * 60)\n    print(\"For each model, rank all configs by F1 on dataset A, then check\")\n    print(\"if those ranks predict performance on dataset B.\\n\")\n\n    all_rhos = []\n    for model in sorted(rag_gen['model_short'].unique()):\n        m_df = rag_gen[rag_gen['model_short'] == model]\n        ds_scores = {}\n        for ds in datasets:\n            ds_df = m_df[m_df['dataset'] == ds]\n            if len(ds_df) < 5:\n                continue\n            # Mean F1 per config on this dataset\n            means = ds_df.groupby('config_key')[PRIMARY_METRIC].mean()\n            ds_scores[ds] = means\n\n        if len(ds_scores) < 2:\n            continue\n\n        # Pairwise Spearman correlations between datasets\n        from itertools import combinations as combs\n        for ds_a, ds_b in combs(sorted(ds_scores.keys()), 2):\n            # Align on shared configs\n            shared = ds_scores[ds_a].index.intersection(ds_scores[ds_b].index)\n            if len(shared) < 5:\n                continue\n            rho, p = scipy_stats.spearmanr(ds_scores[ds_a].loc[shared],\n                                           ds_scores[ds_b].loc[shared])\n            all_rhos.append({\n                'model': model, 'dataset_a': ds_a, 'dataset_b': ds_b,\n                'rho': rho, 'p_value': p, 'n_shared': len(shared),\n            })\n            sig = '*' if p < 0.05 else ''\n            print(f\"  {model:<16s} {ds_a} ↔ {ds_b}: rho={rho:+.3f} \"\n                  f\"(p={p:.3f}{sig}, n={len(shared)} shared configs)\")\n\n    rho_df = pd.DataFrame(all_rhos)\n\n    if not rho_df.empty:\n        # Summary: overall generalizability\n        mean_rho = rho_df['rho'].mean()\n        median_rho = rho_df['rho'].median()\n        n_sig = (rho_df['p_value'] < 0.05).sum()\n        n_pos = (rho_df['rho'] > 0).sum()\n        print(f\"\\nOverall generalizability:\")\n        print(f\"  Mean rho:   {mean_rho:+.3f}\")\n        print(f\"  Median rho: {median_rho:+.3f}\")\n        print(f\"  Significant (p<.05): {n_sig}/{len(rho_df)}\")\n        print(f\"  Positive:   {n_pos}/{len(rho_df)}\")\n        if mean_rho > 0.5:\n            print(\"  => Configs GENERALIZE well across datasets\")\n        elif mean_rho > 0.2:\n            print(\"  => MODERATE generalization — some dataset-specific tuning helps\")\n        else:\n            print(\"  => WEAK generalization — dataset-specific configs needed\")\n\n        # Heatmap: per-model mean rho (averaged across dataset pairs)\n        if len(rho_df['model'].unique()) > 1:\n            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n            # Panel 1: rho by model (bar chart)\n            model_rho = rho_df.groupby('model')['rho'].mean().sort_values()\n            colors = ['#2ecc71' if r > 0.3 else '#f39c12' if r > 0 else '#e74c3c'\n                      for r in model_rho.values]\n            ax = axes[0]\n            ax.barh(range(len(model_rho)), model_rho.values, color=colors,\n                    edgecolor='black', linewidth=0.5)\n            ax.set_yticks(range(len(model_rho)))\n            ax.set_yticklabels(model_rho.index)\n            ax.axvline(x=0, color='black', lw=1)\n            ax.set_xlabel('Mean Spearman rho')\n            ax.set_title('Config Generalizability by Model\\n(higher = configs transfer better)')\n            ax.grid(axis='x', alpha=0.3)\n\n            # Panel 2: heatmap of rho per dataset pair, averaged across models\n            pair_rho = rho_df.groupby(['dataset_a', 'dataset_b'])['rho'].mean()\n            # Build symmetric matrix\n            ds_list = sorted(set(rho_df['dataset_a']) | set(rho_df['dataset_b']))\n            rho_matrix = pd.DataFrame(1.0, index=ds_list, columns=ds_list)\n            for (da, db), r in pair_rho.items():\n                rho_matrix.loc[da, db] = r\n                rho_matrix.loc[db, da] = r\n            ax2 = axes[1]\n            sns.heatmap(rho_matrix, annot=True, fmt='.2f', cmap='RdYlGn',\n                        vmin=-1, vmax=1, ax=ax2, square=True)\n            ax2.set_title('Cross-Dataset Config Rank Correlation\\n(averaged across models)')\n\n            plt.tight_layout()\n            plt.show()\n\n    # Identify \"universal\" vs \"specialist\" configs\n    print(\"\\n\\nUniversal vs Specialist Configurations:\")\n    print(\"=\" * 60)\n\n    # For each config that appears on all datasets, compute per-dataset percentile\n    for model in sorted(rag_gen['model_short'].unique()):\n        m_df = rag_gen[rag_gen['model_short'] == model]\n        ds_pcts = {}\n        for ds in datasets:\n            ds_df = m_df[m_df['dataset'] == ds]\n            if len(ds_df) < 5:\n                continue\n            means = ds_df.groupby('config_key')[PRIMARY_METRIC].mean()\n            # Convert to percentile ranks\n            pcts = means.rank(pct=True)\n            ds_pcts[ds] = pcts\n\n        if len(ds_pcts) < 2:\n            continue\n\n        # Find configs present in all datasets\n        shared_keys = set.intersection(*[set(p.index) for p in ds_pcts.values()])\n        if len(shared_keys) < 3:\n            continue\n\n        # Build percentile matrix\n        pct_df = pd.DataFrame({ds: pcts.loc[list(shared_keys)]\n                               for ds, pcts in ds_pcts.items()})\n        pct_df['min_pct'] = pct_df.min(axis=1)\n        pct_df['mean_pct'] = pct_df.mean(axis=1)\n        pct_df['spread'] = pct_df[datasets[:len(ds_pcts)]].max(axis=1) - pct_df[datasets[:len(ds_pcts)]].min(axis=1)\n\n        # Universal: high min_pct (good everywhere)\n        universal = pct_df.nlargest(3, 'min_pct')\n        # Specialist: high spread (great on one, poor on another)\n        specialist = pct_df.nlargest(3, 'spread')\n\n        if not universal.empty:\n            print(f\"\\n  {model} — Top Universal Configs (high everywhere):\")\n            for cfg_key, row in universal.iterrows():\n                # Decode config key\n                parts = cfg_key.split('|')\n                short = '/'.join(str(p) for p in parts[:4])\n                ds_str = ', '.join(f\"{ds}={row.get(ds, 0):.0%}\" for ds in datasets if ds in row)\n                print(f\"    [{short}...] {ds_str}  (min={row['min_pct']:.0%})\")\n\n        if not specialist.empty and specialist['spread'].max() > 0.3:\n            print(f\"  {model} — Top Specialist Configs (dataset-dependent):\")\n            for cfg_key, row in specialist.iterrows():\n                parts = cfg_key.split('|')\n                short = '/'.join(str(p) for p in parts[:4])\n                ds_str = ', '.join(f\"{ds}={row.get(ds, 0):.0%}\" for ds in datasets if ds in row)\n                print(f\"    [{short}...] {ds_str}  (spread={row['spread']:.0%})\")\nelse:\n    print(\"Only 1 dataset found — cross-dataset analysis requires >= 2 datasets.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- Which factor explains the most variance\n",
    "- Best prompt strategy\n",
    "- Optimal top-K range\n",
    "- Synergistic and redundant combinations\n",
    "- Universal vs dataset-specific configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}