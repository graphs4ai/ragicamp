{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Overview\n",
    "\n",
    "**Purpose:** Load experiment data, validate quality, and understand experiment coverage.\n",
    "\n",
    "**Key Questions:**\n",
    "- How many experiments do we have?\n",
    "- What's the distribution across models, datasets, and RAG components?\n",
    "- Are there any data quality issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting,\n",
    "    METRICS, PRIMARY_METRIC, DEFAULT_STUDY_PATH\n",
    ")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setup_plotting()\n",
    "\n",
    "# Load data\n",
    "df = load_all_results()\n",
    "print(f\"Loaded {len(df)} experiments from {DEFAULT_STUDY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    print(\"Data Quality Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check for unknown datasets\n",
    "    unknown_ds = df[df['dataset'] == 'unknown']\n",
    "    if len(unknown_ds) > 0:\n",
    "        print(f\"\\n⚠️ {len(unknown_ds)} experiments with 'unknown' dataset\")\n",
    "    else:\n",
    "        print(\"\\n✅ All experiments have known datasets\")\n",
    "    \n",
    "    # Check for unknown models\n",
    "    unknown_model = df[df['model_short'] == 'unknown']\n",
    "    if len(unknown_model) > 0:\n",
    "        print(f\"⚠️ {len(unknown_model)} experiments with 'unknown' model\")\n",
    "    else:\n",
    "        print(\"✅ All experiments have known models\")\n",
    "    \n",
    "    # Check for missing metrics\n",
    "    missing_metric = df[PRIMARY_METRIC].isna()\n",
    "    if missing_metric.sum() > 0:\n",
    "        print(f\"⚠️ {missing_metric.sum()} experiments missing {PRIMARY_METRIC}\")\n",
    "    else:\n",
    "        print(f\"✅ All experiments have {PRIMARY_METRIC} metric\")\n",
    "    \n",
    "    # Available metrics\n",
    "    available_metrics = [m for m in METRICS if m in df.columns and df[m].notna().sum() > 0]\n",
    "    print(f\"\\nAvailable metrics: {available_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Experiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    print(\"Experiment Distribution\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Experiment types\n",
    "    print(f\"\\nExperiment types: {df['exp_type'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # By key dimensions\n",
    "    dimensions = ['model_short', 'dataset', 'retriever_type', 'embedding_model', \n",
    "                  'query_transform', 'reranker', 'prompt']\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        if dim in df.columns:\n",
    "            counts = df[dim].value_counts()\n",
    "            if len(counts) > 0:\n",
    "                print(f\"\\n{dim}:\")\n",
    "                for val, count in counts.items():\n",
    "                    print(f\"  {val}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration Coverage Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    if len(rag_df) > 0 and 'model_short' in rag_df.columns and 'dataset' in rag_df.columns:\n",
    "        # Model × Dataset coverage\n",
    "        coverage = rag_df.groupby(['model_short', 'dataset']).size().unstack(fill_value=0)\n",
    "        \n",
    "        print(\"RAG Experiments: Model × Dataset Coverage\")\n",
    "        print(\"=\" * 60)\n",
    "        display(coverage)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        im = ax.imshow(coverage.values, cmap='YlGn', aspect='auto')\n",
    "        \n",
    "        ax.set_xticks(range(len(coverage.columns)))\n",
    "        ax.set_xticklabels(coverage.columns)\n",
    "        ax.set_yticks(range(len(coverage.index)))\n",
    "        ax.set_yticklabels(coverage.index)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(coverage.index)):\n",
    "            for j in range(len(coverage.columns)):\n",
    "                ax.text(j, i, coverage.values[i, j], ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        ax.set_title('Number of RAG Experiments per Model × Dataset')\n",
    "        plt.colorbar(im, ax=ax, label='Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Performance Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    print(f\"Performance Overview ({PRIMARY_METRIC})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Summary statistics\n",
    "    for exp_type in ['direct', 'rag']:\n",
    "        subset = df[df['exp_type'] == exp_type][PRIMARY_METRIC].dropna()\n",
    "        if len(subset) > 0:\n",
    "            print(f\"\\n{exp_type.upper()}:\")\n",
    "            print(f\"  Count: {len(subset)}\")\n",
    "            print(f\"  Mean:  {subset.mean():.4f}\")\n",
    "            print(f\"  Std:   {subset.std():.4f}\")\n",
    "            print(f\"  Min:   {subset.min():.4f}\")\n",
    "            print(f\"  Max:   {subset.max():.4f}\")\n",
    "    \n",
    "    # Distribution plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for exp_type, color in [('direct', 'steelblue'), ('rag', 'coral')]:\n",
    "        subset = df[df['exp_type'] == exp_type][PRIMARY_METRIC].dropna()\n",
    "        if len(subset) > 0:\n",
    "            ax.hist(subset, bins=20, alpha=0.6, color=color, label=exp_type.upper(), edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel(PRIMARY_METRIC.upper())\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Distribution of {PRIMARY_METRIC.upper()} by Experiment Type')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- Total experiments loaded\n",
    "- Data quality status\n",
    "- Coverage gaps to address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total experiments: {len(df)}\")\n",
    "    print(f\"  - Direct: {len(df[df['exp_type'] == 'direct'])}\")\n",
    "    print(f\"  - RAG: {len(df[df['exp_type'] == 'rag'])}\")\n",
    "    print(f\"\\nModels: {sorted(df['model_short'].unique().tolist())}\")\n",
    "    print(f\"Datasets: {sorted(df['dataset'].unique().tolist())}\")\n",
    "    \n",
    "    if 'retriever_type' in df.columns:\n",
    "        print(f\"Retriever types: {df['retriever_type'].dropna().unique().tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
