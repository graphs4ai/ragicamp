{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB06: When RAG Fails\n",
    "\n",
    "**Question:** When does retrieval *hurt* performance? Why?\n",
    "\n",
    "RAG doesn't always help. This notebook investigates the failure modes:\n",
    "1. **Ceiling effect** — does RAG hurt when the direct baseline is already strong?\n",
    "2. **Component correlates** — which RAG configurations are most likely to hurt?\n",
    "3. **Per-question analysis** — on which questions does RAG consistently hurt?\n",
    "4. **Case studies** — inspect retrieved docs for RAG-hurts cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting,\n",
    "    analyze_rag_benefit_distribution, identify_rag_success_factors,\n",
    "    load_per_item_scores, compute_per_question_rag_delta,\n",
    "    sample_predictions_with_docs,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "print(f\"Loaded {len(df)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ceiling Effect Hypothesis\n",
    "\n",
    "**H1:** RAG hurts more when the direct baseline is already strong (ceiling effect).\n",
    "\n",
    "For each model+dataset, plot direct F1 (x) vs RAG delta (y). A negative correlation supports the ceiling hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benefit = analyze_rag_benefit_distribution(df, PRIMARY_METRIC)\n",
    "rag_df = benefit['rag_df'] if benefit else pd.DataFrame()\n",
    "\n",
    "if not rag_df.empty and 'direct_baseline' in rag_df.columns:\n",
    "    # Mean RAG delta per model+dataset\n",
    "    group_delta = rag_df.groupby(['model_short', 'dataset']).agg(\n",
    "        mean_delta=('rag_benefit', 'mean'),\n",
    "        best_delta=('rag_benefit', 'max'),\n",
    "        worst_delta=('rag_benefit', 'min'),\n",
    "        direct_baseline=('direct_baseline', 'first'),\n",
    "        n_configs=('rag_benefit', 'count'),\n",
    "    ).reset_index()\n",
    "    group_delta['tier'] = group_delta['model_short'].map(MODEL_TIER)\n",
    "\n",
    "    # Scatter: direct baseline vs mean RAG delta\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Mean delta\n",
    "    ax = axes[0]\n",
    "    for ds in sorted(group_delta['dataset'].unique()):\n",
    "        sub = group_delta[group_delta['dataset'] == ds]\n",
    "        ax.scatter(sub['direct_baseline'], sub['mean_delta'], s=80, label=ds, alpha=0.8)\n",
    "        for _, row in sub.iterrows():\n",
    "            ax.annotate(row['model_short'], (row['direct_baseline'], row['mean_delta']),\n",
    "                        textcoords='offset points', xytext=(5, 3), fontsize=7)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    rho, pval = scipy_stats.spearmanr(group_delta['direct_baseline'], group_delta['mean_delta'])\n",
    "    ax.set_title(f'Mean RAG Delta vs Direct Baseline\\n(Spearman rho={rho:.3f}, p={pval:.3f})')\n",
    "    ax.set_xlabel('Direct Baseline F1')\n",
    "    ax.set_ylabel('Mean RAG Delta')\n",
    "    ax.legend(title='Dataset')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    # Best delta (optimistic)\n",
    "    ax = axes[1]\n",
    "    for ds in sorted(group_delta['dataset'].unique()):\n",
    "        sub = group_delta[group_delta['dataset'] == ds]\n",
    "        ax.scatter(sub['direct_baseline'], sub['best_delta'], s=80, label=ds, alpha=0.8)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    rho2, pval2 = scipy_stats.spearmanr(group_delta['direct_baseline'], group_delta['best_delta'])\n",
    "    ax.set_title(f'Best RAG Delta vs Direct Baseline\\n(Spearman rho={rho2:.3f}, p={pval2:.3f})')\n",
    "    ax.set_xlabel('Direct Baseline F1')\n",
    "    ax.set_ylabel('Best RAG Delta')\n",
    "    ax.legend(title='Dataset')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nCeiling effect test (mean delta): rho={rho:.3f}, p={pval:.3f}\")\n",
    "    print(f\"Ceiling effect test (best delta): rho={rho2:.3f}, p={pval2:.3f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for ceiling effect analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Component Correlates of RAG Failure\n",
    "\n",
    "Which RAG component values are most associated with RAG *hurting* performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rag_df.empty:\n",
    "    # Binary: did RAG hurt? (> 1 point F1 loss)\n",
    "    rag_df['rag_hurts_binary'] = (rag_df['rag_benefit'] < -0.01).astype(int)\n",
    "\n",
    "    factors = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n",
    "               'query_transform', 'top_k', 'agent_type']\n",
    "    factors = [f for f in factors if f in rag_df.columns and rag_df[f].nunique() > 1]\n",
    "\n",
    "    fig, axes = plt.subplots(2, (len(factors) + 1) // 2,\n",
    "                             figsize=(6 * ((len(factors) + 1) // 2), 10))\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    for idx, factor in enumerate(factors):\n",
    "        ax = axes_flat[idx]\n",
    "        rates = rag_df.groupby(factor)['rag_hurts_binary'].agg(['mean', 'count']).reset_index()\n",
    "        rates.columns = [factor, 'hurt_rate', 'n']\n",
    "        rates = rates.sort_values('hurt_rate', ascending=False)\n",
    "\n",
    "        colors = plt.cm.RdYlGn_r(rates['hurt_rate'] / max(rates['hurt_rate'].max(), 0.01))\n",
    "        ax.barh(range(len(rates)), rates['hurt_rate'], color=colors)\n",
    "        ax.set_yticks(range(len(rates)))\n",
    "        ax.set_yticklabels([f\"{v} (n={n})\" for v, n in zip(rates[factor], rates['n'])],\n",
    "                           fontsize=9)\n",
    "        ax.set_xlabel('Hurt Rate')\n",
    "        ax.set_title(factor)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for idx in range(len(factors), len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle('RAG Hurt Rate by Component Value', y=1.01, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean RAG benefit heatmap: which combos help most / hurt most?\n",
    "if not rag_df.empty:\n",
    "    combos = [('retriever_type', 'prompt'), ('retriever_type', 'reranker'),\n",
    "              ('prompt', 'query_transform')]\n",
    "    combos = [(f1, f2) for f1, f2 in combos\n",
    "              if f1 in rag_df.columns and f2 in rag_df.columns\n",
    "              and rag_df[f1].nunique() > 1 and rag_df[f2].nunique() > 1]\n",
    "\n",
    "    if combos:\n",
    "        fig, axes = plt.subplots(1, len(combos), figsize=(7 * len(combos), 5))\n",
    "        if len(combos) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, (f1, f2) in zip(axes, combos):\n",
    "            pivot = rag_df.groupby([f1, f2])['rag_benefit'].mean().unstack()\n",
    "            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                        center=0, ax=ax, linewidths=0.5)\n",
    "            ax.set_title(f'Mean RAG Benefit: {f1} x {f2}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Question RAG Delta\n",
    "\n",
    "Which questions does RAG consistently hurt across configurations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-item scores (this may take a minute for large studies)\n",
    "print(\"Loading per-item scores...\")\n",
    "item_scores = load_per_item_scores(STUDY_PATH, metric=PRIMARY_METRIC, include_text=True)\n",
    "print(f\"Loaded {len(item_scores)} per-item scores\")\n",
    "print(f\"Experiments: {item_scores['experiment'].nunique()}\")\n",
    "print(f\"Questions: {item_scores['idx'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not item_scores.empty:\n",
    "    # Filter out broken models\n",
    "    item_scores = item_scores[~item_scores['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "\n",
    "    # Compute per-question RAG delta\n",
    "    rag_delta = compute_per_question_rag_delta(item_scores, PRIMARY_METRIC)\n",
    "    print(f\"Per-question RAG deltas: {len(rag_delta)} rows\")\n",
    "\n",
    "    if not rag_delta.empty:\n",
    "        # Aggregate: mean delta per question across all RAG configs\n",
    "        q_agg = rag_delta.groupby(['idx', 'dataset']).agg(\n",
    "            mean_delta=('rag_delta', 'mean'),\n",
    "            pct_positive=('rag_delta', lambda x: (x > 0.01).mean()),\n",
    "            pct_negative=('rag_delta', lambda x: (x < -0.01).mean()),\n",
    "            n_configs=('rag_delta', 'count'),\n",
    "        ).reset_index()\n",
    "\n",
    "        # Distribution of mean per-question delta\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        axes[0].hist(q_agg['mean_delta'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "        axes[0].set_xlabel('Mean RAG Delta per Question')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title('Distribution of Per-Question RAG Effect')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "\n",
    "        # Questions where RAG hurts > 80% of configs\n",
    "        consistently_hurts = q_agg[q_agg['pct_negative'] > 0.8]\n",
    "        consistently_helps = q_agg[q_agg['pct_positive'] > 0.8]\n",
    "        print(f\"\\nQuestions where RAG consistently hurts (>80% configs): {len(consistently_hurts)}\")\n",
    "        print(f\"Questions where RAG consistently helps (>80% configs): {len(consistently_helps)}\")\n",
    "\n",
    "        # Scatter: pct_positive vs pct_negative\n",
    "        for ds in sorted(q_agg['dataset'].unique()):\n",
    "            sub = q_agg[q_agg['dataset'] == ds]\n",
    "            axes[1].scatter(sub['pct_positive'], sub['pct_negative'],\n",
    "                            s=20, alpha=0.4, label=ds)\n",
    "        axes[1].set_xlabel('% RAG configs that help')\n",
    "        axes[1].set_ylabel('% RAG configs that hurt')\n",
    "        axes[1].set_title('Per-Question: Help Rate vs Hurt Rate')\n",
    "        axes[1].legend(title='Dataset')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top questions where RAG consistently hurts\n",
    "if not rag_delta.empty:\n",
    "    worst_questions = q_agg.nsmallest(20, 'mean_delta')\n",
    "    print(\"Top 20 questions where RAG hurts most (mean delta across all configs):\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for _, row in worst_questions.iterrows():\n",
    "        # Look up the question text\n",
    "        q_text_rows = item_scores[\n",
    "            (item_scores['idx'] == row['idx']) & (item_scores['dataset'] == row['dataset'])\n",
    "        ]\n",
    "        q_text = q_text_rows['question'].iloc[0] if 'question' in q_text_rows.columns and len(q_text_rows) > 0 else 'N/A'\n",
    "        print(f\"\\n  [{row['dataset']}] Q{row['idx']}: {q_text[:120]}\")\n",
    "        print(f\"    Mean delta: {row['mean_delta']:.4f}, \"\n",
    "              f\"Hurts in {row['pct_negative']*100:.0f}% of configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Case Studies: Inspecting Retrieved Docs\n",
    "\n",
    "For the worst RAG-hurts questions, inspect what the retriever brought back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few experiments where RAG hurt on specific questions\n",
    "if not rag_delta.empty:\n",
    "    # Find a representative RAG experiment that hurt on the worst question\n",
    "    worst_q = worst_questions.iloc[0]\n",
    "    hurting_exps = rag_delta[\n",
    "        (rag_delta['idx'] == worst_q['idx'])\n",
    "        & (rag_delta['dataset'] == worst_q['dataset'])\n",
    "        & (rag_delta['rag_delta'] < -0.05)\n",
    "    ].nsmallest(3, 'rag_delta')\n",
    "\n",
    "    print(f\"Inspecting worst RAG-hurts case:\")\n",
    "    print(f\"  Question idx={worst_q['idx']}, dataset={worst_q['dataset']}\")\n",
    "    print(f\"  Mean delta={worst_q['mean_delta']:.4f}\\n\")\n",
    "\n",
    "    for _, exp_row in hurting_exps.iterrows():\n",
    "        exp_name = exp_row['experiment']\n",
    "        preds = sample_predictions_with_docs(\n",
    "            STUDY_PATH, exp_name,\n",
    "            indices=[int(worst_q['idx'])]\n",
    "        )\n",
    "        if not preds:\n",
    "            continue\n",
    "\n",
    "        p = preds[0]\n",
    "        print(f\"  Experiment: {exp_name}\")\n",
    "        print(f\"  Question:   {p.get('question', 'N/A')[:200]}\")\n",
    "        print(f\"  Expected:   {p.get('expected', 'N/A')}\")\n",
    "        print(f\"  Prediction: {p.get('prediction', 'N/A')[:200]}\")\n",
    "\n",
    "        docs = p.get('retrieved_docs', [])\n",
    "        if docs:\n",
    "            print(f\"  Retrieved docs ({len(docs)}):\")\n",
    "            for doc in docs[:3]:\n",
    "                content = doc.get('content', '')[:150]\n",
    "                score = doc.get('score', 'N/A')\n",
    "                print(f\"    [{score:.3f}] {content}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Failure Patterns by Dataset\n",
    "\n",
    "Is RAG failure more common on certain question types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rag_delta.empty:\n",
    "    # Per-dataset: what fraction of questions are consistently hurt by RAG?\n",
    "    ds_summary = q_agg.groupby('dataset').agg(\n",
    "        n_questions=('idx', 'count'),\n",
    "        mean_delta=('mean_delta', 'mean'),\n",
    "        pct_mostly_hurt=('pct_negative', lambda x: (x > 0.5).mean()),\n",
    "        pct_mostly_help=('pct_positive', lambda x: (x > 0.5).mean()),\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"RAG Failure Patterns by Dataset:\")\n",
    "    display(ds_summary.round(3))\n",
    "\n",
    "    # Violin plot of mean_delta by dataset\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    datasets = sorted(q_agg['dataset'].unique())\n",
    "    data_for_violin = [q_agg[q_agg['dataset'] == ds]['mean_delta'].values for ds in datasets]\n",
    "    parts = ax.violinplot(data_for_violin, showmeans=True, showmedians=True)\n",
    "    ax.set_xticks(range(1, len(datasets) + 1))\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylabel('Mean RAG Delta per Question')\n",
    "    ax.set_title('Per-Question RAG Effect Distribution by Dataset')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key findings on RAG failure modes:\n",
    "- Whether the ceiling effect hypothesis is supported\n",
    "- Which component configurations predict RAG failure\n",
    "- What fraction of questions are consistently hurt by RAG\n",
    "- Common patterns in retrieved documents for RAG-hurts cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
