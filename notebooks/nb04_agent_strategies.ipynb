{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB04: Agent Strategies\n",
    "\n",
    "**Question:** Do iterative_rag and self_rag beat fixed_rag? When? At what cost?\n",
    "\n",
    "This notebook compares the four agent types:\n",
    "- `direct_llm` — no retrieval baseline\n",
    "- `fixed_rag` — single-shot RAG\n",
    "- `iterative_rag` — multi-iteration with sufficiency checking\n",
    "- `self_rag` — self-reflective with assessment/verification\n",
    "\n",
    "Key analyses:\n",
    "- Per-agent performance with CI\n",
    "- Agent x Model and Agent x Dataset interactions\n",
    "- Agent x Retrieval component interactions\n",
    "- Cost-benefit: does the advanced agent beat the best fixed_rag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, effect_size,\n",
    "    weighted_mean_with_ci, plot_interaction_heatmap,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "print(f\"Loaded {len(df)} experiments (after removing {BROKEN_MODELS})\")\n",
    "print(f\"Agent types: {df['agent_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agent Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# weighted_mean_with_ci now uses stratify_by='dataset' by default\nagent_stats = weighted_mean_with_ci(df, 'agent_type', PRIMARY_METRIC)\ndisplay(agent_stats.round(4))\n\n# Bar chart with CI\nfig, ax = plt.subplots(figsize=(8, 5))\nx = range(len(agent_stats))\nyerr_low = np.maximum(agent_stats['mean'] - agent_stats['ci_low'], 0)\nyerr_high = np.maximum(agent_stats['ci_high'] - agent_stats['mean'], 0)\ncolors = ['steelblue', 'coral', '#66bb6a', '#ffa726'][:len(agent_stats)]\nax.bar(x, agent_stats['mean'], yerr=[yerr_low, yerr_high], capsize=5,\n       color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\nax.set_xticks(x)\nax.set_xticklabels(agent_stats['agent_type'])\nax.set_ylabel('Mean F1 (dataset-stratified)')\nax.set_title('Agent Type Performance (Mean F1 with 95% CI, dataset-stratified)')\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Pairwise effect sizes vs direct — per-dataset then average\ndatasets = sorted(df['dataset'].unique())\nprint(\"\\nPairwise effect sizes vs direct_llm (per-dataset, then averaged):\")\nfor agent in ['fixed_rag', 'iterative_rag', 'self_rag']:\n    per_ds_d = []\n    for ds in datasets:\n        direct_vals = df.loc[(df['agent_type'] == 'direct_llm') & (df['dataset'] == ds), PRIMARY_METRIC].dropna().values\n        agent_vals = df.loc[(df['agent_type'] == agent) & (df['dataset'] == ds), PRIMARY_METRIC].dropna().values\n        if len(agent_vals) >= 2 and len(direct_vals) >= 2:\n            d_val, _, _ = effect_size(direct_vals, agent_vals)\n            per_ds_d.append(d_val)\n    if per_ds_d:\n        avg_d = np.mean(per_ds_d)\n        interp = 'large' if abs(avg_d) >= 0.8 else 'medium' if abs(avg_d) >= 0.5 else 'small' if abs(avg_d) >= 0.2 else 'negligible'\n        print(f\"  {agent:<16s}: d={avg_d:+.3f} ({interp}, avg of {len(per_ds_d)} datasets)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent x Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap: agent_type x model_short — stratified by dataset\n# Per (agent, model, dataset) mean, then average across datasets\nper_ds = df.groupby(['agent_type', 'model_short', 'dataset'])[PRIMARY_METRIC].mean()\nagent_model = per_ds.groupby(['agent_type', 'model_short']).mean().unstack()\n\nif not agent_model.empty:\n    fig, ax = plt.subplots(figsize=(12, 5))\n    sns.heatmap(agent_model, annot=True, fmt='.3f', cmap='RdYlGn',\n                ax=ax, linewidths=0.5)\n    ax.set_title('Mean F1: Agent Type x Model (dataset-stratified)')\n    plt.tight_layout()\n    plt.show()\n\n    # Also as grouped bars per model\n    models = sorted(df['model_short'].unique())\n    agents = sorted(df['agent_type'].unique())\n    n_agents = len(agents)\n    datasets = sorted(df['dataset'].unique())\n\n    fig, ax = plt.subplots(figsize=(14, 5))\n    x = np.arange(len(models))\n    w = 0.8 / n_agents\n    agent_colors = {'direct_llm': 'steelblue', 'fixed_rag': 'coral',\n                    'iterative_rag': '#66bb6a', 'self_rag': '#ffa726'}\n\n    for i, agent in enumerate(agents):\n        # Stratified mean: per (model, dataset) then average across datasets\n        means = []\n        for m in models:\n            per_ds_vals = [df.loc[(df['model_short'] == m) & (df['agent_type'] == agent) & (df['dataset'] == ds),\n                                  PRIMARY_METRIC].mean()\n                           for ds in datasets\n                           if len(df.loc[(df['model_short'] == m) & (df['agent_type'] == agent) & (df['dataset'] == ds)]) > 0]\n            means.append(np.nanmean(per_ds_vals) if per_ds_vals else np.nan)\n        ax.bar(x + i * w - (n_agents - 1) * w / 2, means, w,\n               label=agent, color=agent_colors.get(agent, f'C{i}'), alpha=0.8)\n\n    ax.set_xticks(x)\n    ax.set_xticklabels(models, rotation=30, ha='right')\n    ax.set_ylabel('Mean F1 (dataset-stratified)')\n    ax.set_title('Agent Performance by Model (dataset-stratified)')\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent x Dataset\n",
    "\n",
    "Key test: does `iterative_rag` help more on multi-hop (HotpotQA)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = sorted(df['dataset'].unique())\n",
    "agents = sorted(df['agent_type'].unique())\n",
    "\n",
    "fig, axes = plt.subplots(1, len(datasets), figsize=(6 * len(datasets), 5), sharey=True)\n",
    "if len(datasets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "agent_colors = {'direct_llm': 'steelblue', 'fixed_rag': 'coral',\n",
    "                'iterative_rag': '#66bb6a', 'self_rag': '#ffa726'}\n",
    "\n",
    "for ax, ds in zip(axes, datasets):\n",
    "    ds_df = df[df['dataset'] == ds]\n",
    "    stats = weighted_mean_with_ci(ds_df, 'agent_type', PRIMARY_METRIC)\n",
    "    if stats.empty:\n",
    "        continue\n",
    "\n",
    "    x = range(len(stats))\n",
    "    yerr_low = np.maximum(stats['mean'] - stats['ci_low'], 0)\n",
    "    yerr_high = np.maximum(stats['ci_high'] - stats['mean'], 0)\n",
    "    colors = [agent_colors.get(a, 'grey') for a in stats['agent_type']]\n",
    "    ax.bar(x, stats['mean'], yerr=[yerr_low, yerr_high], capsize=4,\n",
    "           color=colors, alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stats['agent_type'], rotation=30, ha='right')\n",
    "    ax.set_title(ds)\n",
    "    ax.set_ylabel('Mean F1' if ax == axes[0] else '')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Agent Performance by Dataset', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print per-dataset effect sizes: iterative_rag vs fixed_rag\n",
    "print(\"\\nIterative RAG vs Fixed RAG by dataset:\")\n",
    "for ds in datasets:\n",
    "    fixed = df.loc[(df['agent_type'] == 'fixed_rag') & (df['dataset'] == ds), PRIMARY_METRIC].dropna().values\n",
    "    iterative = df.loc[(df['agent_type'] == 'iterative_rag') & (df['dataset'] == ds), PRIMARY_METRIC].dropna().values\n",
    "    if len(fixed) >= 2 and len(iterative) >= 2:\n",
    "        d, pval, interp = effect_size(fixed, iterative)\n",
    "        print(f\"  {ds:<12s}: d={d:+.3f} ({interp}), p={pval:.2e}\")\n",
    "    else:\n",
    "        print(f\"  {ds:<12s}: insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent x Retrieval Components\n",
    "\n",
    "Is `self_rag + reranker` synergistic or redundant? Does agent type interact with retriever type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rag_df = df[df['exp_type'] == 'rag'].copy()\n\ninteraction_pairs = [\n    ('agent_type', 'retriever_type'),\n    ('agent_type', 'reranker'),\n]\ninteraction_pairs = [(f1, f2) for f1, f2 in interaction_pairs\n                     if f1 in rag_df.columns and f2 in rag_df.columns\n                     and rag_df[f1].nunique() > 1 and rag_df[f2].nunique() > 1]\n\nif interaction_pairs:\n    fig, axes = plt.subplots(1, len(interaction_pairs),\n                             figsize=(7 * len(interaction_pairs), 5))\n    if len(interaction_pairs) == 1:\n        axes = [axes]\n\n    for ax, (f1, f2) in zip(axes, interaction_pairs):\n        # Stratified: per (f1, f2, dataset) mean, then average across datasets\n        per_ds = rag_df.groupby([f1, f2, 'dataset'])[PRIMARY_METRIC].mean()\n        pivot = per_ds.groupby([f1, f2]).mean().unstack()\n        if not pivot.empty:\n            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn',\n                        center=pivot.values.mean(), ax=ax, linewidths=0.5)\n            ax.set_title(f'{f1} x {f2} (dataset-stratified)')\n\n    plt.suptitle('Agent x Retrieval Component Interactions (dataset-stratified)', y=1.02)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough factor pairs for interaction analysis.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost-Benefit: Advanced Agents vs Best Fixed RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model+dataset, compare best fixed_rag vs best advanced agent\n",
    "group_cols = ['model_short', 'dataset']\n",
    "results = []\n",
    "\n",
    "for group_vals, group_df in rag_df.groupby(group_cols):\n",
    "    model, dataset = group_vals\n",
    "    fixed = group_df[group_df['agent_type'] == 'fixed_rag']\n",
    "    advanced = group_df[group_df['agent_type'].isin(['iterative_rag', 'self_rag'])]\n",
    "\n",
    "    if fixed[PRIMARY_METRIC].notna().sum() == 0 or advanced[PRIMARY_METRIC].notna().sum() == 0:\n",
    "        continue\n",
    "\n",
    "    best_fixed = fixed[PRIMARY_METRIC].max()\n",
    "    best_advanced = advanced[PRIMARY_METRIC].max()\n",
    "    best_agent = advanced.loc[advanced[PRIMARY_METRIC].idxmax(), 'agent_type']\n",
    "\n",
    "    results.append({\n",
    "        'model': model, 'dataset': dataset,\n",
    "        'best_fixed_rag': best_fixed,\n",
    "        'best_advanced': best_advanced,\n",
    "        'best_agent': best_agent,\n",
    "        'delta': best_advanced - best_fixed,\n",
    "        'advanced_wins': best_advanced > best_fixed,\n",
    "    })\n",
    "\n",
    "cost_df = pd.DataFrame(results)\n",
    "if not cost_df.empty:\n",
    "    n_wins = cost_df['advanced_wins'].sum()\n",
    "    n_total = len(cost_df)\n",
    "    print(f\"Advanced agent beats best fixed_rag in {n_wins}/{n_total} \"\n",
    "          f\"({n_wins/n_total*100:.0f}%) of model+dataset scenarios\")\n",
    "    print(f\"Mean delta when wins: {cost_df.loc[cost_df['advanced_wins'], 'delta'].mean():.4f}\")\n",
    "    print(f\"Mean delta when loses: {cost_df.loc[~cost_df['advanced_wins'], 'delta'].mean():.4f}\")\n",
    "    print()\n",
    "    display(cost_df.round(4))\n",
    "else:\n",
    "    print(\"Not enough data for cost-benefit analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- Which agent type performs best overall\n",
    "- Whether iterative_rag helps on multi-hop datasets\n",
    "- Whether self_rag + reranker is synergistic or redundant\n",
    "- % of scenarios where advanced agents justify their complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}