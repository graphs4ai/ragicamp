{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB05: Model Scaling\n",
    "\n",
    "**Question:** Can small models + premium retrieval match larger models without retrieval?\n",
    "\n",
    "This notebook explores the scaling relationship:\n",
    "- Direct performance vs model size\n",
    "- RAG uplift vs model size\n",
    "- Whether small+RAG can compensate for model size\n",
    "- What matters most for small vs medium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as scipy_stats\n\nfrom analysis_utils import (\n    load_all_results, setup_plotting, identify_bottlenecks,\n    weighted_mean_with_ci, multi_metric_bottlenecks_df,\n    PRIMARY_METRIC, MODEL_PARAMS, MODEL_TIER, BROKEN_MODELS,\n    MULTI_METRIC_SET, GROUNDEDNESS_METRICS, CONTEXT_METRICS,\n)\n\nsetup_plotting()\nSTUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n\ndf_all = load_all_results(STUDY_PATH)\ndf = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n\n# Add parameter count and tier columns\ndf['params_b'] = df['model_short'].map(MODEL_PARAMS)\ndf['tier'] = df['model_short'].map(MODEL_TIER)\nprint(f\"Loaded {len(df)} experiments\")\nprint(f\"Models with params: {df['params_b'].notna().sum()}\")\nprint(f\"Tiers: {df['tier'].value_counts().to_dict()}\")\n\n# Check available metrics\navailable_metrics = [m for m in MULTI_METRIC_SET if m in df.columns and df[m].notna().sum() >= 10]\nprint(f\"\\nAvailable metrics: {available_metrics}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct Scaling Curve\n",
    "\n",
    "F1 vs model parameters (log scale) for direct (no-retrieval) experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = df[df['exp_type'] == 'direct'].dropna(subset=['params_b', PRIMARY_METRIC])\n",
    "model_direct = direct.groupby('model_short').agg(\n",
    "    mean_f1=(PRIMARY_METRIC, 'mean'),\n",
    "    std_f1=(PRIMARY_METRIC, 'std'),\n",
    "    n=(PRIMARY_METRIC, 'count'),\n",
    "    params_b=('params_b', 'first'),\n",
    "    tier=('tier', 'first'),\n",
    ").reset_index()\n",
    "\n",
    "tier_colors = {'tiny': '#ef5350', 'small': '#ffa726', 'medium': '#66bb6a'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for tier, color in tier_colors.items():\n",
    "    sub = model_direct[model_direct['tier'] == tier]\n",
    "    ax.scatter(sub['params_b'], sub['mean_f1'], c=color, s=120,\n",
    "              label=tier.capitalize(), edgecolors='black', zorder=3)\n",
    "    for _, row in sub.iterrows():\n",
    "        ax.annotate(row['model_short'], (row['params_b'], row['mean_f1']),\n",
    "                    textcoords='offset points', xytext=(8, 4), fontsize=9)\n",
    "\n",
    "# Log-linear regression\n",
    "if len(model_direct) >= 3:\n",
    "    log_params = np.log10(model_direct['params_b'])\n",
    "    slope, intercept, r, p, se = scipy_stats.linregress(log_params, model_direct['mean_f1'])\n",
    "    x_fit = np.linspace(log_params.min() - 0.1, log_params.max() + 0.1, 50)\n",
    "    ax.plot(10**x_fit, slope * x_fit + intercept, '--', color='grey', alpha=0.6,\n",
    "            label=f'Log-linear fit (R²={r**2:.2f})')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (Billions, log scale)')\n",
    "ax.set_ylabel('Mean F1 (Direct)')\n",
    "ax.set_title('Direct Performance Scaling Curve')\n",
    "ax.legend(title='Tier')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG Uplift by Model Size\n",
    "\n",
    "Overlay best-RAG curve on direct curve — the key thesis figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df = df[df['exp_type'] == 'rag'].dropna(subset=['params_b', PRIMARY_METRIC])\n",
    "model_best_rag = rag_df.groupby('model_short').agg(\n",
    "    best_rag_f1=(PRIMARY_METRIC, 'max'),\n",
    "    mean_rag_f1=(PRIMARY_METRIC, 'mean'),\n",
    "    params_b=('params_b', 'first'),\n",
    "    tier=('tier', 'first'),\n",
    ").reset_index()\n",
    "\n",
    "# Merge direct and RAG data\n",
    "scaling = model_direct[['model_short', 'mean_f1', 'params_b', 'tier']].rename(\n",
    "    columns={'mean_f1': 'direct_f1'}\n",
    ").merge(model_best_rag[['model_short', 'best_rag_f1', 'mean_rag_f1']], on='model_short', how='outer')\n",
    "scaling['rag_uplift'] = scaling['best_rag_f1'] - scaling['direct_f1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "# Sort by params for line plot\n",
    "scaling_sorted = scaling.dropna(subset=['params_b']).sort_values('params_b')\n",
    "\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['direct_f1'],\n",
    "        'o-', color='steelblue', markersize=8, label='Direct (no retrieval)', linewidth=2)\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['best_rag_f1'],\n",
    "        's-', color='coral', markersize=8, label='Best RAG', linewidth=2)\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['mean_rag_f1'],\n",
    "        '^--', color='#ffa726', markersize=7, label='Mean RAG', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Annotate models\n",
    "for _, row in scaling_sorted.iterrows():\n",
    "    ax.annotate(row['model_short'], (row['params_b'], row['best_rag_f1']),\n",
    "                textcoords='offset points', xytext=(8, 6), fontsize=8)\n",
    "\n",
    "# Shade the RAG uplift region\n",
    "ax.fill_between(scaling_sorted['params_b'],\n",
    "                scaling_sorted['direct_f1'], scaling_sorted['best_rag_f1'],\n",
    "                alpha=0.1, color='coral', label='RAG uplift zone')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (Billions, log scale)')\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('Model Scaling: Direct vs Best RAG')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(scaling.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Multi-Metric Scaling\n\nDo all quality dimensions scale the same way with model size?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multi-metric scaling: best RAG performance per model across metrics\nscaling_metrics = [m for m in available_metrics if m != 'hallucination']  # hallucination is inverted\nrag_df = df[df['exp_type'] == 'rag'].dropna(subset=['params_b'])\n\nif len(scaling_metrics) >= 2 and not rag_df.empty:\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n    # Left: Best RAG per model across metrics (absolute values)\n    ax = axes[0]\n    for metric in scaling_metrics:\n        metric_data = rag_df.dropna(subset=[metric])\n        if metric_data.empty:\n            continue\n        model_best = metric_data.groupby('model_short').agg(\n            best=(metric, 'max'),\n            params_b=('params_b', 'first'),\n        ).sort_values('params_b')\n        ax.plot(model_best['params_b'], model_best['best'],\n                'o-', label=metric, markersize=6)\n\n    ax.set_xscale('log')\n    ax.set_xlabel('Parameters (Billions, log scale)')\n    ax.set_ylabel('Best Score')\n    ax.set_title('Best RAG Performance by Model Size — Multiple Metrics')\n    ax.legend(fontsize=9)\n    ax.grid(alpha=0.3)\n\n    # Right: RAG uplift (best_rag - direct) per model across metrics\n    ax2 = axes[1]\n    direct_df = df[df['exp_type'] == 'direct'].dropna(subset=['params_b'])\n\n    for metric in scaling_metrics:\n        d_means = direct_df.dropna(subset=[metric]).groupby('model_short').agg(\n            direct_mean=(metric, 'mean'), params_b=('params_b', 'first'))\n        r_best = rag_df.dropna(subset=[metric]).groupby('model_short').agg(\n            best_rag=(metric, 'max'), params_b=('params_b', 'first'))\n\n        merged = d_means.join(r_best[['best_rag']], how='inner')\n        merged['uplift'] = merged['best_rag'] - merged['direct_mean']\n        merged = merged.sort_values('params_b')\n\n        if not merged.empty:\n            ax2.plot(merged['params_b'], merged['uplift'],\n                    'o-', label=metric, markersize=6)\n\n    ax2.set_xscale('log')\n    ax2.axhline(y=0, color='grey', ls='--', alpha=0.4)\n    ax2.set_xlabel('Parameters (Billions, log scale)')\n    ax2.set_ylabel('RAG Uplift (best_RAG - direct)')\n    ax2.set_title('RAG Uplift by Model Size — Multiple Metrics')\n    ax2.legend(fontsize=9)\n    ax2.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough metrics with data for multi-metric scaling analysis.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can Small+RAG Match Medium+Direct?\n",
    "\n",
    "For each tier, compare best-RAG performance against the direct performance of the next tier up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_order = ['tiny', 'small', 'medium']\n",
    "tier_direct = {}\n",
    "tier_best_rag = {}\n",
    "\n",
    "for tier in tier_order:\n",
    "    tier_models = scaling[scaling['tier'] == tier]\n",
    "    if not tier_models.empty:\n",
    "        tier_direct[tier] = tier_models['direct_f1'].mean()\n",
    "        tier_best_rag[tier] = tier_models['best_rag_f1'].max()\n",
    "\n",
    "comparison_rows = []\n",
    "for i, tier in enumerate(tier_order[:-1]):\n",
    "    next_tier = tier_order[i + 1]\n",
    "    if tier in tier_best_rag and next_tier in tier_direct:\n",
    "        rag_perf = tier_best_rag[tier]\n",
    "        next_direct = tier_direct[next_tier]\n",
    "        ratio = rag_perf / next_direct if next_direct > 0 else np.nan\n",
    "        comparison_rows.append({\n",
    "            'tier': tier,\n",
    "            f'{tier}_direct': tier_direct.get(tier, np.nan),\n",
    "            f'{tier}_best_rag': rag_perf,\n",
    "            f'{next_tier}_direct': next_direct,\n",
    "            'compensation_ratio': ratio,\n",
    "            'can_match': rag_perf >= next_direct,\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "display(comp_df.round(4))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(tier_order))\n",
    "w = 0.3\n",
    "\n",
    "direct_vals = [tier_direct.get(t, 0) for t in tier_order]\n",
    "rag_vals = [tier_best_rag.get(t, 0) for t in tier_order]\n",
    "\n",
    "ax.bar(x - w/2, direct_vals, w, label='Direct', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + w/2, rag_vals, w, label='Best RAG', color='coral', alpha=0.8)\n",
    "\n",
    "# Draw arrows showing compensation\n",
    "for i in range(len(tier_order) - 1):\n",
    "    if rag_vals[i] > 0 and direct_vals[i+1] > 0:\n",
    "        ax.annotate('', xy=(x[i+1] - w/2, direct_vals[i+1]),\n",
    "                    xytext=(x[i] + w/2, rag_vals[i]),\n",
    "                    arrowprops=dict(arrowstyle='->', color='grey', lw=1.5, ls='--'))\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{t.capitalize()}\\n({', '.join(scaling[scaling['tier']==t]['model_short'].tolist())})\" for t in tier_order],\n",
    "                    fontsize=9)\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('Can Small+RAG Match Medium+Direct?')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Matters for Small vs Medium Models?\n",
    "\n",
    "Side-by-side variance decomposition for small and medium models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_bottlenecks = {}\n",
    "for tier in ['small', 'medium']:\n",
    "    tier_df = df[(df['tier'] == tier) & (df['exp_type'] == 'rag')]\n",
    "    if len(tier_df) >= 10:\n",
    "        bn = identify_bottlenecks(tier_df, PRIMARY_METRIC)\n",
    "        tier_bottlenecks[tier] = bn\n",
    "        print(f\"\\n{tier.capitalize()} models — variance explained:\")\n",
    "        for factor, pct in bn.items():\n",
    "            print(f\"  {factor:<20s}: {pct:5.1f}%\")\n",
    "\n",
    "# Side-by-side bar chart\n",
    "if len(tier_bottlenecks) >= 2:\n",
    "    all_factors = sorted(set().union(*[bn.keys() for bn in tier_bottlenecks.values()]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(all_factors))\n",
    "    w = 0.35\n",
    "\n",
    "    for i, (tier, bn) in enumerate(tier_bottlenecks.items()):\n",
    "        vals = [bn.get(f, 0) for f in all_factors]\n",
    "        color = '#ffa726' if tier == 'small' else '#66bb6a'\n",
    "        ax.bar(x + (i - 0.5) * w, vals, w, label=tier.capitalize(), color=color, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_factors, rotation=30, ha='right')\n",
    "    ax.set_ylabel('Variance Explained (%)')\n",
    "    ax.set_title('What Matters: Small vs Medium Models')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Also do tiny if available\n",
    "tiny_df = df[(df['tier'] == 'tiny') & (df['exp_type'] == 'rag')]\n",
    "if len(tiny_df) >= 10:\n",
    "    bn = identify_bottlenecks(tiny_df, PRIMARY_METRIC)\n",
    "    print(f\"\\nTiny models — variance explained:\")\n",
    "    for factor, pct in bn.items():\n",
    "        print(f\"  {factor:<20s}: {pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thesis Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table: tier, direct_f1, best_rag_f1, uplift, can_match_next_tier\n",
    "verdict_rows = []\n",
    "for tier in tier_order:\n",
    "    tier_models = scaling[scaling['tier'] == tier]\n",
    "    if tier_models.empty:\n",
    "        continue\n",
    "\n",
    "    direct_mean = tier_models['direct_f1'].mean()\n",
    "    best_rag = tier_models['best_rag_f1'].max()\n",
    "    uplift = best_rag - direct_mean\n",
    "\n",
    "    # Can this tier's best RAG match the next tier's direct?\n",
    "    tier_idx = tier_order.index(tier)\n",
    "    if tier_idx < len(tier_order) - 1:\n",
    "        next_tier = tier_order[tier_idx + 1]\n",
    "        next_direct = tier_direct.get(next_tier, np.nan)\n",
    "        can_match = best_rag >= next_direct if not np.isnan(next_direct) else None\n",
    "    else:\n",
    "        can_match = 'N/A (largest tier)'\n",
    "\n",
    "    verdict_rows.append({\n",
    "        'tier': tier,\n",
    "        'models': ', '.join(tier_models['model_short'].tolist()),\n",
    "        'direct_f1': direct_mean,\n",
    "        'best_rag_f1': best_rag,\n",
    "        'rag_uplift': uplift,\n",
    "        'can_match_next_tier': can_match,\n",
    "    })\n",
    "\n",
    "verdict_df = pd.DataFrame(verdict_rows)\n",
    "print(\"THESIS VERDICT: Model Scaling Summary\")\n",
    "print(\"=\" * 70)\n",
    "display(verdict_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "source": "rag_with_params = df[(df['exp_type'] == 'rag')].dropna(subset=['params_b']).copy()\n\ngroundedness_available = [m for m in ['faithfulness', 'hallucination', 'answer_in_context', 'context_recall']\n                          if m in rag_with_params.columns and rag_with_params[m].notna().sum() >= 10]\n\nif groundedness_available:\n    n_metrics = len(groundedness_available)\n    fig, axes = plt.subplots(1, n_metrics, figsize=(6 * n_metrics, 5))\n    if n_metrics == 1:\n        axes = [axes]\n\n    for ax, metric in zip(axes, groundedness_available):\n        metric_data = rag_with_params.dropna(subset=[metric])\n        model_stats = metric_data.groupby('model_short').agg(\n            mean=(metric, 'mean'),\n            std=(metric, 'std'),\n            n=(metric, 'count'),\n            params_b=('params_b', 'first'),\n            tier=('tier', 'first'),\n        ).sort_values('params_b')\n\n        for tier, color in tier_colors.items():\n            sub = model_stats[model_stats['tier'] == tier]\n            ci = 1.96 * sub['std'] / np.sqrt(sub['n'])\n            ax.errorbar(sub['params_b'], sub['mean'], yerr=ci,\n                       fmt='o', color=color, markersize=8, capsize=4,\n                       label=tier.capitalize())\n            for name, row in sub.iterrows():\n                ax.annotate(name, (row['params_b'], row['mean']),\n                           textcoords='offset points', xytext=(6, 4), fontsize=8)\n\n        # Trend line\n        if len(model_stats) >= 3:\n            log_p = np.log10(model_stats['params_b'])\n            slope, intercept, r, p, _ = scipy_stats.linregress(log_p, model_stats['mean'])\n            x_fit = np.linspace(log_p.min() - 0.1, log_p.max() + 0.1, 50)\n            ax.plot(10**x_fit, slope * x_fit + intercept, '--', color='grey', alpha=0.5)\n            ax.set_title(f'{metric}\\n(slope={slope:.3f}, R²={r**2:.2f})')\n        else:\n            ax.set_title(metric)\n\n        ax.set_xscale('log')\n        ax.set_xlabel('Parameters (B)')\n        ax.set_ylabel(f'Mean {metric}')\n        ax.legend(fontsize=8)\n        ax.grid(alpha=0.3)\n\n    plt.suptitle('Groundedness Metrics by Model Size', y=1.02, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    # Summary table\n    print(\"\\nGroundedness by Model Tier (mean across RAG experiments):\")\n    print(\"=\" * 70)\n    for metric in groundedness_available:\n        tier_means = rag_with_params.groupby('tier')[metric].agg(['mean', 'count'])\n        print(f\"\\n  {metric}:\")\n        for tier in ['tiny', 'small', 'medium']:\n            if tier in tier_means.index:\n                print(f\"    {tier:<8s}: {tier_means.loc[tier, 'mean']:.3f} \"\n                      f\"(n={int(tier_means.loc[tier, 'count'])})\")\n\n    # Key insight: does faithfulness scale differently from F1?\n    if 'faithfulness' in groundedness_available and PRIMARY_METRIC in rag_with_params.columns:\n        print(\"\\n  F1 vs Faithfulness scaling gap:\")\n        for tier in ['tiny', 'small', 'medium']:\n            t_data = rag_with_params[rag_with_params['tier'] == tier]\n            if len(t_data) >= 5:\n                f1_mean = t_data[PRIMARY_METRIC].mean()\n                faith_mean = t_data['faithfulness'].mean()\n                print(f\"    {tier:<8s}: F1={f1_mean:.3f}, faith={faith_mean:.3f}, \"\n                      f\"gap={faith_mean - f1_mean:+.3f}\")\nelse:\n    print(\"No groundedness metrics available. Run:\")\n    print(\"  uv run ragicamp compute-metrics outputs/smart_retrieval_slm \"\n          \"-m faithfulness,hallucination,answer_in_context,context_recall\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Faithfulness & Hallucination by Model Size\n\n**Key safety question:** Do bigger models hallucinate less? Are smaller models less faithful to context?\n\nThis matters for deployment: if small models are equally faithful when given good context,\nthey may be safer than their F1 gap suggests.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- Direct performance scales with model size (log-linear relationship)\n",
    "- RAG uplift magnitude vs model size\n",
    "- Whether small+premium RAG can match medium+direct\n",
    "- Different components matter for small vs medium models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}