{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB05: Model Scaling\n",
    "\n",
    "**Question:** Can small models + premium retrieval match larger models without retrieval?\n",
    "\n",
    "This notebook explores the scaling relationship:\n",
    "- Direct performance vs model size\n",
    "- RAG uplift vs model size\n",
    "- Whether small+RAG can compensate for model size\n",
    "- What matters most for small vs medium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as scipy_stats\n\nfrom analysis_utils import (\n    load_all_results, setup_plotting, identify_bottlenecks,\n    weighted_mean_with_ci, multi_metric_bottlenecks_df,\n    PRIMARY_METRIC, MODEL_PARAMS, MODEL_TIER, BROKEN_MODELS,\n    MULTI_METRIC_SET, GROUNDEDNESS_METRICS, CONTEXT_METRICS,\n)\n\nsetup_plotting()\nSTUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n\ndf_all = load_all_results(STUDY_PATH)\ndf = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n\n# Add parameter count and tier columns\ndf['params_b'] = df['model_short'].map(MODEL_PARAMS)\ndf['tier'] = df['model_short'].map(MODEL_TIER)\nprint(f\"Loaded {len(df)} experiments\")\nprint(f\"Models with params: {df['params_b'].notna().sum()}\")\nprint(f\"Tiers: {df['tier'].value_counts().to_dict()}\")\n\n# Check available metrics\navailable_metrics = [m for m in MULTI_METRIC_SET if m in df.columns and df[m].notna().sum() >= 10]\nprint(f\"\\nAvailable metrics: {available_metrics}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct Scaling Curve\n",
    "\n",
    "F1 vs model parameters (log scale) for direct (no-retrieval) experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stratified: compute mean per (model, dataset), then average across datasets\ndirect = df[df['exp_type'] == 'direct'].dropna(subset=['params_b', PRIMARY_METRIC])\n\n# Per-model-dataset means, then average across datasets per model\nper_ds = direct.groupby(['model_short', 'dataset'])[PRIMARY_METRIC].mean()\nmodel_f1 = per_ds.groupby('model_short').mean()\nmodel_std = per_ds.groupby('model_short').std().fillna(0)\nmodel_n = direct.groupby('model_short')[PRIMARY_METRIC].count()\n\nmodel_direct = direct.groupby('model_short').agg(\n    params_b=('params_b', 'first'),\n    tier=('tier', 'first'),\n).reset_index()\nmodel_direct['mean_f1'] = model_direct['model_short'].map(model_f1)\nmodel_direct['std_f1'] = model_direct['model_short'].map(model_std)\nmodel_direct['n'] = model_direct['model_short'].map(model_n)\n\ntier_colors = {'tiny': '#ef5350', 'small': '#ffa726', 'medium': '#66bb6a'}\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor tier, color in tier_colors.items():\n    sub = model_direct[model_direct['tier'] == tier]\n    ax.scatter(sub['params_b'], sub['mean_f1'], c=color, s=120,\n              label=tier.capitalize(), edgecolors='black', zorder=3)\n    for _, row in sub.iterrows():\n        ax.annotate(row['model_short'], (row['params_b'], row['mean_f1']),\n                    textcoords='offset points', xytext=(8, 4), fontsize=9)\n\n# Log-linear regression\nif len(model_direct) >= 3:\n    log_params = np.log10(model_direct['params_b'])\n    slope, intercept, r, p, se = scipy_stats.linregress(log_params, model_direct['mean_f1'])\n    x_fit = np.linspace(log_params.min() - 0.1, log_params.max() + 0.1, 50)\n    ax.plot(10**x_fit, slope * x_fit + intercept, '--', color='grey', alpha=0.6,\n            label=f'Log-linear fit (R²={r**2:.2f})')\n\nax.set_xscale('log')\nax.set_xlabel('Parameters (Billions, log scale)')\nax.set_ylabel('Mean F1 (Direct, dataset-stratified)')\nax.set_title('Direct Performance Scaling Curve')\nax.legend(title='Tier')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG Uplift by Model Size\n",
    "\n",
    "Overlay best-RAG curve on direct curve — the key thesis figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stratified: compute best/mean RAG per (model, dataset), then average across datasets\nrag_df = df[df['exp_type'] == 'rag'].dropna(subset=['params_b', PRIMARY_METRIC])\n\nper_ds_rag = rag_df.groupby(['model_short', 'dataset'])[PRIMARY_METRIC].agg(['max', 'mean'])\nper_ds_rag.columns = ['best_rag_f1', 'mean_rag_f1']\nmodel_rag_agg = per_ds_rag.groupby('model_short').mean().reset_index()\n\nmodel_best_rag = rag_df.groupby('model_short').agg(\n    params_b=('params_b', 'first'),\n    tier=('tier', 'first'),\n).reset_index().merge(model_rag_agg, on='model_short')\n\n# Merge direct and RAG data\nscaling = model_direct[['model_short', 'mean_f1', 'params_b', 'tier']].rename(\n    columns={'mean_f1': 'direct_f1'}\n).merge(model_best_rag[['model_short', 'best_rag_f1', 'mean_rag_f1']], on='model_short', how='outer')\nscaling['rag_uplift'] = scaling['best_rag_f1'] - scaling['direct_f1']\n\nfig, ax = plt.subplots(figsize=(11, 6))\n\n# Sort by params for line plot\nscaling_sorted = scaling.dropna(subset=['params_b']).sort_values('params_b')\n\nax.plot(scaling_sorted['params_b'], scaling_sorted['direct_f1'],\n        'o-', color='steelblue', markersize=8, label='Direct (no retrieval)', linewidth=2)\nax.plot(scaling_sorted['params_b'], scaling_sorted['best_rag_f1'],\n        's-', color='coral', markersize=8, label='Best RAG (dataset-stratified)', linewidth=2)\nax.plot(scaling_sorted['params_b'], scaling_sorted['mean_rag_f1'],\n        '^--', color='#ffa726', markersize=7, label='Mean RAG (dataset-stratified)', linewidth=1.5, alpha=0.7)\n\n# Annotate models\nfor _, row in scaling_sorted.iterrows():\n    ax.annotate(row['model_short'], (row['params_b'], row['best_rag_f1']),\n                textcoords='offset points', xytext=(8, 6), fontsize=8)\n\n# Shade the RAG uplift region\nax.fill_between(scaling_sorted['params_b'],\n                scaling_sorted['direct_f1'], scaling_sorted['best_rag_f1'],\n                alpha=0.1, color='coral', label='RAG uplift zone')\n\nax.set_xscale('log')\nax.set_xlabel('Parameters (Billions, log scale)')\nax.set_ylabel('F1')\nax.set_title('Model Scaling: Direct vs Best RAG (dataset-stratified)')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\ndisplay(scaling.round(4))"
  },
  {
   "cell_type": "markdown",
   "source": "### Multi-Metric Scaling\n\nDo all quality dimensions scale the same way with model size?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multi-metric scaling: best RAG performance per model across metrics (dataset-stratified)\nscaling_metrics = [m for m in available_metrics if m != 'hallucination']  # hallucination is inverted\nrag_df = df[df['exp_type'] == 'rag'].dropna(subset=['params_b'])\n\nif len(scaling_metrics) >= 2 and not rag_df.empty:\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n    # Left: Best RAG per model across metrics (stratified by dataset)\n    ax = axes[0]\n    for metric in scaling_metrics:\n        metric_data = rag_df.dropna(subset=[metric])\n        if metric_data.empty:\n            continue\n        # Per (model, dataset) best, then average across datasets\n        per_ds_best = metric_data.groupby(['model_short', 'dataset'])[metric].max()\n        model_best = per_ds_best.groupby('model_short').mean()\n        params_map = metric_data.groupby('model_short')['params_b'].first()\n        plot_df = pd.DataFrame({'best': model_best, 'params_b': params_map}).dropna().sort_values('params_b')\n        ax.plot(plot_df['params_b'], plot_df['best'], 'o-', label=metric, markersize=6)\n\n    ax.set_xscale('log')\n    ax.set_xlabel('Parameters (Billions, log scale)')\n    ax.set_ylabel('Best Score (dataset-stratified)')\n    ax.set_title('Best RAG Performance by Model Size — Multiple Metrics')\n    ax.legend(fontsize=9)\n    ax.grid(alpha=0.3)\n\n    # Right: RAG uplift (best_rag - direct) per model across metrics (stratified)\n    ax2 = axes[1]\n    direct_df = df[df['exp_type'] == 'direct'].dropna(subset=['params_b'])\n\n    for metric in scaling_metrics:\n        # Stratified direct means: per (model, dataset) then average across datasets\n        d_per_ds = direct_df.dropna(subset=[metric]).groupby(['model_short', 'dataset'])[metric].mean()\n        d_means = d_per_ds.groupby('model_short').mean()\n        d_params = direct_df.groupby('model_short')['params_b'].first()\n\n        # Stratified RAG best: per (model, dataset) then average across datasets\n        r_per_ds = rag_df.dropna(subset=[metric]).groupby(['model_short', 'dataset'])[metric].max()\n        r_best = r_per_ds.groupby('model_short').mean()\n\n        merged = pd.DataFrame({\n            'direct_mean': d_means, 'best_rag': r_best, 'params_b': d_params,\n        }).dropna()\n        merged['uplift'] = merged['best_rag'] - merged['direct_mean']\n        merged = merged.sort_values('params_b')\n\n        if not merged.empty:\n            ax2.plot(merged['params_b'], merged['uplift'], 'o-', label=metric, markersize=6)\n\n    ax2.set_xscale('log')\n    ax2.axhline(y=0, color='grey', ls='--', alpha=0.4)\n    ax2.set_xlabel('Parameters (Billions, log scale)')\n    ax2.set_ylabel('RAG Uplift (best_RAG - direct)')\n    ax2.set_title('RAG Uplift by Model Size — Multiple Metrics (dataset-stratified)')\n    ax2.legend(fontsize=9)\n    ax2.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough metrics with data for multi-metric scaling analysis.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can Small+RAG Match Medium+Direct?\n",
    "\n",
    "For each tier, compare best-RAG performance against the direct performance of the next tier up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tier comparison using dataset-stratified scaling table (built in cell-5)\ntier_order = ['tiny', 'small', 'medium']\ntier_direct = {}\ntier_best_rag = {}\n\nfor tier in tier_order:\n    tier_models = scaling[scaling['tier'] == tier]\n    if not tier_models.empty:\n        tier_direct[tier] = tier_models['direct_f1'].mean()\n        tier_best_rag[tier] = tier_models['best_rag_f1'].max()\n\ncomparison_rows = []\nfor i, tier in enumerate(tier_order[:-1]):\n    next_tier = tier_order[i + 1]\n    if tier in tier_best_rag and next_tier in tier_direct:\n        rag_perf = tier_best_rag[tier]\n        next_direct = tier_direct[next_tier]\n        ratio = rag_perf / next_direct if next_direct > 0 else np.nan\n        comparison_rows.append({\n            'tier': tier,\n            f'{tier}_direct': tier_direct.get(tier, np.nan),\n            f'{tier}_best_rag': rag_perf,\n            f'{next_tier}_direct': next_direct,\n            'compensation_ratio': ratio,\n            'can_match': rag_perf >= next_direct,\n        })\n\ncomp_df = pd.DataFrame(comparison_rows)\nprint(\"(All values are dataset-stratified means)\")\ndisplay(comp_df.round(4))\n\n# Visualization\nfig, ax = plt.subplots(figsize=(10, 5))\nx = np.arange(len(tier_order))\nw = 0.3\n\ndirect_vals = [tier_direct.get(t, 0) for t in tier_order]\nrag_vals = [tier_best_rag.get(t, 0) for t in tier_order]\n\nax.bar(x - w/2, direct_vals, w, label='Direct', color='steelblue', alpha=0.8)\nax.bar(x + w/2, rag_vals, w, label='Best RAG', color='coral', alpha=0.8)\n\n# Draw arrows showing compensation\nfor i in range(len(tier_order) - 1):\n    if rag_vals[i] > 0 and direct_vals[i+1] > 0:\n        ax.annotate('', xy=(x[i+1] - w/2, direct_vals[i+1]),\n                    xytext=(x[i] + w/2, rag_vals[i]),\n                    arrowprops=dict(arrowstyle='->', color='grey', lw=1.5, ls='--'))\n\nax.set_xticks(x)\nax.set_xticklabels([f\"{t.capitalize()}\\n({', '.join(scaling[scaling['tier']==t]['model_short'].tolist())})\" for t in tier_order],\n                    fontsize=9)\nax.set_ylabel('F1 (dataset-stratified)')\nax.set_title('Can Small+RAG Match Medium+Direct? (dataset-stratified)')\nax.legend()\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Matters for Small vs Medium Models?\n",
    "\n",
    "Side-by-side variance decomposition for small and medium models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_bottlenecks = {}\n",
    "for tier in ['small', 'medium']:\n",
    "    tier_df = df[(df['tier'] == tier) & (df['exp_type'] == 'rag')]\n",
    "    if len(tier_df) >= 10:\n",
    "        bn = identify_bottlenecks(tier_df, PRIMARY_METRIC)\n",
    "        tier_bottlenecks[tier] = bn\n",
    "        print(f\"\\n{tier.capitalize()} models — variance explained:\")\n",
    "        for factor, pct in bn.items():\n",
    "            print(f\"  {factor:<20s}: {pct:5.1f}%\")\n",
    "\n",
    "# Side-by-side bar chart\n",
    "if len(tier_bottlenecks) >= 2:\n",
    "    all_factors = sorted(set().union(*[bn.keys() for bn in tier_bottlenecks.values()]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(all_factors))\n",
    "    w = 0.35\n",
    "\n",
    "    for i, (tier, bn) in enumerate(tier_bottlenecks.items()):\n",
    "        vals = [bn.get(f, 0) for f in all_factors]\n",
    "        color = '#ffa726' if tier == 'small' else '#66bb6a'\n",
    "        ax.bar(x + (i - 0.5) * w, vals, w, label=tier.capitalize(), color=color, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_factors, rotation=30, ha='right')\n",
    "    ax.set_ylabel('Variance Explained (%)')\n",
    "    ax.set_title('What Matters: Small vs Medium Models')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Also do tiny if available\n",
    "tiny_df = df[(df['tier'] == 'tiny') & (df['exp_type'] == 'rag')]\n",
    "if len(tiny_df) >= 10:\n",
    "    bn = identify_bottlenecks(tiny_df, PRIMARY_METRIC)\n",
    "    print(f\"\\nTiny models — variance explained:\")\n",
    "    for factor, pct in bn.items():\n",
    "        print(f\"  {factor:<20s}: {pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thesis Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Thesis verdict — uses dataset-stratified scaling table from cell-5\nverdict_rows = []\nfor tier in tier_order:\n    tier_models = scaling[scaling['tier'] == tier]\n    if tier_models.empty:\n        continue\n\n    direct_mean = tier_models['direct_f1'].mean()\n    best_rag = tier_models['best_rag_f1'].max()\n    uplift = best_rag - direct_mean\n\n    # Can this tier's best RAG match the next tier's direct?\n    tier_idx = tier_order.index(tier)\n    if tier_idx < len(tier_order) - 1:\n        next_tier = tier_order[tier_idx + 1]\n        next_direct = tier_direct.get(next_tier, np.nan)\n        can_match = best_rag >= next_direct if not np.isnan(next_direct) else None\n    else:\n        can_match = 'N/A (largest tier)'\n\n    verdict_rows.append({\n        'tier': tier,\n        'models': ', '.join(tier_models['model_short'].tolist()),\n        'direct_f1': direct_mean,\n        'best_rag_f1': best_rag,\n        'rag_uplift': uplift,\n        'can_match_next_tier': can_match,\n    })\n\nverdict_df = pd.DataFrame(verdict_rows)\nprint(\"THESIS VERDICT: Model Scaling Summary (dataset-stratified)\")\nprint(\"=\" * 70)\ndisplay(verdict_df.round(4))"
  },
  {
   "cell_type": "code",
   "source": "rag_with_params = df[(df['exp_type'] == 'rag')].dropna(subset=['params_b']).copy()\n\ngroundedness_available = [m for m in ['faithfulness', 'hallucination', 'answer_in_context', 'context_recall']\n                          if m in rag_with_params.columns and rag_with_params[m].notna().sum() >= 10]\n\nif groundedness_available:\n    n_metrics = len(groundedness_available)\n    fig, axes = plt.subplots(1, n_metrics, figsize=(6 * n_metrics, 5))\n    if n_metrics == 1:\n        axes = [axes]\n\n    for ax, metric in zip(axes, groundedness_available):\n        metric_data = rag_with_params.dropna(subset=[metric])\n        # Stratified: per (model, dataset) mean, then average across datasets\n        per_ds = metric_data.groupby(['model_short', 'dataset'])[metric].mean()\n        model_mean = per_ds.groupby('model_short').mean()\n        model_std = per_ds.groupby('model_short').std().fillna(0)\n        model_n = metric_data.groupby('model_short')[metric].count()\n        model_stats = metric_data.groupby('model_short').agg(\n            params_b=('params_b', 'first'),\n            tier=('tier', 'first'),\n        )\n        model_stats['mean'] = model_mean\n        model_stats['std'] = model_std\n        model_stats['n'] = model_n\n        model_stats = model_stats.sort_values('params_b')\n\n        for tier, color in tier_colors.items():\n            sub = model_stats[model_stats['tier'] == tier]\n            ci = 1.96 * sub['std'] / np.sqrt(np.maximum(sub['n'], 1))\n            ax.errorbar(sub['params_b'], sub['mean'], yerr=ci,\n                       fmt='o', color=color, markersize=8, capsize=4,\n                       label=tier.capitalize())\n            for name, row in sub.iterrows():\n                ax.annotate(name, (row['params_b'], row['mean']),\n                           textcoords='offset points', xytext=(6, 4), fontsize=8)\n\n        # Trend line\n        if len(model_stats) >= 3:\n            log_p = np.log10(model_stats['params_b'])\n            slope, intercept, r, p, _ = scipy_stats.linregress(log_p, model_stats['mean'])\n            x_fit = np.linspace(log_p.min() - 0.1, log_p.max() + 0.1, 50)\n            ax.plot(10**x_fit, slope * x_fit + intercept, '--', color='grey', alpha=0.5)\n            ax.set_title(f'{metric}\\n(slope={slope:.3f}, R²={r**2:.2f})')\n        else:\n            ax.set_title(metric)\n\n        ax.set_xscale('log')\n        ax.set_xlabel('Parameters (B)')\n        ax.set_ylabel(f'Mean {metric} (dataset-stratified)')\n        ax.legend(fontsize=8)\n        ax.grid(alpha=0.3)\n\n    plt.suptitle('Groundedness Metrics by Model Size (dataset-stratified)', y=1.02, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    # Summary table — stratified by dataset\n    print(\"\\nGroundedness by Model Tier (dataset-stratified means):\")\n    print(\"=\" * 70)\n    for metric in groundedness_available:\n        # Per (tier, dataset) mean, then average across datasets per tier\n        per_ds = rag_with_params.groupby(['tier', 'dataset'])[metric].mean()\n        tier_means = per_ds.groupby('tier').mean()\n        tier_counts = rag_with_params.groupby('tier')[metric].count()\n        print(f\"\\n  {metric}:\")\n        for tier in ['tiny', 'small', 'medium']:\n            if tier in tier_means.index:\n                print(f\"    {tier:<8s}: {tier_means[tier]:.3f} \"\n                      f\"(n={int(tier_counts.get(tier, 0))})\")\n\n    # Key insight: does faithfulness scale differently from F1?\n    if 'faithfulness' in groundedness_available and PRIMARY_METRIC in rag_with_params.columns:\n        print(\"\\n  F1 vs Faithfulness scaling gap (dataset-stratified):\")\n        for tier in ['tiny', 'small', 'medium']:\n            t_data = rag_with_params[rag_with_params['tier'] == tier]\n            if len(t_data) >= 5:\n                # Stratified means\n                f1_per_ds = t_data.groupby('dataset')[PRIMARY_METRIC].mean()\n                faith_per_ds = t_data.groupby('dataset')['faithfulness'].mean()\n                f1_mean = f1_per_ds.mean()\n                faith_mean = faith_per_ds.mean()\n                print(f\"    {tier:<8s}: F1={f1_mean:.3f}, faith={faith_mean:.3f}, \"\n                      f\"gap={faith_mean - f1_mean:+.3f}\")\nelse:\n    print(\"No groundedness metrics available. Run:\")\n    print(\"  uv run ragicamp compute-metrics outputs/smart_retrieval_slm \"\n          \"-m faithfulness,hallucination,answer_in_context,context_recall\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Faithfulness & Hallucination by Model Size\n\n**Key safety question:** Do bigger models hallucinate less? Are smaller models less faithful to context?\n\nThis matters for deployment: if small models are equally faithful when given good context,\nthey may be safer than their F1 gap suggests.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- Direct performance scales with model size (log-linear relationship)\n",
    "- RAG uplift magnitude vs model size\n",
    "- Whether small+premium RAG can match medium+direct\n",
    "- Different components matter for small vs medium models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}