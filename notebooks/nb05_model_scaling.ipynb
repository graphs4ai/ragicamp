{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB05: Model Scaling\n",
    "\n",
    "**Question:** Can small models + premium retrieval match larger models without retrieval?\n",
    "\n",
    "This notebook explores the scaling relationship:\n",
    "- Direct performance vs model size\n",
    "- RAG uplift vs model size\n",
    "- Whether small+RAG can compensate for model size\n",
    "- What matters most for small vs medium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, identify_bottlenecks,\n",
    "    weighted_mean_with_ci,\n",
    "    PRIMARY_METRIC, MODEL_PARAMS, MODEL_TIER, BROKEN_MODELS,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "\n",
    "# Add parameter count and tier columns\n",
    "df['params_b'] = df['model_short'].map(MODEL_PARAMS)\n",
    "df['tier'] = df['model_short'].map(MODEL_TIER)\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "print(f\"Models with params: {df['params_b'].notna().sum()}\")\n",
    "print(f\"Tiers: {df['tier'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct Scaling Curve\n",
    "\n",
    "F1 vs model parameters (log scale) for direct (no-retrieval) experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = df[df['exp_type'] == 'direct'].dropna(subset=['params_b', PRIMARY_METRIC])\n",
    "model_direct = direct.groupby('model_short').agg(\n",
    "    mean_f1=(PRIMARY_METRIC, 'mean'),\n",
    "    std_f1=(PRIMARY_METRIC, 'std'),\n",
    "    n=(PRIMARY_METRIC, 'count'),\n",
    "    params_b=('params_b', 'first'),\n",
    "    tier=('tier', 'first'),\n",
    ").reset_index()\n",
    "\n",
    "tier_colors = {'tiny': '#ef5350', 'small': '#ffa726', 'medium': '#66bb6a'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for tier, color in tier_colors.items():\n",
    "    sub = model_direct[model_direct['tier'] == tier]\n",
    "    ax.scatter(sub['params_b'], sub['mean_f1'], c=color, s=120,\n",
    "              label=tier.capitalize(), edgecolors='black', zorder=3)\n",
    "    for _, row in sub.iterrows():\n",
    "        ax.annotate(row['model_short'], (row['params_b'], row['mean_f1']),\n",
    "                    textcoords='offset points', xytext=(8, 4), fontsize=9)\n",
    "\n",
    "# Log-linear regression\n",
    "if len(model_direct) >= 3:\n",
    "    log_params = np.log10(model_direct['params_b'])\n",
    "    slope, intercept, r, p, se = scipy_stats.linregress(log_params, model_direct['mean_f1'])\n",
    "    x_fit = np.linspace(log_params.min() - 0.1, log_params.max() + 0.1, 50)\n",
    "    ax.plot(10**x_fit, slope * x_fit + intercept, '--', color='grey', alpha=0.6,\n",
    "            label=f'Log-linear fit (R²={r**2:.2f})')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (Billions, log scale)')\n",
    "ax.set_ylabel('Mean F1 (Direct)')\n",
    "ax.set_title('Direct Performance Scaling Curve')\n",
    "ax.legend(title='Tier')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG Uplift by Model Size\n",
    "\n",
    "Overlay best-RAG curve on direct curve — the key thesis figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df = df[df['exp_type'] == 'rag'].dropna(subset=['params_b', PRIMARY_METRIC])\n",
    "model_best_rag = rag_df.groupby('model_short').agg(\n",
    "    best_rag_f1=(PRIMARY_METRIC, 'max'),\n",
    "    mean_rag_f1=(PRIMARY_METRIC, 'mean'),\n",
    "    params_b=('params_b', 'first'),\n",
    "    tier=('tier', 'first'),\n",
    ").reset_index()\n",
    "\n",
    "# Merge direct and RAG data\n",
    "scaling = model_direct[['model_short', 'mean_f1', 'params_b', 'tier']].rename(\n",
    "    columns={'mean_f1': 'direct_f1'}\n",
    ").merge(model_best_rag[['model_short', 'best_rag_f1', 'mean_rag_f1']], on='model_short', how='outer')\n",
    "scaling['rag_uplift'] = scaling['best_rag_f1'] - scaling['direct_f1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "# Sort by params for line plot\n",
    "scaling_sorted = scaling.dropna(subset=['params_b']).sort_values('params_b')\n",
    "\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['direct_f1'],\n",
    "        'o-', color='steelblue', markersize=8, label='Direct (no retrieval)', linewidth=2)\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['best_rag_f1'],\n",
    "        's-', color='coral', markersize=8, label='Best RAG', linewidth=2)\n",
    "ax.plot(scaling_sorted['params_b'], scaling_sorted['mean_rag_f1'],\n",
    "        '^--', color='#ffa726', markersize=7, label='Mean RAG', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Annotate models\n",
    "for _, row in scaling_sorted.iterrows():\n",
    "    ax.annotate(row['model_short'], (row['params_b'], row['best_rag_f1']),\n",
    "                textcoords='offset points', xytext=(8, 6), fontsize=8)\n",
    "\n",
    "# Shade the RAG uplift region\n",
    "ax.fill_between(scaling_sorted['params_b'],\n",
    "                scaling_sorted['direct_f1'], scaling_sorted['best_rag_f1'],\n",
    "                alpha=0.1, color='coral', label='RAG uplift zone')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (Billions, log scale)')\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('Model Scaling: Direct vs Best RAG')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(scaling.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can Small+RAG Match Medium+Direct?\n",
    "\n",
    "For each tier, compare best-RAG performance against the direct performance of the next tier up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_order = ['tiny', 'small', 'medium']\n",
    "tier_direct = {}\n",
    "tier_best_rag = {}\n",
    "\n",
    "for tier in tier_order:\n",
    "    tier_models = scaling[scaling['tier'] == tier]\n",
    "    if not tier_models.empty:\n",
    "        tier_direct[tier] = tier_models['direct_f1'].mean()\n",
    "        tier_best_rag[tier] = tier_models['best_rag_f1'].max()\n",
    "\n",
    "comparison_rows = []\n",
    "for i, tier in enumerate(tier_order[:-1]):\n",
    "    next_tier = tier_order[i + 1]\n",
    "    if tier in tier_best_rag and next_tier in tier_direct:\n",
    "        rag_perf = tier_best_rag[tier]\n",
    "        next_direct = tier_direct[next_tier]\n",
    "        ratio = rag_perf / next_direct if next_direct > 0 else np.nan\n",
    "        comparison_rows.append({\n",
    "            'tier': tier,\n",
    "            f'{tier}_direct': tier_direct.get(tier, np.nan),\n",
    "            f'{tier}_best_rag': rag_perf,\n",
    "            f'{next_tier}_direct': next_direct,\n",
    "            'compensation_ratio': ratio,\n",
    "            'can_match': rag_perf >= next_direct,\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "display(comp_df.round(4))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(tier_order))\n",
    "w = 0.3\n",
    "\n",
    "direct_vals = [tier_direct.get(t, 0) for t in tier_order]\n",
    "rag_vals = [tier_best_rag.get(t, 0) for t in tier_order]\n",
    "\n",
    "ax.bar(x - w/2, direct_vals, w, label='Direct', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + w/2, rag_vals, w, label='Best RAG', color='coral', alpha=0.8)\n",
    "\n",
    "# Draw arrows showing compensation\n",
    "for i in range(len(tier_order) - 1):\n",
    "    if rag_vals[i] > 0 and direct_vals[i+1] > 0:\n",
    "        ax.annotate('', xy=(x[i+1] - w/2, direct_vals[i+1]),\n",
    "                    xytext=(x[i] + w/2, rag_vals[i]),\n",
    "                    arrowprops=dict(arrowstyle='->', color='grey', lw=1.5, ls='--'))\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{t.capitalize()}\\n({', '.join(scaling[scaling['tier']==t]['model_short'].tolist())})\" for t in tier_order],\n",
    "                    fontsize=9)\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('Can Small+RAG Match Medium+Direct?')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What Matters for Small vs Medium Models?\n",
    "\n",
    "Side-by-side variance decomposition for small and medium models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_bottlenecks = {}\n",
    "for tier in ['small', 'medium']:\n",
    "    tier_df = df[(df['tier'] == tier) & (df['exp_type'] == 'rag')]\n",
    "    if len(tier_df) >= 10:\n",
    "        bn = identify_bottlenecks(tier_df, PRIMARY_METRIC)\n",
    "        tier_bottlenecks[tier] = bn\n",
    "        print(f\"\\n{tier.capitalize()} models — variance explained:\")\n",
    "        for factor, pct in bn.items():\n",
    "            print(f\"  {factor:<20s}: {pct:5.1f}%\")\n",
    "\n",
    "# Side-by-side bar chart\n",
    "if len(tier_bottlenecks) >= 2:\n",
    "    all_factors = sorted(set().union(*[bn.keys() for bn in tier_bottlenecks.values()]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(all_factors))\n",
    "    w = 0.35\n",
    "\n",
    "    for i, (tier, bn) in enumerate(tier_bottlenecks.items()):\n",
    "        vals = [bn.get(f, 0) for f in all_factors]\n",
    "        color = '#ffa726' if tier == 'small' else '#66bb6a'\n",
    "        ax.bar(x + (i - 0.5) * w, vals, w, label=tier.capitalize(), color=color, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_factors, rotation=30, ha='right')\n",
    "    ax.set_ylabel('Variance Explained (%)')\n",
    "    ax.set_title('What Matters: Small vs Medium Models')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Also do tiny if available\n",
    "tiny_df = df[(df['tier'] == 'tiny') & (df['exp_type'] == 'rag')]\n",
    "if len(tiny_df) >= 10:\n",
    "    bn = identify_bottlenecks(tiny_df, PRIMARY_METRIC)\n",
    "    print(f\"\\nTiny models — variance explained:\")\n",
    "    for factor, pct in bn.items():\n",
    "        print(f\"  {factor:<20s}: {pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thesis Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table: tier, direct_f1, best_rag_f1, uplift, can_match_next_tier\n",
    "verdict_rows = []\n",
    "for tier in tier_order:\n",
    "    tier_models = scaling[scaling['tier'] == tier]\n",
    "    if tier_models.empty:\n",
    "        continue\n",
    "\n",
    "    direct_mean = tier_models['direct_f1'].mean()\n",
    "    best_rag = tier_models['best_rag_f1'].max()\n",
    "    uplift = best_rag - direct_mean\n",
    "\n",
    "    # Can this tier's best RAG match the next tier's direct?\n",
    "    tier_idx = tier_order.index(tier)\n",
    "    if tier_idx < len(tier_order) - 1:\n",
    "        next_tier = tier_order[tier_idx + 1]\n",
    "        next_direct = tier_direct.get(next_tier, np.nan)\n",
    "        can_match = best_rag >= next_direct if not np.isnan(next_direct) else None\n",
    "    else:\n",
    "        can_match = 'N/A (largest tier)'\n",
    "\n",
    "    verdict_rows.append({\n",
    "        'tier': tier,\n",
    "        'models': ', '.join(tier_models['model_short'].tolist()),\n",
    "        'direct_f1': direct_mean,\n",
    "        'best_rag_f1': best_rag,\n",
    "        'rag_uplift': uplift,\n",
    "        'can_match_next_tier': can_match,\n",
    "    })\n",
    "\n",
    "verdict_df = pd.DataFrame(verdict_rows)\n",
    "print(\"THESIS VERDICT: Model Scaling Summary\")\n",
    "print(\"=\" * 70)\n",
    "display(verdict_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- Direct performance scales with model size (log-linear relationship)\n",
    "- RAG uplift magnitude vs model size\n",
    "- Whether small+premium RAG can match medium+direct\n",
    "- Different components matter for small vs medium models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
