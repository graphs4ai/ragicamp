{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Next Experiments Prioritization\n",
    "\n",
    "**Purpose:** Identify gaps and prioritize what to test next.\n",
    "\n",
    "**Key Questions:**\n",
    "- What configurations haven't been tested yet?\n",
    "- Which components have the most improvement potential?\n",
    "- What specific experiments should we run next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting,\n",
    "    identify_bottlenecks, compute_marginal_means,\n",
    "    PRIMARY_METRIC\n",
    ")\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setup_plotting()\n",
    "\n",
    "# Load data\n",
    "df = load_all_results()\n",
    "rag_df = df[df['exp_type'] == 'rag']\n",
    "\n",
    "print(f\"Loaded {len(rag_df)} RAG experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Configuration Space Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_configuration_coverage(df, factors):\n",
    "    \"\"\"Analyze which configurations have been tested and identify gaps.\"\"\"\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    available_factors = [f for f in factors if f in rag_df.columns]\n",
    "    \n",
    "    if not available_factors:\n",
    "        return {}\n",
    "    \n",
    "    factor_values = {f: set(rag_df[f].dropna().unique()) for f in available_factors}\n",
    "    config_counts = rag_df.groupby(available_factors).size().reset_index(name='count')\n",
    "    \n",
    "    theoretical_combos = list(product(*[factor_values[f] for f in available_factors]))\n",
    "    n_theoretical = len(theoretical_combos)\n",
    "    n_actual = len(config_counts)\n",
    "    \n",
    "    tested_combos = set(tuple(row) for row in config_counts[available_factors].values)\n",
    "    \n",
    "    missing = []\n",
    "    if n_theoretical < 10000:\n",
    "        for combo in theoretical_combos:\n",
    "            if combo not in tested_combos:\n",
    "                missing.append(dict(zip(available_factors, combo)))\n",
    "    \n",
    "    return {\n",
    "        'factors': available_factors,\n",
    "        'factor_values': factor_values,\n",
    "        'n_theoretical': n_theoretical,\n",
    "        'n_actual': n_actual,\n",
    "        'coverage_pct': n_actual / n_theoretical * 100 if n_theoretical > 0 else 0,\n",
    "        'missing_combinations': missing[:20],\n",
    "        'n_missing': len(missing) if n_theoretical < 10000 else n_theoretical - n_actual\n",
    "    }\n",
    "\n",
    "\n",
    "if len(rag_df) > 0:\n",
    "    print(\"Configuration Space Coverage\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    core_factors = ['retriever_type', 'reranker', 'prompt', 'embedding_model']\n",
    "    coverage = analyze_configuration_coverage(df, core_factors)\n",
    "    \n",
    "    if coverage:\n",
    "        print(f\"Factors analyzed: {', '.join(coverage['factors'])}\")\n",
    "        print(f\"Theoretical combinations: {coverage['n_theoretical']}\")\n",
    "        print(f\"Tested combinations: {coverage['n_actual']}\")\n",
    "        print(f\"Coverage: {coverage['coverage_pct']:.1f}%\")\n",
    "        \n",
    "        if coverage['n_missing'] > 0:\n",
    "            print(f\"\\nUntested combinations worth exploring ({min(5, coverage['n_missing'])} shown):\")\n",
    "            for combo in coverage['missing_combinations'][:5]:\n",
    "                print(f\"  - {combo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Improvement Potential by Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_improvement_potential(df, factor, metric=PRIMARY_METRIC):\n",
    "    \"\"\"Estimate the potential improvement from optimizing a factor.\"\"\"\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    if factor not in rag_df.columns:\n",
    "        return {}\n",
    "    \n",
    "    factor_means = rag_df.groupby(factor)[metric].mean()\n",
    "    overall_mean = rag_df[metric].mean()\n",
    "    best_value = factor_means.idxmax()\n",
    "    best_mean = factor_means.max()\n",
    "    worst_mean = factor_means.min()\n",
    "    \n",
    "    improvement_potential = best_mean - overall_mean\n",
    "    value_range = best_mean - worst_mean\n",
    "    \n",
    "    return {\n",
    "        'factor': factor,\n",
    "        'best_value': best_value,\n",
    "        'best_mean': best_mean,\n",
    "        'worst_mean': worst_mean,\n",
    "        'overall_mean': overall_mean,\n",
    "        'improvement_potential': improvement_potential,\n",
    "        'value_range': value_range,\n",
    "        'pct_improvement': improvement_potential / overall_mean * 100 if overall_mean > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "if len(rag_df) > 0:\n",
    "    print(\"Improvement Potential by Factor\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_factors = ['model_short', 'retriever_type', 'embedding_model', 'reranker', \n",
    "                   'prompt', 'query_transform', 'top_k']\n",
    "    \n",
    "    potentials = []\n",
    "    for factor in all_factors:\n",
    "        if factor in rag_df.columns and rag_df[factor].nunique() > 1:\n",
    "            p = compute_improvement_potential(df, factor)\n",
    "            if p:\n",
    "                potentials.append(p)\n",
    "    \n",
    "    if potentials:\n",
    "        potential_df = pd.DataFrame(potentials)\n",
    "        potential_df = potential_df.sort_values('value_range', ascending=False)\n",
    "        display(potential_df[['factor', 'best_value', 'value_range', 'pct_improvement']].round(3))\n",
    "        \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"- 'value_range' = performance gap between best and worst option\")\n",
    "        print(\"- Higher range = factor matters more, worth optimizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Prioritized Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_next_experiments(df):\n",
    "    \"\"\"Generate prioritized recommendations for next experiments.\"\"\"\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    recommendations = []\n",
    "    \n",
    "    bottlenecks = identify_bottlenecks(df)\n",
    "    \n",
    "    for factor in ['model_short', 'retriever_type', 'embedding_model', 'reranker', 'prompt', 'query_transform']:\n",
    "        if factor not in rag_df.columns:\n",
    "            continue\n",
    "        \n",
    "        n_values = rag_df[factor].nunique()\n",
    "        variance_explained = bottlenecks.get(factor, 0)\n",
    "        potential = compute_improvement_potential(df, factor)\n",
    "        \n",
    "        if variance_explained > 10:\n",
    "            if n_values <= 3:\n",
    "                recommendations.append({\n",
    "                    'priority': 'HIGH',\n",
    "                    'action': f'Test more {factor} options',\n",
    "                    'reason': f'{factor} explains {variance_explained:.1f}% of variance but only {n_values} values tested',\n",
    "                    'current_best': potential.get('best_value', 'N/A'),\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'action': f'Consider new {factor} options',\n",
    "                    'reason': f'{factor} explains {variance_explained:.1f}% of variance, {n_values} values already tested',\n",
    "                    'current_best': potential.get('best_value', 'N/A'),\n",
    "                })\n",
    "    \n",
    "    for factor in ['top_k']:\n",
    "        if factor in rag_df.columns:\n",
    "            variance_explained = bottlenecks.get(factor, 0)\n",
    "            if variance_explained < 5:\n",
    "                recommendations.append({\n",
    "                    'priority': 'LOW',\n",
    "                    'action': f'Deprioritize {factor} experiments',\n",
    "                    'reason': f'{factor} explains only {variance_explained:.1f}% of variance',\n",
    "                    'current_best': str(rag_df.groupby(factor)[PRIMARY_METRIC].mean().idxmax()),\n",
    "                })\n",
    "    \n",
    "    priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}\n",
    "    return sorted(recommendations, key=lambda x: priority_order.get(x['priority'], 3))\n",
    "\n",
    "\n",
    "if len(rag_df) > 0:\n",
    "    print(\"Prioritized Recommendations\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = prioritize_next_experiments(df)\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        emoji = 'ðŸ”´' if rec['priority'] == 'HIGH' else 'ðŸŸ¡' if rec['priority'] == 'MEDIUM' else 'ðŸŸ¢'\n",
    "        print(f\"\\n[{rec['priority']}] {rec['action']}\")\n",
    "        print(f\"     Reason: {rec['reason']}\")\n",
    "        if rec['current_best'] != 'N/A':\n",
    "            print(f\"     Current best: {rec['current_best']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Concrete Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(rag_df) > 0:\n",
    "    print(\"Concrete Next Steps\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    bottlenecks = identify_bottlenecks(df)\n",
    "    suggestions = []\n",
    "    \n",
    "    # Check model diversity\n",
    "    if 'model_short' in rag_df.columns:\n",
    "        models_tested = rag_df['model_short'].unique()\n",
    "        if len(models_tested) <= 3:\n",
    "            suggestions.append(\"â†’ MODELS: Test newer/larger models (Llama-3.1-8B, Phi-3.5, Qwen-2.5-7B) to see scaling effects\")\n",
    "    \n",
    "    # Check prompt diversity\n",
    "    if 'prompt' in rag_df.columns:\n",
    "        prompts_tested = rag_df['prompt'].unique()\n",
    "        if 'cot' not in prompts_tested:\n",
    "            suggestions.append(\"â†’ PROMPTS: Add chain-of-thought prompting variants\")\n",
    "        if len(prompts_tested) <= 4:\n",
    "            suggestions.append(\"â†’ PROMPTS: Test domain-specific prompts or few-shot with different examples\")\n",
    "    \n",
    "    # Check reranker\n",
    "    if 'reranker' in rag_df.columns:\n",
    "        reranker_effect = bottlenecks.get('reranker', 0)\n",
    "        if reranker_effect > 5:\n",
    "            suggestions.append(\"â†’ RERANKER: Test cross-encoder rerankers (ms-marco, bge-reranker-large)\")\n",
    "    \n",
    "    # Check retriever\n",
    "    if 'retriever_type' in rag_df.columns:\n",
    "        best_retriever = rag_df.groupby('retriever_type')[PRIMARY_METRIC].mean().idxmax()\n",
    "        if best_retriever == 'hierarchical':\n",
    "            suggestions.append(\"â†’ RETRIEVER: Already using hierarchical; test chunk size variations\")\n",
    "        elif best_retriever in ['dense', 'hybrid']:\n",
    "            suggestions.append(\"â†’ RETRIEVER: Consider testing hierarchical retrieval or multi-hop retrieval\")\n",
    "    \n",
    "    # Check embedding model\n",
    "    if 'embedding_model' in rag_df.columns:\n",
    "        embeddings_tested = rag_df['embedding_model'].unique()\n",
    "        if len(embeddings_tested) < 4:\n",
    "            suggestions.append(\"â†’ EMBEDDINGS: Test more embedding models (Nomic, OpenAI, Cohere)\")\n",
    "    \n",
    "    if suggestions:\n",
    "        for s in suggestions:\n",
    "            print(f\"  {s}\")\n",
    "    else:\n",
    "        print(\"  Configuration space appears well-explored. Consider:\")\n",
    "        print(\"  - Testing on new/harder datasets\")\n",
    "        print(\"  - Measuring latency/cost tradeoffs\")\n",
    "        print(\"  - Ablation studies on best configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Statistical Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(rag_df) > 0:\n",
    "    print(\"Statistical Robustness\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data quality summary\n",
    "    total_experiments = len(df)\n",
    "    rag_experiments = len(rag_df)\n",
    "    direct_experiments = len(df[df['exp_type'] == 'direct'])\n",
    "    \n",
    "    print(f\"Total experiments: {total_experiments}\")\n",
    "    print(f\"  - RAG: {rag_experiments}\")\n",
    "    print(f\"  - Direct: {direct_experiments}\")\n",
    "    print(f\"Missing {PRIMARY_METRIC}: {df[PRIMARY_METRIC].isna().sum()} ({df[PRIMARY_METRIC].isna().mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Check for balance\n",
    "    if 'model_short' in rag_df.columns:\n",
    "        model_counts = rag_df['model_short'].value_counts()\n",
    "        balance_ratio = model_counts.min() / model_counts.max() if model_counts.max() > 0 else 0\n",
    "        print(f\"\\nModel balance ratio: {balance_ratio:.2f} (1.0 = perfectly balanced)\")\n",
    "        if balance_ratio < 0.5:\n",
    "            print(\"âš ï¸ Imbalanced model distribution may bias results\")\n",
    "    \n",
    "    # Sample size per factor level\n",
    "    print(\"\\nSample size per factor level:\")\n",
    "    for factor in ['retriever_type', 'reranker', 'prompt']:\n",
    "        if factor in rag_df.columns:\n",
    "            min_n = rag_df.groupby(factor).size().min()\n",
    "            print(f\"  {factor}: min n={min_n}\")\n",
    "            if min_n < 5:\n",
    "                print(f\"    âš ï¸ Some levels have <5 samples - consider adding more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Summary: Top 5 Next Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(rag_df) > 0:\n",
    "    print(\"TOP 5 NEXT EXPERIMENTS TO RUN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = prioritize_next_experiments(df)\n",
    "    high_priority = [r for r in recommendations if r['priority'] == 'HIGH']\n",
    "    \n",
    "    if high_priority:\n",
    "        print(\"\\nBased on the analysis, prioritize:\")\n",
    "        for i, rec in enumerate(high_priority[:5], 1):\n",
    "            print(f\"\\n{i}. {rec['action']}\")\n",
    "            print(f\"   {rec['reason']}\")\n",
    "    else:\n",
    "        print(\"\\nNo high-priority gaps identified.\")\n",
    "        print(\"Consider running ablation studies on the best configuration.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
