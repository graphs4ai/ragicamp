{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB09: Efficiency-Adjusted Performance\n",
    "\n",
    "**Question:** Is the best RAG config worth the extra latency/cost?\n",
    "\n",
    "This notebook computes cost-adjusted metrics:\n",
    "1. **Pareto frontier** — configs not dominated in (F1, latency) space\n",
    "2. **Cost per correct answer** — F1-adjusted throughput\n",
    "3. **Agent complexity vs improvement** — do extra steps pay off?\n",
    "4. **Practical recommendations** — best configs at different latency budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, weighted_mean_with_ci,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER, MODEL_PARAMS,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "df['tier'] = df['model_short'].map(MODEL_TIER)\n",
    "df['params_b'] = df['model_short'].map(MODEL_PARAMS)\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "print(f\"Duration available: {df['duration'].notna().sum()}\")\n",
    "print(f\"Throughput available: {df['throughput'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derived efficiency metrics\n",
    "df['has_timing'] = df['duration'].notna() & (df['duration'] > 0)\n",
    "eff = df[df['has_timing'] & df[PRIMARY_METRIC].notna()].copy()\n",
    "\n",
    "# Questions per second (throughput)\n",
    "eff['qps'] = np.where(eff['throughput'] > 0, eff['throughput'],\n",
    "                       np.where(eff['duration'] > 0,\n",
    "                                eff['n_samples'] / eff['duration'], np.nan))\n",
    "\n",
    "# Seconds per question\n",
    "eff['sec_per_q'] = np.where(eff['qps'] > 0, 1.0 / eff['qps'], np.nan)\n",
    "\n",
    "# \"Correct answers per second\" — F1 * qps\n",
    "eff['correct_per_sec'] = eff[PRIMARY_METRIC] * eff['qps']\n",
    "\n",
    "print(f\"Experiments with timing data: {len(eff)}\")\n",
    "if not eff.empty:\n",
    "    print(f\"Throughput range: {eff['qps'].min():.2f} - {eff['qps'].max():.2f} q/s\")\n",
    "    print(f\"Latency range: {eff['sec_per_q'].min():.2f} - {eff['sec_per_q'].max():.2f} s/q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pareto Frontier\n",
    "\n",
    "Which configurations are not dominated in (F1, latency) space? A config is Pareto-optimal if no other config has both higher F1 and lower latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pareto_frontier(df, x_col, y_col, minimize_x=True):\n",
    "    \"\"\"Compute Pareto frontier points.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        x_col: column to minimize (e.g. latency)\n",
    "        y_col: column to maximize (e.g. F1)\n",
    "        minimize_x: if True, lower x is better\n",
    "    \"\"\"\n",
    "    data = df[[x_col, y_col]].dropna()\n",
    "    if minimize_x:\n",
    "        data = data.sort_values(x_col)\n",
    "    else:\n",
    "        data = data.sort_values(x_col, ascending=False)\n",
    "\n",
    "    pareto_idx = []\n",
    "    best_y = -np.inf\n",
    "    for idx, row in data.iterrows():\n",
    "        if row[y_col] > best_y:\n",
    "            pareto_idx.append(idx)\n",
    "            best_y = row[y_col]\n",
    "\n",
    "    return df.loc[pareto_idx]\n",
    "\n",
    "\n",
    "if not eff.empty and eff['sec_per_q'].notna().sum() >= 5:\n",
    "    pareto = compute_pareto_frontier(eff, 'sec_per_q', PRIMARY_METRIC, minimize_x=True)\n",
    "    print(f\"Pareto-optimal configs: {len(pareto)} / {len(eff)}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # All experiments (background)\n",
    "    type_colors = {'direct': 'steelblue', 'rag': 'lightcoral'}\n",
    "    for exp_type in ['direct', 'rag']:\n",
    "        sub = eff[eff['exp_type'] == exp_type]\n",
    "        ax.scatter(sub['sec_per_q'], sub[PRIMARY_METRIC],\n",
    "                   s=20, alpha=0.3, color=type_colors.get(exp_type, 'grey'),\n",
    "                   label=f'{exp_type} (all)')\n",
    "\n",
    "    # Pareto frontier\n",
    "    pareto_sorted = pareto.sort_values('sec_per_q')\n",
    "    ax.plot(pareto_sorted['sec_per_q'], pareto_sorted[PRIMARY_METRIC],\n",
    "            'k-o', markersize=6, linewidth=2, label='Pareto frontier', zorder=5)\n",
    "\n",
    "    # Annotate Pareto points\n",
    "    for _, row in pareto_sorted.iterrows():\n",
    "        label = f\"{row.get('model_short', '')[:8]}\\n{row.get('agent_type', '')}\"  \n",
    "        ax.annotate(label, (row['sec_per_q'], row[PRIMARY_METRIC]),\n",
    "                    textcoords='offset points', xytext=(8, 4), fontsize=7)\n",
    "\n",
    "    ax.set_xlabel('Latency (seconds per question)')\n",
    "    ax.set_ylabel('F1')\n",
    "    ax.set_title('Pareto Frontier: F1 vs Latency')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Table of Pareto-optimal configs\n",
    "    display_cols = ['model_short', 'dataset', 'agent_type', 'retriever_type',\n",
    "                    'reranker', PRIMARY_METRIC, 'sec_per_q', 'qps']\n",
    "    display_cols = [c for c in display_cols if c in pareto.columns]\n",
    "    display(pareto_sorted[display_cols].round(4))\n",
    "else:\n",
    "    print(\"Insufficient timing data for Pareto analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost Per Correct Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not eff.empty:\n",
    "    # F1-weighted throughput by exp_type and agent_type\n",
    "    for group_col in ['exp_type', 'agent_type']:\n",
    "        if group_col not in eff.columns:\n",
    "            continue\n",
    "        stats = eff.groupby(group_col).agg(\n",
    "            mean_f1=(PRIMARY_METRIC, 'mean'),\n",
    "            mean_qps=('qps', 'mean'),\n",
    "            mean_sec_per_q=('sec_per_q', 'mean'),\n",
    "            mean_correct_per_sec=('correct_per_sec', 'mean'),\n",
    "            n=('qps', 'count'),\n",
    "        ).round(4)\n",
    "        print(f\"\\nEfficiency by {group_col}:\")\n",
    "        display(stats)\n",
    "\n",
    "    # Scatter: F1 vs correct_per_sec colored by agent_type\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    agent_colors = {'direct_llm': 'steelblue', 'fixed_rag': 'coral',\n",
    "                    'iterative_rag': '#66bb6a', 'self_rag': '#ffa726'}\n",
    "    for agent in sorted(eff['agent_type'].unique()):\n",
    "        sub = eff[eff['agent_type'] == agent]\n",
    "        ax.scatter(sub[PRIMARY_METRIC], sub['correct_per_sec'],\n",
    "                   s=30, alpha=0.5, color=agent_colors.get(agent, 'grey'),\n",
    "                   label=agent)\n",
    "    ax.set_xlabel('F1')\n",
    "    ax.set_ylabel('Correct Answers per Second (F1 * QPS)')\n",
    "    ax.set_title('Quality vs Efficiency by Agent Type')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Complexity vs Improvement\n",
    "\n",
    "Do iterative_rag and self_rag justify their extra latency with proportional F1 gains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not eff.empty:\n",
    "    rag_eff = eff[eff['exp_type'] == 'rag'].copy()\n",
    "\n",
    "    if not rag_eff.empty and rag_eff['agent_type'].nunique() > 1:\n",
    "        # Per model+dataset: compare fixed_rag baseline with advanced agents\n",
    "        group_cols = ['model_short', 'dataset']\n",
    "        rows = []\n",
    "\n",
    "        for group_vals, group_df in rag_eff.groupby(group_cols):\n",
    "            model, dataset = group_vals\n",
    "            fixed = group_df[group_df['agent_type'] == 'fixed_rag']\n",
    "            if fixed.empty or fixed[PRIMARY_METRIC].notna().sum() == 0:\n",
    "                continue\n",
    "\n",
    "            fixed_best_f1 = fixed[PRIMARY_METRIC].max()\n",
    "            fixed_mean_latency = fixed['sec_per_q'].mean()\n",
    "\n",
    "            for agent in ['iterative_rag', 'self_rag']:\n",
    "                advanced = group_df[group_df['agent_type'] == agent]\n",
    "                if advanced.empty or advanced[PRIMARY_METRIC].notna().sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                adv_best_f1 = advanced[PRIMARY_METRIC].max()\n",
    "                adv_mean_latency = advanced['sec_per_q'].mean()\n",
    "\n",
    "                rows.append({\n",
    "                    'model': model, 'dataset': dataset,\n",
    "                    'agent': agent,\n",
    "                    'fixed_f1': fixed_best_f1,\n",
    "                    'advanced_f1': adv_best_f1,\n",
    "                    'f1_delta': adv_best_f1 - fixed_best_f1,\n",
    "                    'fixed_latency': fixed_mean_latency,\n",
    "                    'advanced_latency': adv_mean_latency,\n",
    "                    'latency_ratio': adv_mean_latency / fixed_mean_latency if fixed_mean_latency > 0 else np.nan,\n",
    "                })\n",
    "\n",
    "        cost_df = pd.DataFrame(rows)\n",
    "        if not cost_df.empty:\n",
    "            print(\"Agent Complexity Cost-Benefit:\")\n",
    "            display(cost_df.round(4))\n",
    "\n",
    "            # Scatter: F1 delta vs latency ratio\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            for agent in cost_df['agent'].unique():\n",
    "                sub = cost_df[cost_df['agent'] == agent]\n",
    "                color = '#66bb6a' if agent == 'iterative_rag' else '#ffa726'\n",
    "                ax.scatter(sub['latency_ratio'], sub['f1_delta'],\n",
    "                           s=80, color=color, label=agent, edgecolors='black', alpha=0.7)\n",
    "                for _, row in sub.iterrows():\n",
    "                    ax.annotate(f\"{row['model'][:6]}\\n{row['dataset']}\",\n",
    "                                (row['latency_ratio'], row['f1_delta']),\n",
    "                                textcoords='offset points', xytext=(5, 3), fontsize=7)\n",
    "\n",
    "            ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Break-even (F1)')\n",
    "            ax.axvline(x=1, color='grey', linestyle='--', alpha=0.5, label='Same latency')\n",
    "            ax.set_xlabel('Latency Ratio (advanced / fixed_rag)')\n",
    "            ax.set_ylabel('F1 Delta (advanced - fixed_rag)')\n",
    "            ax.set_title('Agent Complexity: Is Extra Latency Worth It?')\n",
    "            ax.legend()\n",
    "            ax.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Summary\n",
    "            for agent in cost_df['agent'].unique():\n",
    "                sub = cost_df[cost_df['agent'] == agent]\n",
    "                n_better = (sub['f1_delta'] > 0).sum()\n",
    "                avg_ratio = sub['latency_ratio'].mean()\n",
    "                print(f\"\\n{agent}:\")\n",
    "                print(f\"  Better than fixed_rag in {n_better}/{len(sub)} scenarios\")\n",
    "                print(f\"  Average latency ratio: {avg_ratio:.2f}x\")\n",
    "                print(f\"  Mean F1 delta: {sub['f1_delta'].mean():+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Recommendations at Latency Budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not eff.empty:\n",
    "    # Best F1 at different latency budgets\n",
    "    latency_budgets = [0.5, 1.0, 2.0, 5.0, 10.0]  # seconds per question\n",
    "\n",
    "    print(\"Best Configurations at Different Latency Budgets:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    config_cols = ['model_short', 'agent_type', 'retriever_type', 'reranker',\n",
    "                   'prompt', PRIMARY_METRIC, 'sec_per_q']\n",
    "    config_cols = [c for c in config_cols if c in eff.columns]\n",
    "\n",
    "    budget_rows = []\n",
    "    for budget in latency_budgets:\n",
    "        within_budget = eff[eff['sec_per_q'] <= budget]\n",
    "        if within_budget.empty or within_budget[PRIMARY_METRIC].notna().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        best_idx = within_budget[PRIMARY_METRIC].idxmax()\n",
    "        best = within_budget.loc[best_idx]\n",
    "\n",
    "        budget_rows.append({\n",
    "            'budget_sec': budget,\n",
    "            'best_f1': best[PRIMARY_METRIC],\n",
    "            'model': best.get('model_short', 'N/A'),\n",
    "            'agent': best.get('agent_type', 'N/A'),\n",
    "            'retriever': best.get('retriever_type', 'N/A'),\n",
    "            'actual_latency': best.get('sec_per_q', np.nan),\n",
    "            'n_options': len(within_budget),\n",
    "        })\n",
    "\n",
    "    budget_df = pd.DataFrame(budget_rows)\n",
    "    if not budget_df.empty:\n",
    "        display(budget_df.round(4))\n",
    "\n",
    "        # Plot: latency budget vs best achievable F1\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(budget_df['budget_sec'], budget_df['best_f1'],\n",
    "                'o-', color='steelblue', markersize=8, linewidth=2)\n",
    "        for _, row in budget_df.iterrows():\n",
    "            ax.annotate(f\"{row['model']}\\n{row['agent']}\",\n",
    "                        (row['budget_sec'], row['best_f1']),\n",
    "                        textcoords='offset points', xytext=(8, -8), fontsize=8)\n",
    "        ax.set_xlabel('Latency Budget (seconds per question)')\n",
    "        ax.set_ylabel('Best Achievable F1')\n",
    "        ax.set_title('F1 vs Latency Budget: Practical Frontier')\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No configs within specified latency budgets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Key findings:\n",
    "- Pareto-optimal configurations in (F1, latency) space\n",
    "- Whether direct_llm offers better efficiency-adjusted performance\n",
    "- Whether advanced agents (iterative/self RAG) justify their latency overhead\n",
    "- Practical recommendations at different latency budgets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
