{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB07: Question-Level Error Analysis\n",
    "\n",
    "**Question:** What do models get wrong? Are there systematic error patterns?\n",
    "\n",
    "This notebook goes beyond aggregate metrics to inspect individual predictions:\n",
    "1. **Question difficulty spectrum** — easy / discriminating / hard\n",
    "2. **Error taxonomy** — classifying wrong answers\n",
    "3. **Answer length analysis** — does verbosity correlate with quality?\n",
    "4. **Model agreement** — do models agree on which questions are hard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting,\n",
    "    load_per_item_scores, compute_question_difficulty,\n",
    "    sample_predictions_with_docs,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "# Load per-item scores with text\n",
    "print(\"Loading per-item scores (this may take a minute)...\")\n",
    "item_scores = load_per_item_scores(STUDY_PATH, metric=PRIMARY_METRIC, include_text=True)\n",
    "item_scores = item_scores[~item_scores['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "print(f\"Loaded {len(item_scores)} per-item scores\")\n",
    "print(f\"  Experiments: {item_scores['experiment'].nunique()}\")\n",
    "print(f\"  Unique questions: {item_scores['idx'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Difficulty Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = compute_question_difficulty(item_scores, PRIMARY_METRIC, threshold=0.5)\n",
    "print(f\"Questions analyzed: {len(difficulty)}\")\n",
    "\n",
    "if not difficulty.empty:\n",
    "    diff_counts = difficulty['difficulty'].value_counts()\n",
    "    print(f\"\\nDifficulty distribution:\")\n",
    "    for cat in ['easy', 'discriminating', 'hard']:\n",
    "        n = diff_counts.get(cat, 0)\n",
    "        pct = n / len(difficulty) * 100\n",
    "        print(f\"  {cat:<15s}: {n:>4d} ({pct:.1f}%)\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram of pct_correct\n",
    "    axes[0].hist(difficulty['pct_correct'], bins=40, color='steelblue',\n",
    "                 alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(x=0.2, color='red', linestyle='--', label='Hard threshold')\n",
    "    axes[0].axvline(x=0.8, color='green', linestyle='--', label='Easy threshold')\n",
    "    axes[0].set_xlabel('% of Configs Getting Question Right (F1 >= 0.5)')\n",
    "    axes[0].set_ylabel('Number of Questions')\n",
    "    axes[0].set_title('Question Difficulty Spectrum')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Pie chart of difficulty categories\n",
    "    colors = {'easy': '#66bb6a', 'discriminating': '#ffa726', 'hard': '#ef5350'}\n",
    "    cats = ['easy', 'discriminating', 'hard']\n",
    "    sizes = [diff_counts.get(c, 0) for c in cats]\n",
    "    axes[1].pie(sizes, labels=cats, colors=[colors[c] for c in cats],\n",
    "               autopct='%1.0f%%', startangle=90, textprops={'fontsize': 12})\n",
    "    axes[1].set_title('Question Difficulty Categories')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficulty by dataset\n",
    "if 'dataset' in difficulty.columns and not difficulty.empty:\n",
    "    ds_diff = difficulty.groupby('dataset')['difficulty'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    # Reorder columns\n",
    "    for col in ['easy', 'discriminating', 'hard']:\n",
    "        if col not in ds_diff.columns:\n",
    "            ds_diff[col] = 0\n",
    "    ds_diff = ds_diff[['easy', 'discriminating', 'hard']]\n",
    "\n",
    "    print(\"Difficulty distribution by dataset:\")\n",
    "    display((ds_diff * 100).round(1))\n",
    "\n",
    "    ds_diff.plot(kind='bar', stacked=True,\n",
    "                color=['#66bb6a', '#ffa726', '#ef5350'],\n",
    "                figsize=(8, 5), edgecolor='black', linewidth=0.5)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.title('Question Difficulty by Dataset')\n",
    "    plt.legend(title='Difficulty')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Taxonomy\n",
    "\n",
    "Classify wrong answers into categories using heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_error(prediction: str, expected: list, f1_score: float) -> str:\n",
    "    \"\"\"Classify a wrong answer into an error category.\"\"\"\n",
    "    pred = str(prediction).strip().lower()\n",
    "\n",
    "    if f1_score >= 0.5:\n",
    "        return 'correct'\n",
    "\n",
    "    if not pred or pred in ('', 'n/a', 'none', 'unknown'):\n",
    "        return 'refusal/empty'\n",
    "\n",
    "    # Check for hedging / \"I don't know\" patterns\n",
    "    hedging_phrases = ['i don\\'t know', 'i\\'m not sure', 'cannot determine',\n",
    "                       'not enough information', 'i am not sure', 'unclear',\n",
    "                       'i cannot', 'unable to']\n",
    "    if any(phrase in pred for phrase in hedging_phrases):\n",
    "        return 'refusal/hedging'\n",
    "\n",
    "    # Partial match: some token overlap\n",
    "    if f1_score > 0.0:\n",
    "        return 'partial_match'\n",
    "\n",
    "    # Over-verbose: answer is very long (>200 chars)\n",
    "    if len(pred) > 200:\n",
    "        return 'over_verbose'\n",
    "\n",
    "    return 'wrong_answer'\n",
    "\n",
    "\n",
    "if 'prediction' in item_scores.columns and not item_scores.empty:\n",
    "    # Classify errors\n",
    "    item_scores['error_type'] = item_scores.apply(\n",
    "        lambda r: classify_error(r.get('prediction', ''), r.get('expected', []), r[PRIMARY_METRIC]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    error_counts = item_scores['error_type'].value_counts()\n",
    "    print(\"Error Taxonomy (all predictions):\")\n",
    "    print(\"=\" * 50)\n",
    "    for cat, count in error_counts.items():\n",
    "        pct = count / len(item_scores) * 100\n",
    "        print(f\"  {cat:<20s}: {count:>6d} ({pct:.1f}%)\")\n",
    "\n",
    "    # Error type distribution by exp_type\n",
    "    error_by_type = pd.crosstab(item_scores['exp_type'], item_scores['error_type'], normalize='index')\n",
    "    print(\"\\nError distribution by experiment type:\")\n",
    "    display((error_by_type * 100).round(1))\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    wrong_only = error_counts.drop('correct', errors='ignore')\n",
    "    wrong_only.plot(kind='bar', ax=ax, color='coral', alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Error Type Distribution (Wrong Answers Only)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error types by model\n",
    "if 'error_type' in item_scores.columns:\n",
    "    error_by_model = pd.crosstab(item_scores['model_short'], item_scores['error_type'],\n",
    "                                  normalize='index')\n",
    "    # Drop correct for better visualization\n",
    "    if 'correct' in error_by_model.columns:\n",
    "        error_by_model = error_by_model.drop(columns='correct')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    error_by_model.plot(kind='bar', stacked=True, ax=ax, edgecolor='black', linewidth=0.3)\n",
    "    ax.set_ylabel('Proportion of Wrong Answers')\n",
    "    ax.set_title('Error Type Breakdown by Model (Wrong Answers Only)')\n",
    "    ax.legend(title='Error Type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Answer Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'prediction' in item_scores.columns:\n",
    "    item_scores['answer_len'] = item_scores['prediction'].astype(str).str.len()\n",
    "    item_scores['answer_words'] = item_scores['prediction'].astype(str).str.split().str.len()\n",
    "\n",
    "    # Length vs F1 scatter\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Sample for scatter (too many points otherwise)\n",
    "    sample = item_scores.sample(min(5000, len(item_scores)), random_state=42)\n",
    "\n",
    "    axes[0].scatter(sample['answer_words'], sample[PRIMARY_METRIC],\n",
    "                    s=5, alpha=0.15, color='steelblue')\n",
    "    axes[0].set_xlabel('Answer Length (words)')\n",
    "    axes[0].set_ylabel('F1')\n",
    "    axes[0].set_title('Answer Length vs F1')\n",
    "    axes[0].set_xlim(0, min(200, sample['answer_words'].quantile(0.99)))\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Mean length by exp_type\n",
    "    len_by_type = item_scores.groupby('exp_type')['answer_words'].agg(['mean', 'median', 'std'])\n",
    "    print(\"Answer length by experiment type:\")\n",
    "    display(len_by_type.round(1))\n",
    "\n",
    "    # Violin: answer length by exp_type\n",
    "    types = sorted(item_scores['exp_type'].unique())\n",
    "    data_violin = [item_scores[item_scores['exp_type'] == t]['answer_words'].clip(upper=100).values\n",
    "                   for t in types]\n",
    "    parts = axes[1].violinplot(data_violin, showmeans=True, showmedians=True)\n",
    "    axes[1].set_xticks(range(1, len(types) + 1))\n",
    "    axes[1].set_xticklabels(types)\n",
    "    axes[1].set_ylabel('Answer Length (words, clipped at 100)')\n",
    "    axes[1].set_title('Answer Length Distribution by Experiment Type')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length by model\n",
    "if 'answer_words' in item_scores.columns:\n",
    "    len_by_model = item_scores.groupby('model_short')['answer_words'].agg(['mean', 'median'])\n",
    "    len_by_model = len_by_model.sort_values('mean', ascending=False)\n",
    "    print(\"Mean answer length by model (words):\")\n",
    "    display(len_by_model.round(1))\n",
    "\n",
    "    # Correlation: answer length vs F1 per model\n",
    "    print(\"\\nCorrelation between answer length and F1 (per model):\")\n",
    "    for model in sorted(item_scores['model_short'].unique()):\n",
    "        sub = item_scores[item_scores['model_short'] == model].dropna(subset=['answer_words', PRIMARY_METRIC])\n",
    "        if len(sub) >= 10:\n",
    "            from scipy.stats import spearmanr\n",
    "            rho, pval = spearmanr(sub['answer_words'], sub[PRIMARY_METRIC])\n",
    "            print(f\"  {model:<18s}: rho={rho:+.3f}, p={pval:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Agreement\n",
    "\n",
    "Do different models agree on which questions are hard? High agreement suggests question-intrinsic difficulty; low agreement suggests model-specific weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-model question difficulty (using best config per model)\n",
    "if not item_scores.empty:\n",
    "    # For each (model, idx, dataset), take the best F1 across all configs\n",
    "    best_per_model = item_scores.groupby(\n",
    "        ['model_short', 'idx', 'dataset']\n",
    "    )[PRIMARY_METRIC].max().reset_index()\n",
    "\n",
    "    # Pivot: rows=questions, columns=models, values=best F1\n",
    "    pivot = best_per_model.pivot_table(\n",
    "        index=['idx', 'dataset'], columns='model_short',\n",
    "        values=PRIMARY_METRIC, aggfunc='max'\n",
    "    )\n",
    "\n",
    "    # Model-model correlation of per-question scores\n",
    "    model_corr = pivot.corr(method='spearman')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    mask = np.triu(np.ones_like(model_corr, dtype=bool), k=1)\n",
    "    sns.heatmap(model_corr, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                vmin=0, vmax=1, mask=mask, ax=ax, square=True)\n",
    "    ax.set_title('Model Agreement on Question Difficulty\\n(Spearman correlation of per-question F1)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    mean_corr = model_corr.where(~np.eye(len(model_corr), dtype=bool)).mean().mean()\n",
    "    print(f\"\\nMean inter-model correlation: {mean_corr:.3f}\")\n",
    "    print(\"High = questions are intrinsically easy/hard\")\n",
    "    print(\"Low = difficulty is model-specific\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Wrong Answers\n",
    "\n",
    "Manually inspect a few wrong answers for qualitative insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample discriminating questions (where some configs get it right, others don't)\n",
    "if not difficulty.empty:\n",
    "    disc = difficulty[difficulty['difficulty'] == 'discriminating'].sample(\n",
    "        min(5, len(difficulty[difficulty['difficulty'] == 'discriminating'])),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Sample Discriminating Questions:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, q in disc.iterrows():\n",
    "        q_rows = item_scores[\n",
    "            (item_scores['idx'] == q['idx'])\n",
    "            & (item_scores.get('dataset', '') == q.get('dataset', ''))\n",
    "        ]\n",
    "        if q_rows.empty:\n",
    "            continue\n",
    "\n",
    "        q_text = q_rows['question'].iloc[0] if 'question' in q_rows.columns else 'N/A'\n",
    "        expected = q_rows['expected'].iloc[0] if 'expected' in q_rows.columns else 'N/A'\n",
    "\n",
    "        print(f\"\\nQ{q['idx']} [{q.get('dataset', 'N/A')}]: {str(q_text)[:150]}\")\n",
    "        print(f\"  Expected: {expected}\")\n",
    "        print(f\"  Correct in {q['pct_correct']*100:.0f}% of configs (n={q['n_configs']})\")\n",
    "\n",
    "        # Show a correct and incorrect answer\n",
    "        correct = q_rows[q_rows[PRIMARY_METRIC] >= 0.5].head(1)\n",
    "        incorrect = q_rows[q_rows[PRIMARY_METRIC] < 0.1].head(1)\n",
    "        if not correct.empty:\n",
    "            print(f\"  Correct answer:   {str(correct['prediction'].iloc[0])[:150]}\")\n",
    "        if not incorrect.empty:\n",
    "            print(f\"  Incorrect answer: {str(incorrect['prediction'].iloc[0])[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key findings:\n",
    "- Question difficulty distribution and dataset differences\n",
    "- Dominant error types and model-specific patterns\n",
    "- Whether answer verbosity correlates with quality\n",
    "- Degree of model agreement on question difficulty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
