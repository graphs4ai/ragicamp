{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB08: TPE Optimization Analysis\n",
    "\n",
    "**Question:** How did the TPE optimizer explore the search space? Did it converge?\n",
    "\n",
    "This notebook analyzes Optuna's TPE sampling behavior:\n",
    "1. **Convergence curves** — F1 vs trial number\n",
    "2. **Parameter importance** — TPE's learned importance vs variance decomposition\n",
    "3. **Exploration heatmaps** — which regions got most trials\n",
    "4. **Best trial per model** — did TPE converge on the same recipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as scipy_stats\n\nfrom analysis_utils import (\n    load_all_results, setup_plotting, identify_bottlenecks,\n    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER, MODEL_PARAMS,\n)\n\nsetup_plotting()\nSTUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n\n# Check for Optuna study database\noptuna_db = STUDY_PATH / \"optuna_study.db\"\nhas_optuna = optuna_db.exists()\nprint(f\"Optuna DB found: {has_optuna}\")\nif has_optuna:\n    print(f\"  Path: {optuna_db}\")\n    print(f\"  Size: {optuna_db.stat().st_size / 1024 / 1024:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = None\n",
    "trials_df = pd.DataFrame()\n",
    "\n",
    "if has_optuna:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    storage = f\"sqlite:///{optuna_db}\"\n",
    "    study_summaries = optuna.study.get_all_study_summaries(storage)\n",
    "    print(f\"Studies in DB: {len(study_summaries)}\")\n",
    "    for s in study_summaries:\n",
    "        print(f\"  {s.study_name}: {s.n_trials} trials, \"\n",
    "              f\"best={s.best_trial.value if s.best_trial else 'N/A'}\")\n",
    "\n",
    "    # Load the main study (first one, or the one with most trials)\n",
    "    best_summary = max(study_summaries, key=lambda s: s.n_trials)\n",
    "    study = optuna.load_study(study_name=best_summary.study_name, storage=storage)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    print(f\"\\nLoaded study: {study.study_name}\")\n",
    "    print(f\"  Trials: {len(trials_df)}\")\n",
    "    print(f\"  Params: {list(trials_df.filter(like='params_').columns)}\")\n",
    "else:\n",
    "    print(\"No Optuna DB — using experiment results as proxy for optimization analysis.\")\n",
    "    df_all = load_all_results(STUDY_PATH)\n",
    "    df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convergence Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not trials_df.empty:\n    completed = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n    completed = completed.sort_values('number')\n\n    # Check if trials span multiple datasets — if so, normalize F1 within each dataset\n    dataset_col = next((c for c in completed.columns\n                        if 'dataset' in c.lower() and 'params' in c.lower()), None)\n\n    if dataset_col and completed[dataset_col].nunique() > 1:\n        # Normalize F1 within each dataset (z-score) to make cross-dataset trials comparable\n        print(f\"Note: Trials span {completed[dataset_col].nunique()} datasets — \"\n              f\"normalizing F1 within each dataset (z-score).\")\n        completed['value_raw'] = completed['value']\n        completed['value'] = completed.groupby(dataset_col)['value'].transform(\n            lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n        )\n        ylabel = 'F1 (z-score, within-dataset)'\n    else:\n        ylabel = 'F1'\n\n    completed['running_best'] = completed['value'].cummax()\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # All trials scatter\n    axes[0].scatter(completed['number'], completed['value'],\n                    s=15, alpha=0.4, color='steelblue', label='Trial')\n    axes[0].plot(completed['number'], completed['running_best'],\n                 color='red', linewidth=2, label='Running best')\n    axes[0].set_xlabel('Trial Number')\n    axes[0].set_ylabel(ylabel)\n    axes[0].set_title('TPE Convergence Curve')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n\n    # Rolling mean (window=20)\n    if len(completed) >= 20:\n        rolling = completed['value'].rolling(window=20, min_periods=5).mean()\n        axes[1].plot(completed['number'], rolling, color='steelblue',\n                     linewidth=2, label='Rolling mean (w=20)')\n        axes[1].plot(completed['number'], completed['running_best'],\n                     color='red', linewidth=1.5, linestyle='--', label='Running best')\n        axes[1].set_xlabel('Trial Number')\n        axes[1].set_ylabel(ylabel)\n        axes[1].set_title('Exploration Trend (Rolling Mean)')\n        axes[1].legend()\n        axes[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Convergence check: improvement in last 25%\n    n = len(completed)\n    q3_start = n * 3 // 4\n    best_first_75 = completed.iloc[:q3_start]['value'].max()\n    best_last_25 = completed.iloc[q3_start:]['value'].max()\n    print(f\"\\nConvergence check:\")\n    print(f\"  Best in first 75% of trials: {best_first_75:.4f}\")\n    print(f\"  Best in last 25% of trials:  {best_last_25:.4f}\")\n    print(f\"  Improvement: {best_last_25 - best_first_75:+.4f}\")\n    if best_last_25 - best_first_75 < 0.005:\n        print(\"  -> Study appears converged\")\n    else:\n        print(\"  -> Study was still improving\")\nelse:\n    print(\"No Optuna trials data. Skipping convergence analysis.\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 1b. Per-Model Convergence\n\nDid TPE converge on every model subspace, or are some models under-explored?\nThis matters for thesis claims: if TPE only ran 20 trials on a model,\nthe \"best config\" is less reliable than one found after 200 trials.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Per-model convergence: did TPE converge on each model subspace?\nconvergence_rows = []\n\nif not trials_df.empty:\n    completed = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n    model_col = next((c for c in completed.columns\n                      if 'model' in c.lower() and 'params' in c.lower()), None)\n\n    # Check for dataset column — normalize if multi-dataset\n    dataset_col = next((c for c in completed.columns\n                        if 'dataset' in c.lower() and 'params' in c.lower()), None)\n\n    if dataset_col and completed[dataset_col].nunique() > 1:\n        completed['value_raw'] = completed['value']\n        completed['value'] = completed.groupby(dataset_col)['value'].transform(\n            lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n        )\n        ylabel = 'F1 (z-score)'\n    else:\n        ylabel = 'F1'\n\n    if model_col:\n        models = sorted(completed[model_col].unique())\n        n_models = len(models)\n        ncols = min(3, n_models)\n        nrows = (n_models + ncols - 1) // ncols\n\n        fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n        axes_flat = np.atleast_1d(axes).flatten()\n\n        for idx, model in enumerate(models):\n            m_trials = completed[completed[model_col] == model].sort_values('number').copy()\n            m_trials['local_trial'] = range(len(m_trials))\n            m_trials['running_best'] = m_trials['value'].cummax()\n\n            ax = axes_flat[idx]\n            ax.scatter(m_trials['local_trial'], m_trials['value'],\n                       s=12, alpha=0.4, color='steelblue')\n            ax.plot(m_trials['local_trial'], m_trials['running_best'],\n                    color='red', linewidth=1.5)\n\n            if len(m_trials) >= 10:\n                w = min(20, len(m_trials) // 3)\n                rolling = m_trials['value'].rolling(window=w, min_periods=3).mean()\n                ax.plot(m_trials['local_trial'], rolling,\n                        color='orange', linewidth=1, alpha=0.7)\n\n            ax.set_title(model, fontsize=10)\n            ax.set_xlabel('Trial #')\n            ax.set_ylabel(ylabel)\n            ax.grid(alpha=0.2)\n\n            n = len(m_trials)\n            if n >= 8:\n                q3 = n * 3 // 4\n                best_75 = m_trials.iloc[:q3]['value'].max()\n                best_25 = m_trials.iloc[q3:]['value'].max()\n                improvement = best_25 - best_75\n                converged = improvement < 0.005\n            else:\n                best_75 = np.nan\n                best_25 = np.nan\n                improvement = np.nan\n                converged = None\n\n            convergence_rows.append({\n                'model': model,\n                'n_trials': n,\n                'best_f1': m_trials['value'].max(),\n                'mean_f1': m_trials['value'].mean(),\n                'std_f1': m_trials['value'].std(),\n                'best_first_75pct': best_75,\n                'best_last_25pct': best_25,\n                'improvement': improvement,\n                'converged': converged,\n            })\n\n        for idx in range(n_models, len(axes_flat)):\n            axes_flat[idx].set_visible(False)\n\n        plt.suptitle('Per-Model Convergence Curves (Dataset-Normalized)', y=1.01, fontsize=13)\n        plt.tight_layout()\n        plt.show()\n\nelse:\n    # Fallback: use experiment results — stratify by dataset\n    if 'df' in dir() and not df.empty:\n        rag = df[df['exp_type'] == 'rag'].copy()\n        for model in sorted(rag['model_short'].unique()):\n            m_df = rag[rag['model_short'] == model].sort_values('name')\n            n = len(m_df)\n            # Stratified: per-(model, dataset) stats, then average\n            per_ds = m_df.groupby('dataset')[PRIMARY_METRIC].agg(['max', 'mean', 'std', 'count'])\n            if per_ds['count'].sum() < 3:\n                continue\n            convergence_rows.append({\n                'model': model,\n                'n_trials': n,\n                'best_f1': per_ds['max'].mean(),  # avg of per-dataset best\n                'mean_f1': per_ds['mean'].mean(),  # avg of per-dataset mean\n                'std_f1': per_ds['std'].mean() if per_ds['std'].notna().any() else np.nan,\n                'best_first_75pct': np.nan,\n                'best_last_25pct': np.nan,\n                'improvement': np.nan,\n                'converged': None,\n            })\n\n# Convergence summary table\nif convergence_rows:\n    conv_df = pd.DataFrame(convergence_rows)\n    conv_df['improvement'] = conv_df['best_last_25pct'] - conv_df['best_first_75pct']\n    conv_df['converged'] = conv_df['improvement'].apply(\n        lambda x: 'Yes' if pd.notna(x) and x < 0.005\n        else ('No' if pd.notna(x) else 'Too few trials')\n    )\n\n    print(\"\\nPer-Model Convergence Assessment:\")\n    print(\"=\" * 80)\n    display(conv_df[['model', 'n_trials', 'best_f1', 'mean_f1', 'std_f1',\n                     'improvement', 'converged']].round(4))\n\n    median_trials = conv_df['n_trials'].median()\n    under_explored = conv_df[conv_df['n_trials'] < median_trials * 0.5]\n    if not under_explored.empty:\n        print(f\"\\nUnder-explored models (< {median_trials * 0.5:.0f} trials, \"\n              f\"half the median of {median_trials:.0f}):\")\n        for _, row in under_explored.iterrows():\n            print(f\"  {row['model']}: only {row['n_trials']} trials \"\n                  f\"(best F1={row['best_f1']:.4f})\")\n\n    n_converged = (conv_df['converged'] == 'Yes').sum()\n    n_not = (conv_df['converged'] == 'No').sum()\n    n_few = (conv_df['converged'] == 'Too few trials').sum()\n    print(f\"\\nOverall: {n_converged} converged, {n_not} still improving, \"\n          f\"{n_few} insufficient trials\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Importance\n",
    "\n",
    "Compare TPE's learned parameter importance with the variance decomposition from NB03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_importance = {}\n",
    "if study is not None:\n",
    "    try:\n",
    "        from optuna.importance import get_param_importances\n",
    "        tpe_importance = get_param_importances(study)\n",
    "        print(\"TPE Parameter Importance:\")\n",
    "        for param, imp in sorted(tpe_importance.items(), key=lambda x: -x[1]):\n",
    "            bar = '#' * int(imp * 50)\n",
    "            print(f\"  {param:<25s}: {imp:.3f} {bar}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute importance: {e}\")\n",
    "\n",
    "# Compare with variance decomposition\n",
    "if not trials_df.empty or 'df' in dir():\n",
    "    source_df = df if 'df' in dir() else load_all_results(STUDY_PATH)\n",
    "    source_df = source_df[~source_df['model_short'].isin(BROKEN_MODELS)]\n",
    "    variance_decomp = identify_bottlenecks(source_df, PRIMARY_METRIC)\n",
    "\n",
    "    if tpe_importance and variance_decomp:\n",
    "        # Normalize variance decomp to sum to 1 for comparison\n",
    "        total_var = sum(variance_decomp.values())\n",
    "        var_norm = {k: v / total_var for k, v in variance_decomp.items()} if total_var > 0 else {}\n",
    "\n",
    "        # Map TPE param names to variance decomp names\n",
    "        param_map = {\n",
    "            'params_retriever': 'retriever_type',\n",
    "            'params_top_k': 'top_k',\n",
    "            'params_prompt': 'prompt',\n",
    "            'params_query_transform': 'query_transform',\n",
    "            'params_reranker': 'reranker',\n",
    "            'params_agent_type': 'agent_type',\n",
    "            'params_model': 'model_short',\n",
    "        }\n",
    "\n",
    "        all_factors = set()\n",
    "        all_factors.update(var_norm.keys())\n",
    "        all_factors.update(param_map.get(k, k) for k in tpe_importance.keys())\n",
    "        all_factors = sorted(all_factors)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        x = np.arange(len(all_factors))\n",
    "        w = 0.35\n",
    "        var_vals = [var_norm.get(f, 0) for f in all_factors]\n",
    "        tpe_vals = [tpe_importance.get(f'params_{f}', tpe_importance.get(f, 0))\n",
    "                    for f in all_factors]\n",
    "\n",
    "        ax.bar(x - w/2, var_vals, w, label='Variance Decomposition', color='steelblue', alpha=0.8)\n",
    "        ax.bar(x + w/2, tpe_vals, w, label='TPE Importance', color='coral', alpha=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(all_factors, rotation=30, ha='right')\n",
    "        ax.set_ylabel('Relative Importance')\n",
    "        ax.set_title('Variance Decomposition vs TPE Parameter Importance')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif variance_decomp:\n",
    "        print(\"\\nVariance decomposition (no TPE data to compare):\")\n",
    "        for f, v in variance_decomp.items():\n",
    "            print(f\"  {f:<20s}: {v:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration Heatmaps\n",
    "\n",
    "Which parameter combinations did TPE explore most heavily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trials_df.empty:\n",
    "    param_cols = [c for c in trials_df.columns if c.startswith('params_')]\n",
    "\n",
    "    # Count how many trials explored each value of each parameter\n",
    "    fig, axes = plt.subplots(2, (len(param_cols) + 1) // 2,\n",
    "                             figsize=(6 * ((len(param_cols) + 1) // 2), 10))\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    for idx, col in enumerate(param_cols):\n",
    "        if idx >= len(axes_flat):\n",
    "            break\n",
    "        ax = axes_flat[idx]\n",
    "        completed = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "        counts = completed[col].value_counts()\n",
    "        counts.plot(kind='bar', ax=ax, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "        ax.set_title(col.replace('params_', ''), fontsize=11)\n",
    "        ax.set_ylabel('Trials')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for idx in range(len(param_cols), len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle('TPE Exploration: Trials per Parameter Value', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trial data for exploration analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2D exploration heatmap: top 2 most important parameters\nif not trials_df.empty and len(param_cols) >= 2:\n    # Pick top-2 by TPE importance, or just first 2 categorical params\n    if tpe_importance:\n        top_params = sorted(tpe_importance.keys(), key=lambda k: -tpe_importance[k])[:2]\n    else:\n        top_params = param_cols[:2]\n\n    # Ensure they exist\n    top_params = [p if p in trials_df.columns else p.replace('params_', '')\n                  for p in top_params]\n    top_params = [p for p in top_params if p in trials_df.columns][:2]\n\n    if len(top_params) == 2:\n        completed = trials_df[trials_df['state'] == 'COMPLETE']\n\n        # Check for dataset column — stratify mean F1 heatmap\n        dataset_col = next((c for c in completed.columns\n                            if 'dataset' in c.lower() and 'params' in c.lower()), None)\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n        # Count heatmap (not affected by stratification)\n        count_pivot = pd.crosstab(completed[top_params[0]], completed[top_params[1]])\n        sns.heatmap(count_pivot, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n        axes[0].set_title(f'Exploration Density: {top_params[0]} x {top_params[1]}')\n\n        # Mean F1 heatmap — stratified by dataset if available\n        if dataset_col and completed[dataset_col].nunique() > 1:\n            # Per-(p1, p2, dataset) mean, then average across datasets\n            strat_mean = (\n                completed.groupby([top_params[0], top_params[1], dataset_col])['value']\n                .mean()\n                .groupby([top_params[0], top_params[1]]).mean()\n                .unstack(top_params[1])\n            )\n            sns.heatmap(strat_mean, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[1])\n            axes[1].set_title(f'Mean F1 (dataset-stratified): {top_params[0]} x {top_params[1]}')\n        else:\n            mean_pivot = completed.pivot_table(\n                index=top_params[0], columns=top_params[1],\n                values='value', aggfunc='mean'\n            )\n            sns.heatmap(mean_pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[1])\n            axes[1].set_title(f'Mean F1: {top_params[0]} x {top_params[1]}')\n\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Trial per Model\n",
    "\n",
    "Did TPE find the same \"optimal recipe\" across different models, or does each model need different settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not trials_df.empty:\n    completed = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n\n    model_col = next((c for c in completed.columns if 'model' in c.lower() and 'params' in c.lower()), None)\n    dataset_col = next((c for c in completed.columns\n                        if 'dataset' in c.lower() and 'params' in c.lower()), None)\n\n    if model_col:\n        # Best trial per (model, dataset) then show table\n        if dataset_col and completed[dataset_col].nunique() > 1:\n            print(\"Best Configuration per Model x Dataset (from TPE):\")\n            print(\"=\" * 70)\n            best_per_md = completed.loc[\n                completed.groupby([model_col, dataset_col])['value'].idxmax()\n            ]\n            for _, row in best_per_md.sort_values([model_col, dataset_col]).iterrows():\n                print(f\"\\n  {row[model_col]} / {row[dataset_col]} \"\n                      f\"(F1={row['value']:.4f}, trial #{row['number']}):\")\n                for col in param_cols:\n                    if col not in (model_col, dataset_col):\n                        print(f\"    {col.replace('params_', ''):<20s}: {row[col]}\")\n\n            # Recipe agreement: check across models AND datasets\n            non_key_params = [c for c in param_cols if c not in (model_col, dataset_col)]\n            print(\"\\n\\nRecipe Agreement Across Models x Datasets:\")\n            for col in non_key_params:\n                values = best_per_md[col].dropna().unique()\n                if len(values) == 1:\n                    print(f\"  {col.replace('params_', ''):<20s}: UNANIMOUS — {values[0]}\")\n                else:\n                    mode = best_per_md[col].mode().iloc[0]\n                    agreement = (best_per_md[col] == mode).mean() * 100\n                    print(f\"  {col.replace('params_', ''):<20s}: VARIES — mode={mode} ({agreement:.0f}% agree)\")\n        else:\n            # Single dataset or no dataset column — original behavior\n            best_per_model = completed.loc[completed.groupby(model_col)['value'].idxmax()]\n            print(\"Best Configuration per Model (from TPE):\")\n            print(\"=\" * 60)\n            for _, row in best_per_model.iterrows():\n                print(f\"\\n  {row[model_col]} (F1={row['value']:.4f}, trial #{row['number']}):\")\n                for col in param_cols:\n                    if col != model_col:\n                        print(f\"    {col.replace('params_', ''):<20s}: {row[col]}\")\n\n            non_model_params = [c for c in param_cols if c != model_col]\n            print(\"\\n\\nRecipe Agreement Across Models:\")\n            for col in non_model_params:\n                values = best_per_model[col].dropna().unique()\n                if len(values) == 1:\n                    print(f\"  {col.replace('params_', ''):<20s}: UNANIMOUS — {values[0]}\")\n                else:\n                    mode = best_per_model[col].mode().iloc[0]\n                    agreement = (best_per_model[col] == mode).mean() * 100\n                    print(f\"  {col.replace('params_', ''):<20s}: VARIES — mode={mode} ({agreement:.0f}% agree)\")\n    else:\n        print(\"No model parameter found in trial data.\")\nelse:\n    # Fallback: use experiment results — stratified by dataset\n    if 'df' in dir() and not df.empty:\n        rag = df[df['exp_type'] == 'rag']\n        config_cols = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n                       'query_transform', 'top_k', 'agent_type']\n        config_cols = [c for c in config_cols if c in rag.columns]\n\n        print(\"Best Configuration per Model x Dataset (from experiment results):\")\n        print(\"=\" * 70)\n        for model in sorted(rag['model_short'].unique()):\n            for ds in sorted(rag['dataset'].unique()):\n                m_df = rag[(rag['model_short'] == model) & (rag['dataset'] == ds)]\n                if m_df[PRIMARY_METRIC].notna().sum() == 0:\n                    continue\n                best_idx = m_df[PRIMARY_METRIC].idxmax()\n                best = m_df.loc[best_idx]\n                print(f\"\\n  {model} / {ds} (F1={best[PRIMARY_METRIC]:.4f}):\")\n                for c in config_cols:\n                    print(f\"    {c:<20s}: {best.get(c, 'N/A')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Key findings:\n",
    "- Whether TPE converged or was still exploring\n",
    "- Agreement between TPE importance and variance decomposition\n",
    "- Which search space regions were over/under-explored\n",
    "- Whether an optimal \"universal recipe\" exists or configs are model-specific"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}