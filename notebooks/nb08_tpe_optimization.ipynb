{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB08: TPE Optimization Analysis\n",
    "\n",
    "**Question:** How did the TPE optimizer explore the search space? Did it converge?\n",
    "\n",
    "This notebook analyzes Optuna's TPE sampling behavior:\n",
    "1. **Convergence curves** — F1 vs trial number\n",
    "2. **Parameter importance** — TPE's learned importance vs variance decomposition\n",
    "3. **Exploration heatmaps** — which regions got most trials\n",
    "4. **Best trial per model** — did TPE converge on the same recipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, identify_bottlenecks,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "# Check for Optuna study database\n",
    "optuna_db = STUDY_PATH / \"optuna_study.db\"\n",
    "has_optuna = optuna_db.exists()\n",
    "print(f\"Optuna DB found: {has_optuna}\")\n",
    "if has_optuna:\n",
    "    print(f\"  Path: {optuna_db}\")\n",
    "    print(f\"  Size: {optuna_db.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = None\n",
    "trials_df = pd.DataFrame()\n",
    "\n",
    "if has_optuna:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    storage = f\"sqlite:///{optuna_db}\"\n",
    "    study_summaries = optuna.study.get_all_study_summaries(storage)\n",
    "    print(f\"Studies in DB: {len(study_summaries)}\")\n",
    "    for s in study_summaries:\n",
    "        print(f\"  {s.study_name}: {s.n_trials} trials, \"\n",
    "              f\"best={s.best_trial.value if s.best_trial else 'N/A'}\")\n",
    "\n",
    "    # Load the main study (first one, or the one with most trials)\n",
    "    best_summary = max(study_summaries, key=lambda s: s.n_trials)\n",
    "    study = optuna.load_study(study_name=best_summary.study_name, storage=storage)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    print(f\"\\nLoaded study: {study.study_name}\")\n",
    "    print(f\"  Trials: {len(trials_df)}\")\n",
    "    print(f\"  Params: {list(trials_df.filter(like='params_').columns)}\")\n",
    "else:\n",
    "    print(\"No Optuna DB — using experiment results as proxy for optimization analysis.\")\n",
    "    df_all = load_all_results(STUDY_PATH)\n",
    "    df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convergence Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trials_df.empty:\n",
    "    # Plot: trial number vs value (F1), with running best\n",
    "    completed = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n",
    "    completed = completed.sort_values('number')\n",
    "    completed['running_best'] = completed['value'].cummax()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # All trials scatter\n",
    "    axes[0].scatter(completed['number'], completed['value'],\n",
    "                    s=15, alpha=0.4, color='steelblue', label='Trial')\n",
    "    axes[0].plot(completed['number'], completed['running_best'],\n",
    "                 color='red', linewidth=2, label='Running best')\n",
    "    axes[0].set_xlabel('Trial Number')\n",
    "    axes[0].set_ylabel('F1')\n",
    "    axes[0].set_title('TPE Convergence Curve')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Rolling mean (window=20) to see exploration trend\n",
    "    if len(completed) >= 20:\n",
    "        rolling = completed['value'].rolling(window=20, min_periods=5).mean()\n",
    "        axes[1].plot(completed['number'], rolling, color='steelblue',\n",
    "                     linewidth=2, label='Rolling mean (w=20)')\n",
    "        axes[1].plot(completed['number'], completed['running_best'],\n",
    "                     color='red', linewidth=1.5, linestyle='--', label='Running best')\n",
    "        axes[1].set_xlabel('Trial Number')\n",
    "        axes[1].set_ylabel('F1')\n",
    "        axes[1].set_title('Exploration Trend (Rolling Mean)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Check convergence: improvement in last 25% of trials\n",
    "    n = len(completed)\n",
    "    q3_start = n * 3 // 4\n",
    "    best_first_75 = completed.iloc[:q3_start]['value'].max()\n",
    "    best_last_25 = completed.iloc[q3_start:]['value'].max()\n",
    "    print(f\"\\nConvergence check:\")\n",
    "    print(f\"  Best in first 75% of trials: {best_first_75:.4f}\")\n",
    "    print(f\"  Best in last 25% of trials:  {best_last_25:.4f}\")\n",
    "    print(f\"  Improvement: {best_last_25 - best_first_75:+.4f}\")\n",
    "    if best_last_25 - best_first_75 < 0.005:\n",
    "        print(\"  -> Study appears converged\")\n",
    "    else:\n",
    "        print(\"  -> Study was still improving\")\n",
    "else:\n",
    "    print(\"No Optuna trials data. Skipping convergence analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Importance\n",
    "\n",
    "Compare TPE's learned parameter importance with the variance decomposition from NB03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_importance = {}\n",
    "if study is not None:\n",
    "    try:\n",
    "        from optuna.importance import get_param_importances\n",
    "        tpe_importance = get_param_importances(study)\n",
    "        print(\"TPE Parameter Importance:\")\n",
    "        for param, imp in sorted(tpe_importance.items(), key=lambda x: -x[1]):\n",
    "            bar = '#' * int(imp * 50)\n",
    "            print(f\"  {param:<25s}: {imp:.3f} {bar}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute importance: {e}\")\n",
    "\n",
    "# Compare with variance decomposition\n",
    "if not trials_df.empty or 'df' in dir():\n",
    "    source_df = df if 'df' in dir() else load_all_results(STUDY_PATH)\n",
    "    source_df = source_df[~source_df['model_short'].isin(BROKEN_MODELS)]\n",
    "    variance_decomp = identify_bottlenecks(source_df, PRIMARY_METRIC)\n",
    "\n",
    "    if tpe_importance and variance_decomp:\n",
    "        # Normalize variance decomp to sum to 1 for comparison\n",
    "        total_var = sum(variance_decomp.values())\n",
    "        var_norm = {k: v / total_var for k, v in variance_decomp.items()} if total_var > 0 else {}\n",
    "\n",
    "        # Map TPE param names to variance decomp names\n",
    "        param_map = {\n",
    "            'params_retriever': 'retriever_type',\n",
    "            'params_top_k': 'top_k',\n",
    "            'params_prompt': 'prompt',\n",
    "            'params_query_transform': 'query_transform',\n",
    "            'params_reranker': 'reranker',\n",
    "            'params_agent_type': 'agent_type',\n",
    "            'params_model': 'model_short',\n",
    "        }\n",
    "\n",
    "        all_factors = set()\n",
    "        all_factors.update(var_norm.keys())\n",
    "        all_factors.update(param_map.get(k, k) for k in tpe_importance.keys())\n",
    "        all_factors = sorted(all_factors)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        x = np.arange(len(all_factors))\n",
    "        w = 0.35\n",
    "        var_vals = [var_norm.get(f, 0) for f in all_factors]\n",
    "        tpe_vals = [tpe_importance.get(f'params_{f}', tpe_importance.get(f, 0))\n",
    "                    for f in all_factors]\n",
    "\n",
    "        ax.bar(x - w/2, var_vals, w, label='Variance Decomposition', color='steelblue', alpha=0.8)\n",
    "        ax.bar(x + w/2, tpe_vals, w, label='TPE Importance', color='coral', alpha=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(all_factors, rotation=30, ha='right')\n",
    "        ax.set_ylabel('Relative Importance')\n",
    "        ax.set_title('Variance Decomposition vs TPE Parameter Importance')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif variance_decomp:\n",
    "        print(\"\\nVariance decomposition (no TPE data to compare):\")\n",
    "        for f, v in variance_decomp.items():\n",
    "            print(f\"  {f:<20s}: {v:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration Heatmaps\n",
    "\n",
    "Which parameter combinations did TPE explore most heavily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trials_df.empty:\n",
    "    param_cols = [c for c in trials_df.columns if c.startswith('params_')]\n",
    "\n",
    "    # Count how many trials explored each value of each parameter\n",
    "    fig, axes = plt.subplots(2, (len(param_cols) + 1) // 2,\n",
    "                             figsize=(6 * ((len(param_cols) + 1) // 2), 10))\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    for idx, col in enumerate(param_cols):\n",
    "        if idx >= len(axes_flat):\n",
    "            break\n",
    "        ax = axes_flat[idx]\n",
    "        completed = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "        counts = completed[col].value_counts()\n",
    "        counts.plot(kind='bar', ax=ax, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "        ax.set_title(col.replace('params_', ''), fontsize=11)\n",
    "        ax.set_ylabel('Trials')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for idx in range(len(param_cols), len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle('TPE Exploration: Trials per Parameter Value', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trial data for exploration analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D exploration heatmap: top 2 most important parameters\n",
    "if not trials_df.empty and len(param_cols) >= 2:\n",
    "    # Pick top-2 by TPE importance, or just first 2 categorical params\n",
    "    if tpe_importance:\n",
    "        top_params = sorted(tpe_importance.keys(), key=lambda k: -tpe_importance[k])[:2]\n",
    "    else:\n",
    "        top_params = param_cols[:2]\n",
    "\n",
    "    # Ensure they exist\n",
    "    top_params = [p if p in trials_df.columns else p.replace('params_', '')\n",
    "                  for p in top_params]\n",
    "    top_params = [p for p in top_params if p in trials_df.columns][:2]\n",
    "\n",
    "    if len(top_params) == 2:\n",
    "        completed = trials_df[trials_df['state'] == 'COMPLETE']\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Count heatmap\n",
    "        count_pivot = pd.crosstab(completed[top_params[0]], completed[top_params[1]])\n",
    "        sns.heatmap(count_pivot, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "        axes[0].set_title(f'Exploration Density: {top_params[0]} x {top_params[1]}')\n",
    "\n",
    "        # Mean F1 heatmap\n",
    "        mean_pivot = completed.pivot_table(\n",
    "            index=top_params[0], columns=top_params[1],\n",
    "            values='value', aggfunc='mean'\n",
    "        )\n",
    "        sns.heatmap(mean_pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[1])\n",
    "        axes[1].set_title(f'Mean F1: {top_params[0]} x {top_params[1]}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Trial per Model\n",
    "\n",
    "Did TPE find the same \"optimal recipe\" across different models, or does each model need different settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trials_df.empty:\n",
    "    completed = trials_df[trials_df['state'] == 'COMPLETE'].copy()\n",
    "\n",
    "    # Identify model parameter column\n",
    "    model_col = next((c for c in completed.columns if 'model' in c.lower() and 'params' in c.lower()), None)\n",
    "\n",
    "    if model_col:\n",
    "        # Best trial per model\n",
    "        best_per_model = completed.loc[completed.groupby(model_col)['value'].idxmax()]\n",
    "\n",
    "        print(\"Best Configuration per Model (from TPE):\")\n",
    "        print(\"=\" * 60)\n",
    "        for _, row in best_per_model.iterrows():\n",
    "            print(f\"\\n  {row[model_col]} (F1={row['value']:.4f}, trial #{row['number']}):\")\n",
    "            for col in param_cols:\n",
    "                if col != model_col:\n",
    "                    print(f\"    {col.replace('params_', ''):<20s}: {row[col]}\")\n",
    "\n",
    "        # Check recipe similarity: how many params agree across models?\n",
    "        non_model_params = [c for c in param_cols if c != model_col]\n",
    "        print(\"\\n\\nRecipe Agreement Across Models:\")\n",
    "        for col in non_model_params:\n",
    "            values = best_per_model[col].dropna().unique()\n",
    "            if len(values) == 1:\n",
    "                print(f\"  {col.replace('params_', ''):<20s}: UNANIMOUS — {values[0]}\")\n",
    "            else:\n",
    "                mode = best_per_model[col].mode().iloc[0]\n",
    "                agreement = (best_per_model[col] == mode).mean() * 100\n",
    "                print(f\"  {col.replace('params_', ''):<20s}: VARIES — mode={mode} ({agreement:.0f}% agree)\")\n",
    "    else:\n",
    "        print(\"No model parameter found in trial data.\")\n",
    "else:\n",
    "    # Fallback: use experiment results\n",
    "    if 'df' in dir() and not df.empty:\n",
    "        rag = df[df['exp_type'] == 'rag']\n",
    "        config_cols = ['retriever_type', 'embedding_model', 'reranker', 'prompt',\n",
    "                       'query_transform', 'top_k', 'agent_type']\n",
    "        config_cols = [c for c in config_cols if c in rag.columns]\n",
    "\n",
    "        print(\"Best Configuration per Model (from experiment results):\")\n",
    "        print(\"=\" * 60)\n",
    "        for model in sorted(rag['model_short'].unique()):\n",
    "            m_df = rag[rag['model_short'] == model]\n",
    "            if m_df[PRIMARY_METRIC].notna().sum() == 0:\n",
    "                continue\n",
    "            best_idx = m_df[PRIMARY_METRIC].idxmax()\n",
    "            best = m_df.loc[best_idx]\n",
    "            print(f\"\\n  {model} (F1={best[PRIMARY_METRIC]:.4f}):\")\n",
    "            for c in config_cols:\n",
    "                print(f\"    {c:<20s}: {best.get(c, 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Key findings:\n",
    "- Whether TPE converged or was still exploring\n",
    "- Agreement between TPE importance and variance decomposition\n",
    "- Which search space regions were over/under-explored\n",
    "- Whether an optimal \"universal recipe\" exists or configs are model-specific"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
