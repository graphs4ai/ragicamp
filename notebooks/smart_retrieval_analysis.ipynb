{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Retrieval SLM Analysis\n",
    "\n",
    "## Hypothesis\n",
    "**\"High-quality retrieval can compensate for smaller, faster LLMs.\"**\n",
    "\n",
    "This notebook analyzes experiment results with proper handling of:\n",
    "- **Stratified sampling** (non-uniform experiment counts)\n",
    "- **Component-wise effect estimation** with confidence intervals\n",
    "- **Bottleneck identification** for maximizing QA performance\n",
    "\n",
    "### Analysis Dimensions\n",
    "| Dimension | Values | Description |\n",
    "|-----------|--------|-------------|\n",
    "| Model | Llama-3.2-3B, Phi-3-mini, Qwen-2.5-3B | Generator LLM |\n",
    "| Retriever Type | dense, hybrid, hierarchical | Retrieval strategy |\n",
    "| Embedding Model | bge-large, bge-m3, gte-qwen2, e5-mistral | Embedding model |\n",
    "| Query Transform | none, hyde, multiquery | Query preprocessing |\n",
    "| Reranker | none, bge, bge-v2 | Cross-encoder reranking |\n",
    "| Prompt | concise, structured, cot, fewshot_3 | Prompt template |\n",
    "| Top-K | 3, 5, 10 | Retrieved documents |\n",
    "| Dataset | nq, triviaqa, hotpotqa | Evaluation benchmark |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Paths\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "# Metrics to analyze\n",
    "METRICS = ['f1', 'exact_match', 'bertscore', 'bleurt', 'llm_judge']\n",
    "PRIMARY_METRIC = 'f1'\n",
    "\n",
    "print(f\"Study path: {STUDY_PATH}\")\n",
    "print(f\"Exists: {STUDY_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Parsing\n",
    "\n",
    "Parse experiment names following the naming convention:\n",
    "- Direct: `direct_{model}_{prompt}_{dataset}`\n",
    "- RAG: `rag_{model}_{retriever}_k{top_k}_{query_transform?}_{reranker?}_{prompt}_{dataset}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name mappings\n",
    "MODEL_MAP = {\n",
    "    'llama': 'Llama-3.2-3B',\n",
    "    'Llama3.23BInstruct': 'Llama-3.2-3B',\n",
    "    'phi': 'Phi-3-mini',\n",
    "    'Phi3mini4kinstruct': 'Phi-3-mini',\n",
    "    'qwen': 'Qwen-2.5-3B',\n",
    "    'Qwen2.53BInstruct': 'Qwen-2.5-3B',\n",
    "}\n",
    "\n",
    "# Retriever type detection\n",
    "RETRIEVER_TYPES = {\n",
    "    'dense': ['dense_bge', 'dense_gte', 'dense_e5', 'en_bge', 'en_gte', 'en_e5'],\n",
    "    'hybrid': ['hybrid_'],\n",
    "    'hierarchical': ['hier_', 'hierarchical_'],\n",
    "}\n",
    "\n",
    "# Embedding model detection\n",
    "EMBEDDING_MAP = {\n",
    "    'bge_large': 'BGE-large',\n",
    "    'bge_m3': 'BGE-M3',\n",
    "    'gte_qwen2': 'GTE-Qwen2-1.5B',\n",
    "    'e5_mistral': 'E5-Mistral-7B',\n",
    "}\n",
    "\n",
    "\n",
    "def parse_experiment_name(name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse experiment name into structured components.\n",
    "    \n",
    "    Handles formats:\n",
    "    - direct_vllm_metallamaLlama3.23BInstruct_concise_nq\n",
    "    - rag_vllm_metallamaLlama3.23BInstruct_dense_bge_large_512_k5_hyde_bge_concise_nq\n",
    "    - Singleton experiments (iterative_*, selfrag_*, premium_*)\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'name': name,\n",
    "        'exp_type': 'unknown',\n",
    "        'model': 'unknown',\n",
    "        'model_short': 'unknown',\n",
    "        'dataset': 'unknown',\n",
    "        'prompt': 'unknown',\n",
    "        'retriever': None,\n",
    "        'retriever_type': None,\n",
    "        'embedding_model': None,\n",
    "        'top_k': None,\n",
    "        'query_transform': 'none',\n",
    "        'reranker': 'none',\n",
    "        'is_singleton': False,\n",
    "    }\n",
    "    \n",
    "    # Detect dataset (always at end)\n",
    "    for ds in ['nq', 'triviaqa', 'hotpotqa']:\n",
    "        if name.endswith(f'_{ds}'):\n",
    "            config['dataset'] = ds\n",
    "            break\n",
    "    \n",
    "    # Handle singleton experiments\n",
    "    if name.startswith('iterative_') or name.startswith('selfrag_') or name.startswith('premium_'):\n",
    "        config['is_singleton'] = True\n",
    "        config['exp_type'] = 'rag'\n",
    "        if 'llama' in name.lower():\n",
    "            config['model_short'] = 'Llama-3.2-3B'\n",
    "        elif 'phi' in name.lower():\n",
    "            config['model_short'] = 'Phi-3-mini'\n",
    "        elif 'qwen' in name.lower():\n",
    "            config['model_short'] = 'Qwen-2.5-3B'\n",
    "        \n",
    "        if name.startswith('iterative_'):\n",
    "            config['retriever_type'] = 'iterative'\n",
    "            # Parse iterations\n",
    "            iter_match = re.search(r'(\\d+)iter', name)\n",
    "            config['query_transform'] = f\"iterative_{iter_match.group(1)}\" if iter_match else 'iterative'\n",
    "        elif name.startswith('selfrag_'):\n",
    "            config['retriever_type'] = 'self_rag'\n",
    "            config['query_transform'] = 'self_rag'\n",
    "        elif name.startswith('premium_'):\n",
    "            config['retriever_type'] = 'hybrid'\n",
    "            config['query_transform'] = 'hyde'\n",
    "            config['reranker'] = 'bge-v2'\n",
    "        return config\n",
    "    \n",
    "    # Direct experiments\n",
    "    if name.startswith('direct_'):\n",
    "        config['exp_type'] = 'direct'\n",
    "        # Parse model from name\n",
    "        for key, display in MODEL_MAP.items():\n",
    "            if key.lower() in name.lower():\n",
    "                config['model_short'] = display\n",
    "                break\n",
    "        # Parse prompt (before dataset)\n",
    "        for prompt in ['concise', 'structured', 'cot', 'fewshot_3', 'fewshot', 'extractive', 'cited']:\n",
    "            if f'_{prompt}_' in name or name.endswith(f'_{prompt}_{config[\"dataset\"]}'):\n",
    "                config['prompt'] = prompt\n",
    "                break\n",
    "        return config\n",
    "    \n",
    "    # RAG experiments\n",
    "    if name.startswith('rag_'):\n",
    "        config['exp_type'] = 'rag'\n",
    "        \n",
    "        # Parse model\n",
    "        for key, display in MODEL_MAP.items():\n",
    "            if key.lower() in name.lower():\n",
    "                config['model_short'] = display\n",
    "                break\n",
    "        \n",
    "        # Parse top_k\n",
    "        k_match = re.search(r'_k(\\d+)_', name)\n",
    "        if k_match:\n",
    "            config['top_k'] = int(k_match.group(1))\n",
    "        \n",
    "        # Parse retriever type and embedding model\n",
    "        for rtype, patterns in RETRIEVER_TYPES.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern in name.lower():\n",
    "                    config['retriever_type'] = rtype\n",
    "                    break\n",
    "        \n",
    "        for key, display in EMBEDDING_MAP.items():\n",
    "            if key in name.lower():\n",
    "                config['embedding_model'] = display\n",
    "                break\n",
    "        \n",
    "        # Parse query transform\n",
    "        if '_hyde_' in name.lower():\n",
    "            config['query_transform'] = 'hyde'\n",
    "        elif '_multiquery_' in name.lower():\n",
    "            config['query_transform'] = 'multiquery'\n",
    "        \n",
    "        # Parse reranker\n",
    "        if '_bgev2_' in name.lower() or '_bge-v2_' in name.lower():\n",
    "            config['reranker'] = 'bge-v2'\n",
    "        elif '_bge_' in name.lower() and config['embedding_model'] is None:\n",
    "            # bge in name but not as embedding = reranker\n",
    "            config['reranker'] = 'bge'\n",
    "        \n",
    "        # Parse prompt\n",
    "        for prompt in ['concise', 'structured', 'cot', 'fewshot_3', 'fewshot', 'extractive', 'cited']:\n",
    "            if f'_{prompt}_' in name:\n",
    "                config['prompt'] = prompt\n",
    "                break\n",
    "        \n",
    "        # Extract full retriever name\n",
    "        # Pattern: after model, before _k{n}_\n",
    "        if k_match:\n",
    "            retriever_match = re.search(r'Instruct_(.+?)_k\\d+', name)\n",
    "            if retriever_match:\n",
    "                config['retriever'] = retriever_match.group(1)\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def load_all_results(study_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all experiment results into a DataFrame.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if not study_path.exists():\n",
    "        print(f\"Warning: Study path does not exist: {study_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for exp_dir in study_path.iterdir():\n",
    "        if not exp_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Try results.json first, then metadata.json\n",
    "        results_file = exp_dir / \"results.json\"\n",
    "        metadata_file = exp_dir / \"metadata.json\"\n",
    "        \n",
    "        data = None\n",
    "        if results_file.exists():\n",
    "            with open(results_file) as f:\n",
    "                data = json.load(f)\n",
    "        elif metadata_file.exists():\n",
    "            with open(metadata_file) as f:\n",
    "                data = json.load(f)\n",
    "            # Also load summary if exists\n",
    "            summary_files = list(exp_dir.glob(\"*_summary.json\"))\n",
    "            if summary_files:\n",
    "                with open(summary_files[0]) as f:\n",
    "                    summary = json.load(f)\n",
    "                data['metrics'] = summary.get('overall_metrics', summary)\n",
    "        \n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Parse experiment name\n",
    "            exp_name = data.get('name', exp_dir.name)\n",
    "            config = parse_experiment_name(exp_name)\n",
    "            \n",
    "            # Add metrics\n",
    "            row = config.copy()\n",
    "            metrics = data.get('metrics', data)\n",
    "            for metric in METRICS:\n",
    "                if metric in metrics:\n",
    "                    row[metric] = metrics[metric]\n",
    "                elif metric in data:\n",
    "                    row[metric] = data[metric]\n",
    "            \n",
    "            # Add sample count and timing\n",
    "            row['n_samples'] = data.get('n_samples', data.get('num_questions', None))\n",
    "            row['duration'] = data.get('duration', 0)\n",
    "            row['throughput'] = data.get('throughput_qps', 0)\n",
    "            \n",
    "            results.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {exp_dir.name}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(['exp_type', 'model_short', 'dataset']).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = load_all_results(STUDY_PATH)\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nExperiment types: {df['exp_type'].value_counts().to_dict()}\")\n",
    "    print(f\"Models: {sorted(df['model_short'].dropna().unique())}\")\n",
    "    print(f\"Datasets: {sorted(df['dataset'].dropna().unique())}\")\n",
    "    print(f\"Retriever types: {df['retriever_type'].dropna().unique().tolist()}\")\n",
    "    print(f\"\\nMetrics available: {[m for m in METRICS if m in df.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show experiment distribution (important for stratified sampling)\n",
    "if len(df) > 0:\n",
    "    print(\"Experiment Distribution (crucial for weighted analysis)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # By key dimensions\n",
    "    for dim in ['model_short', 'retriever_type', 'query_transform', 'reranker', 'prompt', 'dataset']:\n",
    "        if dim in df.columns:\n",
    "            counts = df[dim].value_counts()\n",
    "            print(f\"\\n{dim}:\")\n",
    "            for val, count in counts.items():\n",
    "                print(f\"  {val}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weighted Analysis Functions\n",
    "\n",
    "With stratified sampling, we have non-uniform experiment counts.\n",
    "We use **inverse frequency weighting** and **bootstrap confidence intervals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_with_ci(\n",
    "    df: pd.DataFrame, \n",
    "    group_col: str, \n",
    "    metric: str = PRIMARY_METRIC,\n",
    "    weight_by: str = None,\n",
    "    confidence: float = 0.95,\n",
    "    n_bootstrap: int = 1000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute weighted mean with bootstrap confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with experiment results\n",
    "        group_col: Column to group by\n",
    "        metric: Metric to analyze\n",
    "        weight_by: Column to use for inverse frequency weighting (e.g., 'dataset')\n",
    "        confidence: Confidence level for CI\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "    \"\"\"\n",
    "    if metric not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for group_val, group_df in df.groupby(group_col):\n",
    "        values = group_df[metric].dropna().values\n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute weights if specified\n",
    "        if weight_by and weight_by in group_df.columns:\n",
    "            # Inverse frequency weighting\n",
    "            weight_counts = group_df[weight_by].value_counts()\n",
    "            weights = group_df[weight_by].map(lambda x: 1.0 / weight_counts.get(x, 1))\n",
    "            weights = weights / weights.sum()  # Normalize\n",
    "            weighted_mean = (group_df[metric] * weights).sum()\n",
    "        else:\n",
    "            weighted_mean = np.mean(values)\n",
    "            weights = None\n",
    "        \n",
    "        # Bootstrap CI\n",
    "        if len(values) >= 3:\n",
    "            bootstrap_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                sample = np.random.choice(values, size=len(values), replace=True)\n",
    "                bootstrap_means.append(np.mean(sample))\n",
    "            alpha = (1 - confidence) / 2\n",
    "            ci_low = np.percentile(bootstrap_means, alpha * 100)\n",
    "            ci_high = np.percentile(bootstrap_means, (1 - alpha) * 100)\n",
    "        else:\n",
    "            ci_low = ci_high = weighted_mean\n",
    "        \n",
    "        results.append({\n",
    "            group_col: group_val,\n",
    "            'mean': weighted_mean,\n",
    "            'std': np.std(values) if len(values) > 1 else 0,\n",
    "            'ci_low': ci_low,\n",
    "            'ci_high': ci_high,\n",
    "            'n': len(values),\n",
    "            'min': np.min(values),\n",
    "            'max': np.max(values),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('mean', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def effect_size(baseline_values: np.ndarray, treatment_values: np.ndarray) -> Tuple[float, float, str]:\n",
    "    \"\"\"\n",
    "    Compute Cohen's d effect size and interpret it.\n",
    "    \n",
    "    Returns: (effect_size, p_value, interpretation)\n",
    "    \"\"\"\n",
    "    if len(baseline_values) < 2 or len(treatment_values) < 2:\n",
    "        return 0, 1, 'insufficient data'\n",
    "    \n",
    "    # Cohen's d\n",
    "    pooled_std = np.sqrt((\n",
    "        (len(baseline_values) - 1) * np.var(baseline_values, ddof=1) + \n",
    "        (len(treatment_values) - 1) * np.var(treatment_values, ddof=1)\n",
    "    ) / (len(baseline_values) + len(treatment_values) - 2))\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0, 1, 'no variance'\n",
    "    \n",
    "    d = (np.mean(treatment_values) - np.mean(baseline_values)) / pooled_std\n",
    "    \n",
    "    # t-test\n",
    "    t_stat, p_value = stats.ttest_ind(treatment_values, baseline_values)\n",
    "    \n",
    "    # Interpret\n",
    "    if abs(d) < 0.2:\n",
    "        interpretation = 'negligible'\n",
    "    elif abs(d) < 0.5:\n",
    "        interpretation = 'small'\n",
    "    elif abs(d) < 0.8:\n",
    "        interpretation = 'medium'\n",
    "    else:\n",
    "        interpretation = 'large'\n",
    "    \n",
    "    return d, p_value, interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG vs Direct LLM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    direct_df = df[df['exp_type'] == 'direct']\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    print(\"RAG vs Direct LLM Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if len(direct_df) > 0 and len(rag_df) > 0:\n",
    "        direct_mean = direct_df[PRIMARY_METRIC].mean()\n",
    "        rag_mean = rag_df[PRIMARY_METRIC].mean()\n",
    "        \n",
    "        d, p, interp = effect_size(\n",
    "            direct_df[PRIMARY_METRIC].dropna().values,\n",
    "            rag_df[PRIMARY_METRIC].dropna().values\n",
    "        )\n",
    "        \n",
    "        print(f\"Direct LLM: {direct_mean:.4f} (n={len(direct_df)})\")\n",
    "        print(f\"RAG:        {rag_mean:.4f} (n={len(rag_df)})\")\n",
    "        print(f\"\\nImprovement: {rag_mean - direct_mean:+.4f} ({(rag_mean/direct_mean - 1)*100:+.1f}%)\")\n",
    "        print(f\"Effect size (Cohen's d): {d:.3f} ({interp})\")\n",
    "        print(f\"P-value: {p:.4f} {'âœ“ significant' if p < 0.05 else 'âœ— not significant'}\")\n",
    "    else:\n",
    "        print(\"Need both direct and RAG experiments for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG vs Direct by model and dataset\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    direct_df = df[df['exp_type'] == 'direct']\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    if len(direct_df) > 0 and len(rag_df) > 0:\n",
    "        comparisons = []\n",
    "        for model in df['model_short'].dropna().unique():\n",
    "            for dataset in df['dataset'].dropna().unique():\n",
    "                direct_vals = direct_df[(direct_df['model_short'] == model) & \n",
    "                                        (direct_df['dataset'] == dataset)][PRIMARY_METRIC].dropna()\n",
    "                rag_vals = rag_df[(rag_df['model_short'] == model) & \n",
    "                                  (rag_df['dataset'] == dataset)][PRIMARY_METRIC].dropna()\n",
    "                \n",
    "                if len(direct_vals) > 0 and len(rag_vals) > 0:\n",
    "                    comparisons.append({\n",
    "                        'model': model,\n",
    "                        'dataset': dataset,\n",
    "                        'direct': direct_vals.mean(),\n",
    "                        'rag_mean': rag_vals.mean(),\n",
    "                        'rag_best': rag_vals.max(),\n",
    "                        'improvement': rag_vals.mean() - direct_vals.mean(),\n",
    "                        'n_rag': len(rag_vals),\n",
    "                    })\n",
    "        \n",
    "        if comparisons:\n",
    "            comp_df = pd.DataFrame(comparisons)\n",
    "            print(\"\\nRAG vs Direct by Model Ã— Dataset\")\n",
    "            display(comp_df.round(4))\n",
    "            \n",
    "            # Visualize\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Grouped bar by model\n",
    "            model_comp = comp_df.groupby('model').agg({\n",
    "                'direct': 'mean',\n",
    "                'rag_mean': 'mean',\n",
    "                'rag_best': 'mean',\n",
    "            })\n",
    "            model_comp.plot(kind='bar', ax=axes[0], width=0.7)\n",
    "            axes[0].set_title(f'RAG vs Direct by Model ({PRIMARY_METRIC})')\n",
    "            axes[0].set_ylabel(PRIMARY_METRIC.upper())\n",
    "            axes[0].legend(['Direct', 'RAG Mean', 'RAG Best'])\n",
    "            axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "            \n",
    "            # Improvement heatmap\n",
    "            pivot = comp_df.pivot(index='model', columns='dataset', values='improvement')\n",
    "            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', center=0, ax=axes[1])\n",
    "            axes[1].set_title(f'{PRIMARY_METRIC} Improvement (RAG - Direct)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Component Effect Analysis\n",
    "\n",
    "Analyze the marginal effect of each component while controlling for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_component_effects(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str = PRIMARY_METRIC,\n",
    "    components: List[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze the effect of each component on the metric.\n",
    "    Reports: mean, CI, effect size vs baseline, significance.\n",
    "    \"\"\"\n",
    "    if components is None:\n",
    "        components = ['model_short', 'retriever_type', 'embedding_model', \n",
    "                      'query_transform', 'reranker', 'prompt', 'top_k']\n",
    "    \n",
    "    rag_df = df[df['exp_type'] == 'rag'].copy()\n",
    "    if metric not in rag_df.columns or len(rag_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_effects = []\n",
    "    \n",
    "    for comp in components:\n",
    "        if comp not in rag_df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline (most common or 'none')\n",
    "        value_counts = rag_df[comp].value_counts()\n",
    "        if len(value_counts) < 2:\n",
    "            continue\n",
    "        \n",
    "        if 'none' in value_counts.index:\n",
    "            baseline_val = 'none'\n",
    "        else:\n",
    "            baseline_val = value_counts.index[0]\n",
    "        \n",
    "        baseline_scores = rag_df[rag_df[comp] == baseline_val][metric].dropna().values\n",
    "        \n",
    "        for val in value_counts.index:\n",
    "            if val == baseline_val:\n",
    "                continue\n",
    "            \n",
    "            treatment_scores = rag_df[rag_df[comp] == val][metric].dropna().values\n",
    "            \n",
    "            if len(treatment_scores) < 2:\n",
    "                continue\n",
    "            \n",
    "            d, p, interp = effect_size(baseline_scores, treatment_scores)\n",
    "            \n",
    "            all_effects.append({\n",
    "                'component': comp,\n",
    "                'baseline': baseline_val,\n",
    "                'treatment': val,\n",
    "                'baseline_mean': np.mean(baseline_scores),\n",
    "                'treatment_mean': np.mean(treatment_scores),\n",
    "                'improvement': np.mean(treatment_scores) - np.mean(baseline_scores),\n",
    "                'effect_size': d,\n",
    "                'effect_interp': interp,\n",
    "                'p_value': p,\n",
    "                'significant': p < 0.05,\n",
    "                'n_baseline': len(baseline_scores),\n",
    "                'n_treatment': len(treatment_scores),\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_effects).sort_values('effect_size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    effects_df = analyze_component_effects(df)\n",
    "    if len(effects_df) > 0:\n",
    "        print(f\"Component Effects on {PRIMARY_METRIC}\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Positive effect_size = treatment better than baseline\\n\")\n",
    "        display(effects_df[['component', 'baseline', 'treatment', 'improvement', \n",
    "                           'effect_size', 'effect_interp', 'p_value', 'significant', \n",
    "                           'n_baseline', 'n_treatment']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component effects\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    effects_df = analyze_component_effects(df)\n",
    "    if len(effects_df) > 0:\n",
    "        # Filter to significant effects\n",
    "        sig_effects = effects_df[effects_df['significant']].copy()\n",
    "        \n",
    "        if len(sig_effects) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            # Create labels\n",
    "            sig_effects['label'] = sig_effects.apply(\n",
    "                lambda r: f\"{r['component']}:\\n{r['baseline']}â†’{r['treatment']}\", axis=1\n",
    "            )\n",
    "            \n",
    "            colors = ['green' if x > 0 else 'red' for x in sig_effects['effect_size']]\n",
    "            \n",
    "            bars = ax.barh(sig_effects['label'], sig_effects['effect_size'], color=colors, alpha=0.7)\n",
    "            ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "            ax.set_xlabel(\"Cohen's d (Effect Size)\")\n",
    "            ax.set_title(f\"Significant Component Effects on {PRIMARY_METRIC} (p<0.05)\")\n",
    "            \n",
    "            # Add effect size labels\n",
    "            for bar, interp in zip(bars, sig_effects['effect_interp']):\n",
    "                width = bar.get_width()\n",
    "                ax.annotate(f'{interp}',\n",
    "                           xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                           xytext=(3, 0), textcoords='offset points',\n",
    "                           ha='left' if width > 0 else 'right', va='center', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No statistically significant effects found (p<0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison (with non-uniform weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    if len(rag_df) > 0:\n",
    "        # Weighted by dataset to account for non-uniform sampling\n",
    "        model_stats = weighted_mean_with_ci(rag_df, 'model_short', PRIMARY_METRIC, weight_by='dataset')\n",
    "        \n",
    "        print(f\"Model Comparison (weighted by dataset)\")\n",
    "        print(\"=\"*60)\n",
    "        display(model_stats.round(4))\n",
    "        \n",
    "        # Visualize with error bars\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        \n",
    "        x = range(len(model_stats))\n",
    "        ax.bar(x, model_stats['mean'], \n",
    "               yerr=[model_stats['mean'] - model_stats['ci_low'], \n",
    "                     model_stats['ci_high'] - model_stats['mean']],\n",
    "               capsize=5, alpha=0.7)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_stats['model_short'])\n",
    "        ax.set_ylabel(PRIMARY_METRIC.upper())\n",
    "        ax.set_title(f'Model Performance with 95% CI ({PRIMARY_METRIC})')\n",
    "        \n",
    "        for i, row in model_stats.iterrows():\n",
    "            ax.annotate(f\"{row['mean']:.3f}\\n(n={row['n']})\",\n",
    "                       xy=(i, row['mean']), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    for dimension in ['retriever_type', 'embedding_model', 'query_transform', 'reranker', 'top_k']:\n",
    "        if dimension not in rag_df.columns:\n",
    "            continue\n",
    "        if rag_df[dimension].dropna().nunique() < 2:\n",
    "            continue\n",
    "        \n",
    "        stats = weighted_mean_with_ci(rag_df, dimension, PRIMARY_METRIC, weight_by='dataset')\n",
    "        \n",
    "        if len(stats) > 0:\n",
    "            print(f\"\\n{dimension.replace('_', ' ').title()}\")\n",
    "            print(\"-\"*50)\n",
    "            display(stats.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps for key interactions\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    if len(rag_df) > 10:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "        \n",
    "        interactions = [\n",
    "            ('model_short', 'retriever_type'),\n",
    "            ('model_short', 'query_transform'),\n",
    "            ('retriever_type', 'embedding_model'),\n",
    "            ('query_transform', 'reranker'),\n",
    "        ]\n",
    "        \n",
    "        for ax, (row_dim, col_dim) in zip(axes.flatten(), interactions):\n",
    "            if row_dim not in rag_df.columns or col_dim not in rag_df.columns:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "            \n",
    "            pivot = rag_df.pivot_table(\n",
    "                index=row_dim, columns=col_dim, values=PRIMARY_METRIC, aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            if pivot.empty:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "            \n",
    "            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax)\n",
    "            ax.set_title(f'{row_dim} Ã— {col_dim}')\n",
    "        \n",
    "        plt.suptitle(f'Interaction Effects ({PRIMARY_METRIC})', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    # Combine direct and RAG for prompt analysis\n",
    "    prompt_stats = weighted_mean_with_ci(df, 'prompt', PRIMARY_METRIC, weight_by='dataset')\n",
    "    \n",
    "    if len(prompt_stats) > 0:\n",
    "        print(\"Prompt Strategy Comparison\")\n",
    "        print(\"=\"*60)\n",
    "        display(prompt_stats.round(4))\n",
    "        \n",
    "        # By experiment type\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Overall\n",
    "        ax = axes[0]\n",
    "        ax.bar(prompt_stats['prompt'], prompt_stats['mean'],\n",
    "               yerr=[prompt_stats['mean'] - prompt_stats['ci_low'],\n",
    "                     prompt_stats['ci_high'] - prompt_stats['mean']],\n",
    "               capsize=5, alpha=0.7)\n",
    "        ax.set_ylabel(PRIMARY_METRIC.upper())\n",
    "        ax.set_title('Prompt Performance (Overall)')\n",
    "        ax.set_xticklabels(prompt_stats['prompt'], rotation=45, ha='right')\n",
    "        \n",
    "        # By type (Direct vs RAG)\n",
    "        ax = axes[1]\n",
    "        type_prompt = df.groupby(['exp_type', 'prompt'])[PRIMARY_METRIC].mean().unstack()\n",
    "        type_prompt.plot(kind='bar', ax=ax, width=0.7)\n",
    "        ax.set_ylabel(PRIMARY_METRIC.upper())\n",
    "        ax.set_title('Prompt Ã— Experiment Type')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "        ax.legend(title='Prompt')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    \n",
    "    print(\"Best Configurations per Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for dataset in df['dataset'].dropna().unique():\n",
    "        ds_df = rag_df[rag_df['dataset'] == dataset]\n",
    "        if len(ds_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        best_idx = ds_df[PRIMARY_METRIC].idxmax()\n",
    "        best = ds_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{dataset.upper()}:\")\n",
    "        print(f\"  Best {PRIMARY_METRIC}: {best[PRIMARY_METRIC]:.4f}\")\n",
    "        print(f\"  Model: {best['model_short']}\")\n",
    "        print(f\"  Retriever: {best.get('retriever_type', 'N/A')}\")\n",
    "        print(f\"  Embedding: {best.get('embedding_model', 'N/A')}\")\n",
    "        print(f\"  Query transform: {best.get('query_transform', 'none')}\")\n",
    "        print(f\"  Reranker: {best.get('reranker', 'none')}\")\n",
    "        print(f\"  Prompt: {best.get('prompt', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset difficulty comparison\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    dataset_stats = weighted_mean_with_ci(df, 'dataset', PRIMARY_METRIC)\n",
    "    \n",
    "    print(\"\\nDataset Difficulty (lower = harder)\")\n",
    "    print(\"=\"*60)\n",
    "    display(dataset_stats.round(4))\n",
    "    \n",
    "    # Which strategies work best on hardest dataset?\n",
    "    hardest = dataset_stats.iloc[-1]['dataset']\n",
    "    print(f\"\\nStrategies on hardest dataset ({hardest}):\")\n",
    "    \n",
    "    rag_df = df[(df['exp_type'] == 'rag') & (df['dataset'] == hardest)]\n",
    "    if len(rag_df) > 0:\n",
    "        for comp in ['retriever_type', 'query_transform', 'reranker']:\n",
    "            if comp in rag_df.columns:\n",
    "                stats = rag_df.groupby(comp)[PRIMARY_METRIC].mean().sort_values(ascending=False)\n",
    "                best = stats.index[0]\n",
    "                print(f\"  Best {comp}: {best} ({stats.iloc[0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bottleneck Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bottlenecks(df: pd.DataFrame, metric: str = PRIMARY_METRIC) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Identify bottlenecks in the RAG pipeline.\n",
    "    \n",
    "    Returns analysis of where improvements would have biggest impact.\n",
    "    \"\"\"\n",
    "    rag_df = df[df['exp_type'] == 'rag'].copy()\n",
    "    if len(rag_df) < 10:\n",
    "        return {}\n",
    "    \n",
    "    # Compute variance explained by each component\n",
    "    total_var = rag_df[metric].var()\n",
    "    \n",
    "    variance_explained = {}\n",
    "    components = ['model_short', 'retriever_type', 'embedding_model', \n",
    "                  'query_transform', 'reranker', 'prompt', 'top_k', 'dataset']\n",
    "    \n",
    "    for comp in components:\n",
    "        if comp not in rag_df.columns:\n",
    "            continue\n",
    "        if rag_df[comp].dropna().nunique() < 2:\n",
    "            continue\n",
    "        \n",
    "        # Between-group variance\n",
    "        group_means = rag_df.groupby(comp)[metric].mean()\n",
    "        grand_mean = rag_df[metric].mean()\n",
    "        group_sizes = rag_df.groupby(comp).size()\n",
    "        \n",
    "        ss_between = sum(group_sizes[g] * (group_means[g] - grand_mean)**2 \n",
    "                        for g in group_means.index)\n",
    "        \n",
    "        variance_explained[comp] = ss_between / (total_var * len(rag_df)) if total_var > 0 else 0\n",
    "    \n",
    "    # Sort by impact\n",
    "    sorted_components = sorted(variance_explained.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'variance_explained': dict(sorted_components),\n",
    "        'top_bottleneck': sorted_components[0][0] if sorted_components else None,\n",
    "        'total_experiments': len(rag_df),\n",
    "    }\n",
    "\n",
    "\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    bottlenecks = identify_bottlenecks(df)\n",
    "    \n",
    "    if bottlenecks:\n",
    "        print(\"Bottleneck Analysis (Variance Explained)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Higher = more impact on performance variance\\n\")\n",
    "        \n",
    "        var_exp = bottlenecks['variance_explained']\n",
    "        for comp, var in var_exp.items():\n",
    "            bar = 'â–ˆ' * int(var * 50)\n",
    "            print(f\"{comp:20s} {var:6.1%} {bar}\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Top bottleneck: {bottlenecks['top_bottleneck']}\")\n",
    "        print(\"   â†’ Focus optimization efforts here for biggest gains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(df: pd.DataFrame, metric: str = PRIMARY_METRIC):\n",
    "    \"\"\"Generate actionable recommendations based on analysis.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“‹ RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rag_df = df[df['exp_type'] == 'rag']\n",
    "    direct_df = df[df['exp_type'] == 'direct']\n",
    "    \n",
    "    if len(rag_df) == 0:\n",
    "        print(\"Insufficient RAG experiments for recommendations.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Best overall\n",
    "    best_idx = rag_df[metric].idxmax()\n",
    "    best = rag_df.loc[best_idx]\n",
    "    print(f\"\\n1. BEST OVERALL CONFIGURATION ({metric}={best[metric]:.4f})\")\n",
    "    print(f\"   Model: {best['model_short']}\")\n",
    "    print(f\"   Retriever: {best.get('retriever_type', 'N/A')} / {best.get('embedding_model', 'N/A')}\")\n",
    "    print(f\"   Query: {best.get('query_transform', 'none')}, Reranker: {best.get('reranker', 'none')}\")\n",
    "    print(f\"   Prompt: {best.get('prompt', 'N/A')}, top_k: {best.get('top_k', 'N/A')}\")\n",
    "    \n",
    "    # 2. Component recommendations\n",
    "    effects_df = analyze_component_effects(df, metric)\n",
    "    if len(effects_df) > 0:\n",
    "        sig_positive = effects_df[(effects_df['significant']) & (effects_df['improvement'] > 0)]\n",
    "        if len(sig_positive) > 0:\n",
    "            print(f\"\\n2. SIGNIFICANT IMPROVEMENTS (p<0.05)\")\n",
    "            for _, row in sig_positive.head(5).iterrows():\n",
    "                print(f\"   âœ“ {row['component']}: {row['baseline']} â†’ {row['treatment']}\")\n",
    "                print(f\"     Effect: {row['improvement']:+.4f} ({row['effect_interp']})\")\n",
    "    \n",
    "    # 3. Bottleneck\n",
    "    bottlenecks = identify_bottlenecks(df, metric)\n",
    "    if bottlenecks and bottlenecks['top_bottleneck']:\n",
    "        print(f\"\\n3. PRIORITY FOR OPTIMIZATION\")\n",
    "        print(f\"   Focus on: {bottlenecks['top_bottleneck']}\")\n",
    "        var = bottlenecks['variance_explained'].get(bottlenecks['top_bottleneck'], 0)\n",
    "        print(f\"   Explains {var:.1%} of performance variance\")\n",
    "    \n",
    "    # 4. Dataset-specific\n",
    "    print(f\"\\n4. DATASET-SPECIFIC INSIGHTS\")\n",
    "    for dataset in df['dataset'].dropna().unique():\n",
    "        ds_rag = rag_df[rag_df['dataset'] == dataset]\n",
    "        ds_direct = direct_df[direct_df['dataset'] == dataset]\n",
    "        \n",
    "        if len(ds_rag) > 0 and len(ds_direct) > 0:\n",
    "            rag_best = ds_rag[metric].max()\n",
    "            direct_best = ds_direct[metric].max()\n",
    "            improvement = (rag_best - direct_best) / direct_best * 100 if direct_best > 0 else 0\n",
    "            print(f\"   {dataset}: RAG improves by {improvement:+.1f}% over direct\")\n",
    "    \n",
    "    # 5. Quick wins\n",
    "    print(f\"\\n5. QUICK WINS (low complexity, positive effect)\")\n",
    "    quick_wins = [\n",
    "        ('prompt', 'Try structured or cot prompts'),\n",
    "        ('reranker', 'Add bge-v2 reranker'),\n",
    "        ('query_transform', 'Enable HyDE'),\n",
    "    ]\n",
    "    for comp, desc in quick_wins:\n",
    "        if comp in effects_df['component'].values:\n",
    "            comp_effects = effects_df[effects_df['component'] == comp]\n",
    "            best_effect = comp_effects.iloc[0] if len(comp_effects) > 0 else None\n",
    "            if best_effect is not None and best_effect['improvement'] > 0:\n",
    "                print(f\"   âœ“ {desc}: +{best_effect['improvement']:.4f} {metric}\")\n",
    "\n",
    "\n",
    "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
    "    generate_recommendations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    output_dir = STUDY_PATH / \"analysis\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Full results\n",
    "    df.to_csv(output_dir / \"full_results.csv\", index=False)\n",
    "    \n",
    "    # Component effects\n",
    "    if PRIMARY_METRIC in df.columns:\n",
    "        effects_df = analyze_component_effects(df)\n",
    "        if len(effects_df) > 0:\n",
    "            effects_df.to_csv(output_dir / \"component_effects.csv\", index=False)\n",
    "    \n",
    "    # Bottleneck analysis\n",
    "    bottlenecks = identify_bottlenecks(df)\n",
    "    if bottlenecks:\n",
    "        with open(output_dir / \"bottleneck_analysis.json\", 'w') as f:\n",
    "            json.dump(bottlenecks, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
