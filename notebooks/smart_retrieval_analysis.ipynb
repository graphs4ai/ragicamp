{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Retrieval Analysis: What Improves RAG Performance?\n",
    "\n",
    "This notebook analyzes the Smart Retrieval SLM study results.\n",
    "\n",
    "## Experiment Groups\n",
    "- **Group A**: Embedding Model Comparison (BGE-large vs BGE-M3)\n",
    "- **Group D**: Reranking (overfetch + rerank)\n",
    "- **Group E**: Query Transformation (HyDE, MultiQuery)\n",
    "- **Group F**: Advanced Agents (Iterative RAG, Self-RAG)\n",
    "\n",
    "## Key Questions\n",
    "1. Does RAG improve over Direct LLM?\n",
    "2. Which embedding model performs better?\n",
    "3. Does reranking help? At what fetch_k?\n",
    "4. Do query transformations (HyDE, MultiQuery) improve retrieval?\n",
    "5. Do advanced agents outperform simple RAG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_experiment_name(name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse experiment name to extract configuration.\n",
    "    \n",
    "    Naming convention: {group}{num}_{description}_{dataset}\n",
    "    Examples:\n",
    "        - direct_vllmmetallama_Llama3.23BInstruct_concise_nq\n",
    "        - a1_bge_large_baseline_nq\n",
    "        - d1_rerank_bge_top3_hotpotqa\n",
    "        - f1_iterative_1round_triviaqa\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'name': name,\n",
    "        'group': None,\n",
    "        'group_name': None,\n",
    "        'exp_type': 'rag',\n",
    "        'hypothesis': None,\n",
    "        'dataset': None,\n",
    "        'embedding_model': None,\n",
    "        'reranker': None,\n",
    "        'query_transform': None,\n",
    "        'agent_type': 'fixed_rag',\n",
    "        'top_k': None,\n",
    "        'fetch_k': None,\n",
    "    }\n",
    "    \n",
    "    # Detect direct LLM baseline\n",
    "    if name.startswith('direct_'):\n",
    "        config['exp_type'] = 'direct'\n",
    "        config['group'] = 'baseline'\n",
    "        config['group_name'] = 'Direct LLM'\n",
    "    \n",
    "    # Parse group from prefix\n",
    "    if name.startswith('a1_') or name.startswith('a2_'):\n",
    "        config['group'] = 'A'\n",
    "        config['group_name'] = 'Embedding Model'\n",
    "        if 'bge_large' in name:\n",
    "            config['embedding_model'] = 'BGE-large'\n",
    "        elif 'bge_m3' in name:\n",
    "            config['embedding_model'] = 'BGE-M3'\n",
    "    \n",
    "    elif name.startswith('d1_') or name.startswith('d2_'):\n",
    "        config['group'] = 'D'\n",
    "        config['group_name'] = 'Reranking'\n",
    "        config['reranker'] = 'BGE-reranker'\n",
    "        if 'top3' in name:\n",
    "            config['top_k'] = 3\n",
    "            config['fetch_k'] = 20\n",
    "        elif 'top5' in name:\n",
    "            config['top_k'] = 5\n",
    "            config['fetch_k'] = 25\n",
    "    \n",
    "    elif name.startswith('e1_') or name.startswith('e2_') or name.startswith('e3_'):\n",
    "        config['group'] = 'E'\n",
    "        config['group_name'] = 'Query Transform'\n",
    "        if 'hyde' in name:\n",
    "            config['query_transform'] = 'HyDE'\n",
    "        elif 'multiquery' in name:\n",
    "            config['query_transform'] = 'MultiQuery'\n",
    "        if 'rerank' in name:\n",
    "            config['reranker'] = 'BGE-reranker'\n",
    "    \n",
    "    elif name.startswith('f1_') or name.startswith('f2_') or name.startswith('f3_') or name.startswith('f4_'):\n",
    "        config['group'] = 'F'\n",
    "        config['group_name'] = 'Advanced Agents'\n",
    "        if 'iterative' in name:\n",
    "            config['agent_type'] = 'iterative_rag'\n",
    "            if '1round' in name:\n",
    "                config['hypothesis'] = 'Iterative (1 round)'\n",
    "            elif '2round' in name:\n",
    "                config['hypothesis'] = 'Iterative (2 rounds)'\n",
    "        elif 'selfrag' in name:\n",
    "            config['agent_type'] = 'self_rag'\n",
    "            if 'verified' in name:\n",
    "                config['hypothesis'] = 'Self-RAG (verified)'\n",
    "            else:\n",
    "                config['hypothesis'] = 'Self-RAG (balanced)'\n",
    "    \n",
    "    # Extract dataset\n",
    "    if '_nq' in name or name.endswith('_nq'):\n",
    "        config['dataset'] = 'NQ'\n",
    "    elif '_hotpotqa' in name or name.endswith('_hotpotqa'):\n",
    "        config['dataset'] = 'HotpotQA'\n",
    "    elif '_triviaqa' in name or name.endswith('_triviaqa'):\n",
    "        config['dataset'] = 'TriviaQA'\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def load_all_results(study_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all experiment results into a DataFrame.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for exp_dir in study_path.iterdir():\n",
    "        if not exp_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Try results.json first, then predictions.json\n",
    "        results_file = exp_dir / \"results.json\"\n",
    "        predictions_file = exp_dir / \"predictions.json\"\n",
    "        \n",
    "        if results_file.exists():\n",
    "            with open(results_file) as f:\n",
    "                data = json.load(f)\n",
    "        elif predictions_file.exists():\n",
    "            with open(predictions_file) as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Parse experiment name\n",
    "            exp_name = data.get('name', exp_dir.name)\n",
    "            config = parse_experiment_name(exp_name)\n",
    "            \n",
    "            # Add metrics\n",
    "            metrics = data.get('metrics', data.get('aggregate_metrics', {}))\n",
    "            config.update({\n",
    "                'f1': metrics.get('f1'),\n",
    "                'exact_match': metrics.get('exact_match'),\n",
    "                'bertscore_f1': metrics.get('bertscore_f1'),\n",
    "                'bleurt': metrics.get('bleurt'),\n",
    "                'num_predictions': len(data.get('predictions', [])),\n",
    "            })\n",
    "            \n",
    "            results.append(config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {exp_dir.name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "df = load_all_results(STUDY_PATH)\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "print(f\"\\nExperiments by group:\")\n",
    "print(df.groupby('group_name').size())\n",
    "print(f\"\\nExperiments by dataset:\")\n",
    "print(df.groupby('dataset').size())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by group\n",
    "metrics = ['f1', 'exact_match', 'bertscore_f1', 'bleurt']\n",
    "summary = df.groupby('group_name')[metrics].agg(['mean', 'std', 'max']).round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: F1 by Group\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot by group\n",
    "df_plot = df[df['group_name'].notna()].copy()\n",
    "order = ['Direct LLM', 'Embedding Model', 'Reranking', 'Query Transform', 'Advanced Agents']\n",
    "order = [g for g in order if g in df_plot['group_name'].unique()]\n",
    "\n",
    "sns.boxplot(data=df_plot, x='group_name', y='f1', order=order, ax=axes[0])\n",
    "axes[0].set_title('F1 Score by Experiment Group')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Box plot by dataset\n",
    "sns.boxplot(data=df_plot, x='dataset', y='f1', ax=axes[1])\n",
    "axes[1].set_title('F1 Score by Dataset')\n",
    "axes[1].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG vs Direct LLM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline (direct LLM) performance\n",
    "direct_df = df[df['exp_type'] == 'direct'].copy()\n",
    "rag_df = df[df['exp_type'] == 'rag'].copy()\n",
    "\n",
    "print(\"Direct LLM Baselines:\")\n",
    "print(direct_df[['name', 'dataset', 'f1', 'exact_match', 'bertscore_f1']].to_string())\n",
    "\n",
    "# Create baseline lookup\n",
    "baseline_lookup = direct_df.groupby('dataset')['f1'].mean().to_dict()\n",
    "print(f\"\\nBaseline F1 by dataset: {baseline_lookup}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement over baseline\n",
    "rag_df = rag_df.copy()\n",
    "rag_df['baseline_f1'] = rag_df['dataset'].map(baseline_lookup)\n",
    "rag_df['f1_improvement'] = rag_df['f1'] - rag_df['baseline_f1']\n",
    "rag_df['f1_improvement_pct'] = (rag_df['f1_improvement'] / rag_df['baseline_f1'] * 100).round(1)\n",
    "\n",
    "# Summary\n",
    "print(f\"RAG experiments that improve over baseline: {(rag_df['f1_improvement'] > 0).sum()} / {len(rag_df)}\")\n",
    "print(f\"Average F1 improvement: {rag_df['f1_improvement'].mean():.3f} ({rag_df['f1_improvement_pct'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Improvement distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(rag_df['f1_improvement'].dropna(), bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', label='Baseline')\n",
    "axes[0].set_xlabel('F1 Improvement over Direct LLM')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of RAG Improvement')\n",
    "axes[0].legend()\n",
    "\n",
    "# By group\n",
    "improvement_by_group = rag_df.groupby('group_name')['f1_improvement'].mean().sort_values(ascending=False)\n",
    "improvement_by_group.plot(kind='bar', ax=axes[1], color='steelblue', edgecolor='black')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('Average F1 Improvement')\n",
    "axes[1].set_title('F1 Improvement by Experiment Group')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Embedding Model Comparison (Group A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Group A experiments\n",
    "group_a = rag_df[rag_df['group'] == 'A'].copy()\n",
    "\n",
    "if len(group_a) > 0:\n",
    "    print(\"Embedding Model Comparison:\")\n",
    "    comparison = group_a.groupby(['embedding_model', 'dataset'])[metrics].mean().round(3)\n",
    "    print(comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    group_a.pivot_table(values='f1', index='dataset', columns='embedding_model').plot(kind='bar', ax=ax)\n",
    "    ax.set_title('F1 by Embedding Model and Dataset')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.legend(title='Embedding Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Group A experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Reranking Impact (Group D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Group D experiments\n",
    "group_d = rag_df[rag_df['group'] == 'D'].copy()\n",
    "\n",
    "if len(group_d) > 0:\n",
    "    print(\"Reranking Impact:\")\n",
    "    comparison = group_d.groupby(['top_k', 'fetch_k', 'dataset'])[metrics].mean().round(3)\n",
    "    print(comparison)\n",
    "    \n",
    "    # Compare with Group A baseline (no reranking)\n",
    "    if len(group_a) > 0:\n",
    "        baseline_f1 = group_a.groupby('dataset')['f1'].mean()\n",
    "        rerank_f1 = group_d.groupby('dataset')['f1'].mean()\n",
    "        \n",
    "        print(\"\\nReranking improvement over baseline:\")\n",
    "        for ds in rerank_f1.index:\n",
    "            if ds in baseline_f1.index:\n",
    "                diff = rerank_f1[ds] - baseline_f1[ds]\n",
    "                print(f\"  {ds}: {diff:+.3f} ({diff/baseline_f1[ds]*100:+.1f}%)\")\n",
    "else:\n",
    "    print(\"No Group D experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Query Transformation Impact (Group E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Group E experiments\n",
    "group_e = rag_df[rag_df['group'] == 'E'].copy()\n",
    "\n",
    "if len(group_e) > 0:\n",
    "    print(\"Query Transformation Impact:\")\n",
    "    comparison = group_e.groupby(['query_transform', 'dataset'])[metrics].mean().round(3)\n",
    "    print(comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    group_e.pivot_table(values='f1', index='dataset', columns='query_transform').plot(kind='bar', ax=ax)\n",
    "    ax.set_title('F1 by Query Transformation and Dataset')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.legend(title='Query Transform')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Group E experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Advanced Agents (Group F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Group F experiments\n",
    "group_f = rag_df[rag_df['group'] == 'F'].copy()\n",
    "\n",
    "if len(group_f) > 0:\n",
    "    print(\"Advanced Agents Impact:\")\n",
    "    comparison = group_f.groupby(['hypothesis', 'dataset'])[metrics].mean().round(3)\n",
    "    print(comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    group_f.pivot_table(values='f1', index='dataset', columns='hypothesis').plot(kind='bar', ax=ax)\n",
    "    ax.set_title('F1 by Agent Type and Dataset')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.legend(title='Agent Type', bbox_to_anchor=(1.02, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Group F experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best and Worst Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best RAG configurations\n",
    "print(\"Top 10 Best RAG Configurations by F1:\")\n",
    "top_10 = rag_df.nlargest(10, 'f1')[['name', 'group_name', 'dataset', 'f1', 'f1_improvement_pct']]\n",
    "print(top_10.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 worst RAG configurations\n",
    "print(\"Bottom 10 Worst RAG Configurations by F1:\")\n",
    "bottom_10 = rag_df.nsmallest(10, 'f1')[['name', 'group_name', 'dataset', 'f1', 'f1_improvement_pct']]\n",
    "print(bottom_10.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration per dataset\n",
    "print(\"Best RAG Configuration per Dataset:\")\n",
    "best_per_dataset = rag_df.loc[rag_df.groupby('dataset')['f1'].idxmax()]\n",
    "print(best_per_dataset[['name', 'dataset', 'group_name', 'f1', 'f1_improvement_pct']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Metric Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between metrics\n",
    "metric_cols = ['f1', 'exact_match', 'bertscore_f1', 'bleurt']\n",
    "available_metrics = [m for m in metric_cols if m in df.columns and df[m].notna().any()]\n",
    "\n",
    "if len(available_metrics) >= 2:\n",
    "    corr = df[available_metrics].corr().round(2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, ax=ax)\n",
    "    ax.set_title('Metric Correlations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Only {len(available_metrics)} metrics available, need at least 2 for correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY: What Improves RAG Performance?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# RAG vs Direct\n",
    "if len(rag_df) > 0 and 'f1_improvement' in rag_df.columns:\n",
    "    rag_helps_pct = (rag_df['f1_improvement'] > 0).mean() * 100\n",
    "    avg_improvement = rag_df['f1_improvement'].mean()\n",
    "    print(f\"\\n1. RAG vs Direct LLM:\")\n",
    "    print(f\"   - RAG improves over baseline in {rag_helps_pct:.0f}% of experiments\")\n",
    "    print(f\"   - Average improvement: {avg_improvement:+.3f} F1\")\n",
    "\n",
    "# Best group\n",
    "if len(rag_df) > 0:\n",
    "    group_means = rag_df.groupby('group_name')['f1'].mean().sort_values(ascending=False)\n",
    "    print(f\"\\n2. Best Performing Groups (by avg F1):\")\n",
    "    for group, f1 in group_means.items():\n",
    "        print(f\"   - {group}: {f1:.3f}\")\n",
    "\n",
    "# Best overall config\n",
    "if len(rag_df) > 0:\n",
    "    best = rag_df.loc[rag_df['f1'].idxmax()]\n",
    "    print(f\"\\n3. Best Overall Configuration:\")\n",
    "    print(f\"   - {best['name']}\")\n",
    "    print(f\"   - F1: {best['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
