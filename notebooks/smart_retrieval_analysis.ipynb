{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Smart Retrieval SLM Analysis\n",
        "\n",
        "## Hypothesis\n",
        "**\"High-quality retrieval can compensate for smaller, faster LLMs.\"**\n",
        "\n",
        "This notebook provides structured analysis of the experiment results, organized by:\n",
        "\n",
        "### Analysis Sections\n",
        "1. **Data Loading & Overview** - Load results, parse experiment groups\n",
        "2. **RAG vs Direct LLM** - Does retrieval help SLMs?\n",
        "3. **SLM Comparison** - Which small model performs best with RAG?\n",
        "4. **Retrieval Strategy Analysis** - Component-by-component breakdown:\n",
        "   - Aâ†’B: Effect of Hybrid Retrieval\n",
        "   - Aâ†’C: Effect of Reranking\n",
        "   - Aâ†’D: Effect of Query Transform (HyDE)\n",
        "   - Aâ†’E: Effect of Iterative RAG\n",
        "   - Aâ†’F: Effect of Premium Stack (Hybrid + Rerank)\n",
        "5. **Embedding Model Comparison** - Does E5-Mistral beat BGE-large?\n",
        "6. **Cross-Dataset Analysis** - Which strategies generalize?\n",
        "7. **Statistical Significance** - Confidence in our findings\n",
        "8. **Recommendations** - Actionable insights\n",
        "\n",
        "### Experiment Groups\n",
        "| Group | Strategy | Description |\n",
        "|-------|----------|-------------|\n",
        "| Direct | No RAG | Baseline (3 SLMs) |\n",
        "| A | Simple Dense | BGE-large, top_k=5 |\n",
        "| B | Hybrid | Dense + BM25 (70/30) |\n",
        "| C | Reranking | Dense + BGE reranker |\n",
        "| D | HyDE | Query transformation |\n",
        "| E | Iterative | Query refinement |\n",
        "| F | Premium | Hybrid + Rerank |\n",
        "| G | E5-Mistral | Better embeddings |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Style settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Paths\n",
        "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
        "\n",
        "# Metrics to analyze (in order of importance)\n",
        "METRICS = ['f1', 'exact_match', 'bertscore', 'bleurt']\n",
        "PRIMARY_METRIC = 'f1'  # Main metric for comparisons\n",
        "\n",
        "# SLM display names\n",
        "SLM_NAMES = {\n",
        "    'llama': 'Llama-3.2-3B',\n",
        "    'phi3': 'Phi-3-mini',\n",
        "    'qwen': 'Qwen-2.5-3B',\n",
        "}\n",
        "\n",
        "# Group descriptions\n",
        "GROUP_INFO = {\n",
        "    'direct': {'name': 'Direct LLM', 'strategy': 'No retrieval'},\n",
        "    'a': {'name': 'Simple Dense', 'strategy': 'BGE-large embeddings'},\n",
        "    'b': {'name': 'Hybrid', 'strategy': 'Dense + BM25 (70/30)'},\n",
        "    'c': {'name': 'Reranking', 'strategy': 'Dense + BGE reranker'},\n",
        "    'd': {'name': 'HyDE', 'strategy': 'Query transformation'},\n",
        "    'e': {'name': 'Iterative', 'strategy': 'Query refinement'},\n",
        "    'f': {'name': 'Premium', 'strategy': 'Hybrid + Rerank'},\n",
        "    'g': {'name': 'E5-Mistral', 'strategy': 'Better embeddings'},\n",
        "}\n",
        "\n",
        "print(f\"Study path: {STUDY_PATH}\")\n",
        "print(f\"Exists: {STUDY_PATH.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_experiment_name(name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Parse smart_retrieval_slm experiment names.\n",
        "    \n",
        "    Formats:\n",
        "    - direct_vllm_metallamaLlama3.23BInstruct_concise_none_nq\n",
        "    - a1_llama_dense_nq\n",
        "    - b2_phi3_hybrid_triviaqa\n",
        "    - g1_qwen_e5mistral_dense_hotpotqa\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        'name': name,\n",
        "        'group': None,\n",
        "        'group_num': None,\n",
        "        'slm': None,\n",
        "        'slm_display': None,\n",
        "        'strategy': None,\n",
        "        'dataset': None,\n",
        "        'is_direct': False,\n",
        "        'is_rag': False,\n",
        "    }\n",
        "    \n",
        "    # Handle direct experiments\n",
        "    if name.startswith('direct_'):\n",
        "        config['group'] = 'direct'\n",
        "        config['is_direct'] = True\n",
        "        config['strategy'] = 'No RAG'\n",
        "        \n",
        "        # Extract SLM from direct name\n",
        "        if 'Llama' in name or 'llama' in name:\n",
        "            config['slm'] = 'llama'\n",
        "        elif 'Phi' in name or 'phi' in name:\n",
        "            config['slm'] = 'phi3'\n",
        "        elif 'Qwen' in name or 'qwen' in name:\n",
        "            config['slm'] = 'qwen'\n",
        "        \n",
        "        # Extract dataset\n",
        "        for ds in ['nq', 'triviaqa', 'hotpotqa']:\n",
        "            if f'_{ds}' in name or name.endswith(ds):\n",
        "                config['dataset'] = ds\n",
        "                break\n",
        "    else:\n",
        "        # Parse singleton experiment: a1_llama_dense_nq\n",
        "        config['is_rag'] = True\n",
        "        parts = name.split('_')\n",
        "        \n",
        "        if len(parts) >= 2:\n",
        "            # First part: group + number (e.g., 'a1', 'b2', 'g1')\n",
        "            group_part = parts[0]\n",
        "            if len(group_part) >= 2 and group_part[0].isalpha():\n",
        "                config['group'] = group_part[0].lower()\n",
        "                config['group_num'] = group_part[1:] if group_part[1:].isdigit() else None\n",
        "            \n",
        "            # Second part: SLM (llama, phi3, qwen)\n",
        "            slm_part = parts[1].lower()\n",
        "            if 'llama' in slm_part:\n",
        "                config['slm'] = 'llama'\n",
        "            elif 'phi' in slm_part:\n",
        "                config['slm'] = 'phi3'\n",
        "            elif 'qwen' in slm_part:\n",
        "                config['slm'] = 'qwen'\n",
        "            \n",
        "            # Strategy from group\n",
        "            group_strategies = {\n",
        "                'a': 'Simple Dense',\n",
        "                'b': 'Hybrid',\n",
        "                'c': 'Reranking',\n",
        "                'd': 'HyDE',\n",
        "                'e': 'Iterative',\n",
        "                'f': 'Premium',\n",
        "                'g': 'E5-Mistral',\n",
        "            }\n",
        "            config['strategy'] = group_strategies.get(config['group'], 'Unknown')\n",
        "            \n",
        "            # Dataset is usually last part\n",
        "            for ds in ['nq', 'triviaqa', 'hotpotqa']:\n",
        "                if ds in parts[-1]:\n",
        "                    config['dataset'] = ds\n",
        "                    break\n",
        "    \n",
        "    # Add display name\n",
        "    config['slm_display'] = SLM_NAMES.get(config['slm'], config['slm'])\n",
        "    \n",
        "    return config\n",
        "\n",
        "\n",
        "def load_all_results(study_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Load all experiment results into a DataFrame.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    if not study_path.exists():\n",
        "        print(f\"Warning: Study path does not exist: {study_path}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    for exp_dir in study_path.iterdir():\n",
        "        if not exp_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        results_file = exp_dir / \"results.json\"\n",
        "        if not results_file.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            with open(results_file) as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            # Parse experiment name\n",
        "            config = parse_experiment_name(data['name'])\n",
        "            \n",
        "            # Add metrics\n",
        "            row = config.copy()\n",
        "            for metric in METRICS:\n",
        "                if metric in data:\n",
        "                    row[metric] = data[metric]\n",
        "                elif 'metrics' in data and metric in data['metrics']:\n",
        "                    row[metric] = data['metrics'][metric]\n",
        "            \n",
        "            # Add sample count if available\n",
        "            row['n_samples'] = data.get('n_samples', data.get('num_questions', None))\n",
        "            \n",
        "            results.append(row)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {exp_dir.name}: {e}\")\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Sort by group, slm, dataset\n",
        "    if not df.empty:\n",
        "        df = df.sort_values(['group', 'slm', 'dataset']).reset_index(drop=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load data\n",
        "df = load_all_results(STUDY_PATH)\n",
        "print(f\"Loaded {len(df)} experiments\")\n",
        "\n",
        "if len(df) > 0:\n",
        "    print(f\"\\nGroups: {sorted(df['group'].dropna().unique())}\")\n",
        "    print(f\"SLMs: {sorted(df['slm'].dropna().unique())}\")\n",
        "    print(f\"Datasets: {sorted(df['dataset'].dropna().unique())}\")\n",
        "    print(f\"\\nMetrics available: {[m for m in METRICS if m in df.columns]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show experiment overview by group\n",
        "if len(df) > 0:\n",
        "    overview = df.groupby(['group', 'strategy']).agg({\n",
        "        'name': 'count',\n",
        "        'slm': lambda x: ', '.join(sorted(x.dropna().unique())),\n",
        "        PRIMARY_METRIC: ['mean', 'std'] if PRIMARY_METRIC in df.columns else 'count'\n",
        "    }).round(3)\n",
        "    overview.columns = ['n_experiments', 'slms', f'{PRIMARY_METRIC}_mean', f'{PRIMARY_METRIC}_std']\n",
        "    display(overview)\n",
        "else:\n",
        "    print(\"No results loaded yet. Run experiments first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. RAG vs Direct LLM\n",
        "\n",
        "**Question:** Does retrieval help small language models?\n",
        "\n",
        "Compare Direct LLM baselines against Group A (Simple Dense RAG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_rag_vs_direct(df: pd.DataFrame, metric: str = PRIMARY_METRIC) -> pd.DataFrame:\n",
        "    \"\"\"Compare RAG (Group A) vs Direct for each SLM and dataset.\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    direct = df[df['group'] == 'direct'].copy()\n",
        "    rag_a = df[df['group'] == 'a'].copy()\n",
        "    \n",
        "    comparisons = []\n",
        "    for slm in df['slm'].dropna().unique():\n",
        "        for dataset in df['dataset'].dropna().unique():\n",
        "            direct_score = direct[(direct['slm'] == slm) & (direct['dataset'] == dataset)][metric].values\n",
        "            rag_score = rag_a[(rag_a['slm'] == slm) & (rag_a['dataset'] == dataset)][metric].values\n",
        "            \n",
        "            if len(direct_score) > 0 and len(rag_score) > 0:\n",
        "                diff = rag_score[0] - direct_score[0]\n",
        "                pct_change = (diff / direct_score[0]) * 100 if direct_score[0] > 0 else 0\n",
        "                comparisons.append({\n",
        "                    'slm': slm,\n",
        "                    'slm_display': SLM_NAMES.get(slm, slm),\n",
        "                    'dataset': dataset,\n",
        "                    f'direct_{metric}': direct_score[0],\n",
        "                    f'rag_{metric}': rag_score[0],\n",
        "                    'improvement': diff,\n",
        "                    'pct_change': pct_change,\n",
        "                    'rag_helps': diff > 0,\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(comparisons)\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    rag_comparison = compare_rag_vs_direct(df)\n",
        "    if not rag_comparison.empty:\n",
        "        print(\"RAG vs Direct LLM Comparison (Group A vs Direct)\")\n",
        "        print(\"=\"*60)\n",
        "        display(rag_comparison.round(3))\n",
        "        \n",
        "        # Summary\n",
        "        helps_pct = rag_comparison['rag_helps'].mean() * 100\n",
        "        avg_improvement = rag_comparison['improvement'].mean()\n",
        "        print(f\"\\nðŸ“Š Summary:\")\n",
        "        print(f\"   RAG helps in {helps_pct:.0f}% of cases\")\n",
        "        print(f\"   Average {PRIMARY_METRIC} improvement: {avg_improvement:+.3f}\")\n",
        "else:\n",
        "    print(\"Waiting for results...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize RAG vs Direct\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    rag_comparison = compare_rag_vs_direct(df)\n",
        "    if not rag_comparison.empty:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Plot 1: Grouped bar chart by SLM\n",
        "        ax1 = axes[0]\n",
        "        slm_summary = rag_comparison.groupby('slm_display').agg({\n",
        "            f'direct_{PRIMARY_METRIC}': 'mean',\n",
        "            f'rag_{PRIMARY_METRIC}': 'mean',\n",
        "        })\n",
        "        slm_summary.plot(kind='bar', ax=ax1, width=0.7)\n",
        "        ax1.set_title(f'RAG vs Direct by SLM ({PRIMARY_METRIC})')\n",
        "        ax1.set_xlabel('Small Language Model')\n",
        "        ax1.set_ylabel(PRIMARY_METRIC.upper())\n",
        "        ax1.legend(['Direct', 'RAG'])\n",
        "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
        "        \n",
        "        # Plot 2: Improvement by dataset\n",
        "        ax2 = axes[1]\n",
        "        pivot = rag_comparison.pivot(index='dataset', columns='slm_display', values='improvement')\n",
        "        pivot.plot(kind='bar', ax=ax2, width=0.7)\n",
        "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2.set_title(f'{PRIMARY_METRIC} Improvement (RAG - Direct)')\n",
        "        ax2.set_xlabel('Dataset')\n",
        "        ax2.set_ylabel(f'{PRIMARY_METRIC} Improvement')\n",
        "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SLM Comparison\n",
        "\n",
        "**Question:** Which small language model performs best with RAG?\n",
        "\n",
        "Compare Llama-3.2-3B, Phi-3-mini, and Qwen-2.5-3B across all strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_slms(df: pd.DataFrame, metric: str = PRIMARY_METRIC) -> pd.DataFrame:\n",
        "    \"\"\"Compare SLMs across all RAG strategies.\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    rag_df = df[df['is_rag'] == True].copy()\n",
        "    \n",
        "    # Aggregate by SLM and strategy\n",
        "    slm_comparison = rag_df.groupby(['slm_display', 'strategy']).agg({\n",
        "        metric: ['mean', 'std', 'count']\n",
        "    }).round(3)\n",
        "    slm_comparison.columns = ['mean', 'std', 'n']\n",
        "    \n",
        "    return slm_comparison\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    slm_comp = compare_slms(df)\n",
        "    if not slm_comp.empty:\n",
        "        print(f\"SLM Performance by Strategy ({PRIMARY_METRIC})\")\n",
        "        print(\"=\"*60)\n",
        "        display(slm_comp)\n",
        "        \n",
        "        # Best SLM per strategy\n",
        "        rag_df = df[df['is_rag'] == True]\n",
        "        best_per_strategy = rag_df.loc[rag_df.groupby('strategy')[PRIMARY_METRIC].idxmax()][['strategy', 'slm_display', PRIMARY_METRIC]]\n",
        "        print(f\"\\nðŸ† Best SLM per Strategy:\")\n",
        "        for _, row in best_per_strategy.iterrows():\n",
        "            print(f\"   {row['strategy']}: {row['slm_display']} ({row[PRIMARY_METRIC]:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap of SLM x Strategy\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    rag_df = df[df['is_rag'] == True]\n",
        "    if not rag_df.empty:\n",
        "        pivot = rag_df.pivot_table(\n",
        "            index='slm_display', \n",
        "            columns='strategy', \n",
        "            values=PRIMARY_METRIC, \n",
        "            aggfunc='mean'\n",
        "        )\n",
        "        \n",
        "        plt.figure(figsize=(12, 4))\n",
        "        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': PRIMARY_METRIC})\n",
        "        plt.title(f'SLM x Strategy Performance ({PRIMARY_METRIC})')\n",
        "        plt.xlabel('Strategy')\n",
        "        plt.ylabel('Small Language Model')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Retrieval Strategy Analysis\n",
        "\n",
        "**Question:** What is the effect of each retrieval component?\n",
        "\n",
        "We compare each strategy against the baseline (Group A: Simple Dense)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def component_analysis(df: pd.DataFrame, baseline_group: str = 'a', metric: str = PRIMARY_METRIC) -> pd.DataFrame:\n",
        "    \"\"\"Analyze improvement of each strategy over baseline.\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    baseline = df[df['group'] == baseline_group].copy()\n",
        "    results = []\n",
        "    \n",
        "    for group in df['group'].dropna().unique():\n",
        "        if group in ['direct', baseline_group]:\n",
        "            continue\n",
        "        \n",
        "        strategy_df = df[df['group'] == group]\n",
        "        group_name = GROUP_INFO.get(group, {}).get('name', group)\n",
        "        \n",
        "        for slm in df['slm'].dropna().unique():\n",
        "            for dataset in df['dataset'].dropna().unique():\n",
        "                base_score = baseline[\n",
        "                    (baseline['slm'] == slm) & (baseline['dataset'] == dataset)\n",
        "                ][metric].values\n",
        "                \n",
        "                strat_score = strategy_df[\n",
        "                    (strategy_df['slm'] == slm) & (strategy_df['dataset'] == dataset)\n",
        "                ][metric].values\n",
        "                \n",
        "                if len(base_score) > 0 and len(strat_score) > 0:\n",
        "                    diff = strat_score[0] - base_score[0]\n",
        "                    pct = (diff / base_score[0]) * 100 if base_score[0] > 0 else 0\n",
        "                    results.append({\n",
        "                        'group': group,\n",
        "                        'strategy': group_name,\n",
        "                        'slm': slm,\n",
        "                        'slm_display': SLM_NAMES.get(slm, slm),\n",
        "                        'dataset': dataset,\n",
        "                        f'baseline_{metric}': base_score[0],\n",
        "                        f'strategy_{metric}': strat_score[0],\n",
        "                        'improvement': diff,\n",
        "                        'pct_change': pct,\n",
        "                    })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    component_df = component_analysis(df)\n",
        "    if not component_df.empty:\n",
        "        # Summary by strategy\n",
        "        summary = component_df.groupby('strategy').agg({\n",
        "            'improvement': ['mean', 'std', 'min', 'max'],\n",
        "            'pct_change': 'mean',\n",
        "        }).round(3)\n",
        "        summary.columns = ['mean_improvement', 'std', 'min', 'max', 'avg_pct_change']\n",
        "        summary = summary.sort_values('mean_improvement', ascending=False)\n",
        "        \n",
        "        print(f\"Strategy Improvements over Simple Dense (Group A)\")\n",
        "        print(\"=\"*60)\n",
        "        display(summary)\n",
        "        \n",
        "        # Ranking\n",
        "        print(f\"\\nðŸ† Strategy Ranking (by avg {PRIMARY_METRIC} improvement):\")\n",
        "        for i, (strategy, row) in enumerate(summary.iterrows(), 1):\n",
        "            emoji = \"âœ…\" if row['mean_improvement'] > 0 else \"âŒ\"\n",
        "            print(f\"   {i}. {strategy}: {row['mean_improvement']:+.3f} ({row['avg_pct_change']:+.1f}%) {emoji}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize component effects\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    component_df = component_analysis(df)\n",
        "    if not component_df.empty:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Plot 1: Average improvement by strategy\n",
        "        ax1 = axes[0]\n",
        "        strategy_means = component_df.groupby('strategy')['improvement'].mean().sort_values(ascending=True)\n",
        "        colors = ['green' if x > 0 else 'red' for x in strategy_means.values]\n",
        "        strategy_means.plot(kind='barh', ax=ax1, color=colors)\n",
        "        ax1.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax1.set_title(f'Average {PRIMARY_METRIC} Improvement over Simple Dense')\n",
        "        ax1.set_xlabel(f'{PRIMARY_METRIC} Improvement')\n",
        "        ax1.set_ylabel('Strategy')\n",
        "        \n",
        "        # Plot 2: Box plot of improvements by strategy\n",
        "        ax2 = axes[1]\n",
        "        order = component_df.groupby('strategy')['improvement'].mean().sort_values(ascending=False).index\n",
        "        sns.boxplot(data=component_df, x='strategy', y='improvement', order=order, ax=ax2)\n",
        "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2.set_title(f'{PRIMARY_METRIC} Improvement Distribution')\n",
        "        ax2.set_xlabel('Strategy')\n",
        "        ax2.set_ylabel(f'{PRIMARY_METRIC} Improvement')\n",
        "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Detailed Component Breakdown\n",
        "\n",
        "Let's examine each component's effect in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_component_detail(component_df: pd.DataFrame, strategy: str, description: str):\n",
        "    \"\"\"Print detailed analysis for a specific strategy.\"\"\"\n",
        "    subset = component_df[component_df['strategy'] == strategy]\n",
        "    if subset.empty:\n",
        "        print(f\"No data for {strategy}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸ“Š {strategy}: {description}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # By SLM\n",
        "    by_slm = subset.groupby('slm_display')['improvement'].agg(['mean', 'std'])\n",
        "    print(f\"\\nBy SLM:\")\n",
        "    for slm, row in by_slm.iterrows():\n",
        "        emoji = \"âœ…\" if row['mean'] > 0 else \"âŒ\"\n",
        "        print(f\"   {slm}: {row['mean']:+.3f} Â± {row['std']:.3f} {emoji}\")\n",
        "    \n",
        "    # By Dataset\n",
        "    by_ds = subset.groupby('dataset')['improvement'].agg(['mean', 'std'])\n",
        "    print(f\"\\nBy Dataset:\")\n",
        "    for ds, row in by_ds.iterrows():\n",
        "        emoji = \"âœ…\" if row['mean'] > 0 else \"âŒ\"\n",
        "        print(f\"   {ds}: {row['mean']:+.3f} Â± {row['std']:.3f} {emoji}\")\n",
        "    \n",
        "    # Overall\n",
        "    overall = subset['improvement'].mean()\n",
        "    overall_pct = subset['pct_change'].mean()\n",
        "    print(f\"\\nOverall: {overall:+.3f} ({overall_pct:+.1f}%)\")\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    component_df = component_analysis(df)\n",
        "    if not component_df.empty:\n",
        "        print_component_detail(component_df, 'Hybrid', 'Dense + BM25 (70/30) keyword matching')\n",
        "        print_component_detail(component_df, 'Reranking', 'Cross-encoder reranking (fetch 25 â†’ keep 5)')\n",
        "        print_component_detail(component_df, 'HyDE', 'Hypothetical Document Embeddings')\n",
        "        print_component_detail(component_df, 'Iterative', 'Query refinement with 1 iteration')\n",
        "        print_component_detail(component_df, 'Premium', 'Hybrid + Reranking combined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embedding Model Comparison\n",
        "\n",
        "**Question:** Does E5-Mistral-7B (top MTEB) outperform BGE-large?\n",
        "\n",
        "Compare Group G (E5-Mistral) against equivalent experiments with BGE-large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_embeddings(df: pd.DataFrame, metric: str = PRIMARY_METRIC) -> pd.DataFrame:\n",
        "    \"\"\"Compare E5-Mistral (Group G) vs BGE-large (Group A).\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # G1 vs A3 (Qwen), G3 vs A2 (Phi-3), G4 vs A1 (Llama)\n",
        "    bge = df[df['group'] == 'a'].copy()\n",
        "    e5 = df[df['group'] == 'g'].copy()\n",
        "    \n",
        "    # Filter G to only dense experiments (not g2 which has reranking)\n",
        "    e5_dense = e5[~e5['name'].str.contains('rerank', case=False, na=False)]\n",
        "    \n",
        "    comparisons = []\n",
        "    for slm in df['slm'].dropna().unique():\n",
        "        for dataset in df['dataset'].dropna().unique():\n",
        "            bge_score = bge[(bge['slm'] == slm) & (bge['dataset'] == dataset)][metric].values\n",
        "            e5_score = e5_dense[(e5_dense['slm'] == slm) & (e5_dense['dataset'] == dataset)][metric].values\n",
        "            \n",
        "            if len(bge_score) > 0 and len(e5_score) > 0:\n",
        "                diff = e5_score[0] - bge_score[0]\n",
        "                comparisons.append({\n",
        "                    'slm': slm,\n",
        "                    'slm_display': SLM_NAMES.get(slm, slm),\n",
        "                    'dataset': dataset,\n",
        "                    f'bge_large_{metric}': bge_score[0],\n",
        "                    f'e5_mistral_{metric}': e5_score[0],\n",
        "                    'improvement': diff,\n",
        "                    'e5_better': diff > 0,\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(comparisons)\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    embed_comp = compare_embeddings(df)\n",
        "    if not embed_comp.empty:\n",
        "        print(\"E5-Mistral-7B vs BGE-large Comparison\")\n",
        "        print(\"=\"*60)\n",
        "        display(embed_comp.round(3))\n",
        "        \n",
        "        # Summary\n",
        "        e5_wins = embed_comp['e5_better'].mean() * 100\n",
        "        avg_diff = embed_comp['improvement'].mean()\n",
        "        print(f\"\\nðŸ“Š Summary:\")\n",
        "        print(f\"   E5-Mistral wins in {e5_wins:.0f}% of cases\")\n",
        "        print(f\"   Average improvement: {avg_diff:+.3f}\")\n",
        "        \n",
        "        if avg_diff > 0:\n",
        "            print(f\"   âœ… E5-Mistral-7B is better overall\")\n",
        "        else:\n",
        "            print(f\"   âŒ BGE-large is sufficient (E5 not worth the cost)\")\n",
        "    else:\n",
        "        print(\"No E5-Mistral experiments found in Group G yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cross-Dataset Analysis\n",
        "\n",
        "**Question:** Which strategies generalize across datasets?\n",
        "\n",
        "Some strategies may work well on NQ but fail on HotpotQA (multi-hop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    # Pivot: Strategy x Dataset\n",
        "    rag_df = df[df['is_rag'] == True]\n",
        "    if not rag_df.empty:\n",
        "        pivot = rag_df.pivot_table(\n",
        "            index='strategy',\n",
        "            columns='dataset',\n",
        "            values=PRIMARY_METRIC,\n",
        "            aggfunc='mean'\n",
        "        )\n",
        "        \n",
        "        # Add variance across datasets (lower = more consistent)\n",
        "        pivot['variance'] = pivot.var(axis=1)\n",
        "        pivot['consistent'] = pivot['variance'] < pivot['variance'].median()\n",
        "        pivot = pivot.sort_values('variance')\n",
        "        \n",
        "        print(f\"Strategy Performance by Dataset ({PRIMARY_METRIC})\")\n",
        "        print(\"=\"*60)\n",
        "        display(pivot.round(3))\n",
        "        \n",
        "        # Most consistent strategies\n",
        "        print(f\"\\nðŸŽ¯ Most Consistent Strategies (lowest variance):\")\n",
        "        for strategy in pivot.head(3).index:\n",
        "            var = pivot.loc[strategy, 'variance']\n",
        "            print(f\"   {strategy}: variance={var:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap of Strategy x Dataset\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    rag_df = df[df['is_rag'] == True]\n",
        "    if not rag_df.empty:\n",
        "        pivot = rag_df.pivot_table(\n",
        "            index='strategy',\n",
        "            columns='dataset',\n",
        "            values=PRIMARY_METRIC,\n",
        "            aggfunc='mean'\n",
        "        )\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn', center=pivot.values.mean())\n",
        "        plt.title(f'Strategy Performance by Dataset ({PRIMARY_METRIC})')\n",
        "        plt.xlabel('Dataset')\n",
        "        plt.ylabel('Strategy')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Significance\n",
        "\n",
        "**Question:** Are our findings statistically significant?\n",
        "\n",
        "We use paired t-tests to compare strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def statistical_tests(df: pd.DataFrame, metric: str = PRIMARY_METRIC) -> pd.DataFrame:\n",
        "    \"\"\"Perform statistical tests comparing strategies.\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    baseline = df[df['group'] == 'a']\n",
        "    results = []\n",
        "    \n",
        "    for group in ['b', 'c', 'd', 'e', 'f', 'g']:\n",
        "        strategy_df = df[df['group'] == group]\n",
        "        if strategy_df.empty:\n",
        "            continue\n",
        "        \n",
        "        strategy_name = GROUP_INFO.get(group, {}).get('name', group)\n",
        "        \n",
        "        # Match by slm and dataset\n",
        "        paired_scores = []\n",
        "        for slm in df['slm'].dropna().unique():\n",
        "            for dataset in df['dataset'].dropna().unique():\n",
        "                base = baseline[(baseline['slm'] == slm) & (baseline['dataset'] == dataset)][metric].values\n",
        "                strat = strategy_df[(strategy_df['slm'] == slm) & (strategy_df['dataset'] == dataset)][metric].values\n",
        "                if len(base) > 0 and len(strat) > 0:\n",
        "                    paired_scores.append((base[0], strat[0]))\n",
        "        \n",
        "        if len(paired_scores) >= 3:  # Need at least 3 pairs\n",
        "            base_scores = [p[0] for p in paired_scores]\n",
        "            strat_scores = [p[1] for p in paired_scores]\n",
        "            \n",
        "            t_stat, p_value = stats.ttest_rel(strat_scores, base_scores)\n",
        "            mean_diff = np.mean(strat_scores) - np.mean(base_scores)\n",
        "            \n",
        "            results.append({\n",
        "                'strategy': strategy_name,\n",
        "                'n_pairs': len(paired_scores),\n",
        "                'mean_difference': mean_diff,\n",
        "                't_statistic': t_stat,\n",
        "                'p_value': p_value,\n",
        "                'significant_05': p_value < 0.05,\n",
        "                'significant_01': p_value < 0.01,\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    stat_results = statistical_tests(df)\n",
        "    if not stat_results.empty:\n",
        "        print(f\"Statistical Significance (paired t-test vs Simple Dense)\")\n",
        "        print(\"=\"*60)\n",
        "        display(stat_results.round(4))\n",
        "        \n",
        "        sig_05 = stat_results[stat_results['significant_05']]['strategy'].tolist()\n",
        "        if sig_05:\n",
        "            print(f\"\\nâœ… Significant at p<0.05: {', '.join(sig_05)}\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ No strategies significantly better at p<0.05\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Recommendations\n",
        "\n",
        "Based on the analysis, here are the actionable insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_recommendations(df: pd.DataFrame, metric: str = PRIMARY_METRIC):\n",
        "    \"\"\"Generate recommendations based on analysis.\"\"\"\n",
        "    if df.empty or metric not in df.columns:\n",
        "        print(\"No data available for recommendations.\")\n",
        "        return\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ“‹ RECOMMENDATIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    rag_df = df[df['is_rag'] == True]\n",
        "    direct_df = df[df['is_direct'] == True]\n",
        "    \n",
        "    # 1. Best overall configuration\n",
        "    if not rag_df.empty:\n",
        "        best_idx = rag_df[metric].idxmax()\n",
        "        best = rag_df.loc[best_idx]\n",
        "        print(f\"\\n1. BEST OVERALL CONFIGURATION\")\n",
        "        print(f\"   Strategy: {best['strategy']}\")\n",
        "        print(f\"   SLM: {best['slm_display']}\")\n",
        "        print(f\"   {metric}: {best[metric]:.3f}\")\n",
        "    \n",
        "    # 2. Best SLM\n",
        "    if not rag_df.empty:\n",
        "        slm_avg = rag_df.groupby('slm_display')[metric].mean().sort_values(ascending=False)\n",
        "        print(f\"\\n2. BEST SLM (averaged across strategies)\")\n",
        "        for i, (slm, score) in enumerate(slm_avg.items(), 1):\n",
        "            emoji = \"ðŸ¥‡\" if i == 1 else \"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\"\n",
        "            print(f\"   {emoji} {slm}: {score:.3f}\")\n",
        "    \n",
        "    # 3. Most impactful component\n",
        "    component_df = component_analysis(df)\n",
        "    if not component_df.empty:\n",
        "        strategy_impact = component_df.groupby('strategy')['improvement'].mean().sort_values(ascending=False)\n",
        "        print(f\"\\n3. MOST IMPACTFUL COMPONENTS (vs Simple Dense)\")\n",
        "        for strategy, impact in strategy_impact.head(3).items():\n",
        "            emoji = \"âœ…\" if impact > 0 else \"âŒ\"\n",
        "            print(f\"   {emoji} {strategy}: {impact:+.3f}\")\n",
        "    \n",
        "    # 4. RAG vs Direct summary\n",
        "    if not rag_df.empty and not direct_df.empty:\n",
        "        rag_avg = rag_df[metric].mean()\n",
        "        direct_avg = direct_df[metric].mean()\n",
        "        print(f\"\\n4. RAG VALUE\")\n",
        "        print(f\"   Direct LLM avg: {direct_avg:.3f}\")\n",
        "        print(f\"   RAG avg: {rag_avg:.3f}\")\n",
        "        print(f\"   Improvement: {rag_avg - direct_avg:+.3f} ({((rag_avg/direct_avg)-1)*100:+.1f}%)\")\n",
        "    \n",
        "    # 5. Cost-effectiveness\n",
        "    print(f\"\\n5. COST-EFFECTIVENESS RANKING\")\n",
        "    print(f\"   (Best performance-to-complexity ratio)\")\n",
        "    complexity = {\n",
        "        'Simple Dense': 1,\n",
        "        'Hybrid': 2,\n",
        "        'Reranking': 3,\n",
        "        'HyDE': 4,\n",
        "        'Iterative': 5,\n",
        "        'Premium': 6,\n",
        "        'E5-Mistral': 7,\n",
        "    }\n",
        "    if not component_df.empty:\n",
        "        strategy_scores = component_df.groupby('strategy')['improvement'].mean()\n",
        "        cost_eff = [(s, strategy_scores.get(s, 0) / complexity.get(s, 1)) \n",
        "                    for s in complexity.keys() if s in strategy_scores.index]\n",
        "        cost_eff.sort(key=lambda x: x[1], reverse=True)\n",
        "        for strategy, eff in cost_eff[:3]:\n",
        "            print(f\"   - {strategy}: efficiency={eff:.3f}\")\n",
        "\n",
        "\n",
        "if len(df) > 0 and PRIMARY_METRIC in df.columns:\n",
        "    generate_recommendations(df)\n",
        "else:\n",
        "    print(\"Run experiments first to generate recommendations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. All Metrics Comparison\n",
        "\n",
        "Let's see the full picture with all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0:\n",
        "    # Summary table with all metrics\n",
        "    available_metrics = [m for m in METRICS if m in df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        rag_df = df[df['is_rag'] == True]\n",
        "        if not rag_df.empty:\n",
        "            summary = rag_df.groupby('strategy')[available_metrics].mean().round(3)\n",
        "            summary = summary.sort_values(PRIMARY_METRIC, ascending=False)\n",
        "            \n",
        "            print(\"All Metrics Summary by Strategy\")\n",
        "            print(\"=\"*60)\n",
        "            display(summary)\n",
        "            \n",
        "            # Correlation between metrics\n",
        "            if len(available_metrics) > 1:\n",
        "                print(f\"\\nMetric Correlations:\")\n",
        "                corr = rag_df[available_metrics].corr()\n",
        "                display(corr.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export processed results\n",
        "if len(df) > 0:\n",
        "    output_dir = STUDY_PATH / \"analysis\"\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Full results\n",
        "    df.to_csv(output_dir / \"full_results.csv\", index=False)\n",
        "    \n",
        "    # Component analysis\n",
        "    if PRIMARY_METRIC in df.columns:\n",
        "        component_df = component_analysis(df)\n",
        "        if not component_df.empty:\n",
        "            component_df.to_csv(output_dir / \"component_analysis.csv\", index=False)\n",
        "    \n",
        "    print(f\"Results exported to: {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
