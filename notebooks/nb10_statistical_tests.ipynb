{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB10: Statistical Significance Tests\n",
    "\n",
    "**Question:** Are the performance differences we observe statistically significant?\n",
    "\n",
    "This notebook produces thesis-ready results:\n",
    "1. **Paired bootstrap tests** for key comparisons (RAG vs Direct, reranker impact, etc.)\n",
    "2. **Wilcoxon signed-rank tests** as non-parametric alternative\n",
    "3. **Multiple comparison correction** (Holm-Bonferroni)\n",
    "4. **LaTeX tables** for direct inclusion in thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "from itertools import combinations\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, load_per_item_scores, setup_plotting,\n",
    "    compute_per_question_rag_delta, effect_size,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER, MODEL_PARAMS,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "np.random.seed(42)\n",
    "\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "ALPHA = 0.05  # Significance level\n",
    "N_BOOTSTRAP = 10_000  # Bootstrap iterations\n",
    "\n",
    "# Load experiment-level results\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "df['tier'] = df['model_short'].map(MODEL_TIER)\n",
    "df['params_b'] = df['model_short'].map(MODEL_PARAMS)\n",
    "\n",
    "rag = df[df['exp_type'] == 'rag'].copy()\n",
    "direct = df[df['exp_type'] == 'direct'].copy()\n",
    "\n",
    "print(f\"Total experiments: {len(df)} ({len(rag)} RAG, {len(direct)} direct)\")\n",
    "print(f\"Models: {sorted(df['model_short'].unique())}\")\n",
    "print(f\"Datasets: {sorted(df['dataset'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Statistical Test Functions\n",
    "\n",
    "Paired bootstrap and Wilcoxon tests with Holm-Bonferroni correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_bootstrap_test(a: np.ndarray, b: np.ndarray, n_bootstrap: int = N_BOOTSTRAP,\n",
    "                          seed: int = 42) -> dict:\n",
    "    \"\"\"Paired bootstrap test: is mean(b) > mean(a)?\n",
    "\n",
    "    Returns dict with observed_diff, ci_low, ci_high, p_value.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    assert len(a) == len(b), f\"Paired test requires equal lengths: {len(a)} vs {len(b)}\"\n",
    "\n",
    "    observed_diff = np.mean(b) - np.mean(a)\n",
    "    diffs = b - a\n",
    "\n",
    "    boot_diffs = np.empty(n_bootstrap)\n",
    "    for i in range(n_bootstrap):\n",
    "        idx = rng.choice(len(diffs), size=len(diffs), replace=True)\n",
    "        boot_diffs[i] = np.mean(diffs[idx])\n",
    "\n",
    "    ci_low = np.percentile(boot_diffs, 2.5)\n",
    "    ci_high = np.percentile(boot_diffs, 97.5)\n",
    "\n",
    "    # Two-sided p-value: proportion of bootstrap samples on the other side of 0\n",
    "    if observed_diff >= 0:\n",
    "        p_value = np.mean(boot_diffs <= 0) * 2\n",
    "    else:\n",
    "        p_value = np.mean(boot_diffs >= 0) * 2\n",
    "    p_value = min(p_value, 1.0)\n",
    "\n",
    "    return {\n",
    "        'observed_diff': observed_diff,\n",
    "        'ci_low': ci_low,\n",
    "        'ci_high': ci_high,\n",
    "        'p_value': p_value,\n",
    "        'n': len(a),\n",
    "    }\n",
    "\n",
    "\n",
    "def unpaired_bootstrap_test(a: np.ndarray, b: np.ndarray, n_bootstrap: int = N_BOOTSTRAP,\n",
    "                            seed: int = 42) -> dict:\n",
    "    \"\"\"Unpaired bootstrap test: is mean(b) > mean(a)?\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    observed_diff = np.mean(b) - np.mean(a)\n",
    "\n",
    "    boot_diffs = np.empty(n_bootstrap)\n",
    "    for i in range(n_bootstrap):\n",
    "        boot_a = rng.choice(a, size=len(a), replace=True)\n",
    "        boot_b = rng.choice(b, size=len(b), replace=True)\n",
    "        boot_diffs[i] = np.mean(boot_b) - np.mean(boot_a)\n",
    "\n",
    "    ci_low = np.percentile(boot_diffs, 2.5)\n",
    "    ci_high = np.percentile(boot_diffs, 97.5)\n",
    "\n",
    "    if observed_diff >= 0:\n",
    "        p_value = np.mean(boot_diffs <= 0) * 2\n",
    "    else:\n",
    "        p_value = np.mean(boot_diffs >= 0) * 2\n",
    "    p_value = min(p_value, 1.0)\n",
    "\n",
    "    return {\n",
    "        'observed_diff': observed_diff,\n",
    "        'ci_low': ci_low,\n",
    "        'ci_high': ci_high,\n",
    "        'p_value': p_value,\n",
    "        'n_a': len(a),\n",
    "        'n_b': len(b),\n",
    "    }\n",
    "\n",
    "\n",
    "def holm_bonferroni(p_values: list[float], alpha: float = ALPHA) -> list[dict]:\n",
    "    \"\"\"Apply Holm-Bonferroni correction to a list of p-values.\n",
    "\n",
    "    Returns list of dicts with original_p, adjusted_p, significant.\n",
    "    \"\"\"\n",
    "    n = len(p_values)\n",
    "    indexed = sorted(enumerate(p_values), key=lambda x: x[1])\n",
    "\n",
    "    results = [None] * n\n",
    "    for rank, (orig_idx, p) in enumerate(indexed):\n",
    "        adjusted = p * (n - rank)\n",
    "        adjusted = min(adjusted, 1.0)\n",
    "        results[orig_idx] = {\n",
    "            'original_p': p,\n",
    "            'adjusted_p': adjusted,\n",
    "            'significant': adjusted < alpha,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def significance_stars(p: float) -> str:\n",
    "    \"\"\"Return significance stars for a p-value.\"\"\"\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "\n",
    "def format_ci(diff, ci_low, ci_high, decimals=3) -> str:\n",
    "    \"\"\"Format difference with CI for display.\"\"\"\n",
    "    return f\"{diff:+.{decimals}f} [{ci_low:+.{decimals}f}, {ci_high:+.{decimals}f}]\"\n",
    "\n",
    "\n",
    "print(\"Statistical test functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. RAG vs Direct Baseline\n",
    "\n",
    "For each model x dataset pair, compare the best RAG config against the best direct baseline.\n",
    "Uses unpaired bootstrap since RAG and direct have different numbers of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG vs Direct: per model x dataset\n",
    "rag_vs_direct_rows = []\n",
    "\n",
    "for model in sorted(df['model_short'].unique()):\n",
    "    for dataset in sorted(df['dataset'].unique()):\n",
    "        d_vals = direct[(direct['model_short'] == model) & (direct['dataset'] == dataset)][PRIMARY_METRIC].dropna().values\n",
    "        r_vals = rag[(rag['model_short'] == model) & (rag['dataset'] == dataset)][PRIMARY_METRIC].dropna().values\n",
    "\n",
    "        if len(d_vals) < 1 or len(r_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        result = unpaired_bootstrap_test(d_vals, r_vals)\n",
    "\n",
    "        # Also compute Wilcoxon if enough samples\n",
    "        if len(r_vals) >= 5:\n",
    "            # Mann-Whitney U (unpaired non-parametric)\n",
    "            u_stat, mwu_p = scipy_stats.mannwhitneyu(r_vals, d_vals, alternative='two-sided')\n",
    "        else:\n",
    "            mwu_p = np.nan\n",
    "\n",
    "        d_cohen, _, interp = effect_size(d_vals, r_vals)\n",
    "\n",
    "        rag_vs_direct_rows.append({\n",
    "            'model': model,\n",
    "            'tier': MODEL_TIER.get(model, 'unknown'),\n",
    "            'dataset': dataset,\n",
    "            'direct_mean': np.mean(d_vals),\n",
    "            'rag_mean': np.mean(r_vals),\n",
    "            'diff': result['observed_diff'],\n",
    "            'ci_low': result['ci_low'],\n",
    "            'ci_high': result['ci_high'],\n",
    "            'p_boot': result['p_value'],\n",
    "            'p_mwu': mwu_p,\n",
    "            'cohens_d': d_cohen,\n",
    "            'effect': interp,\n",
    "            'n_direct': len(d_vals),\n",
    "            'n_rag': len(r_vals),\n",
    "        })\n",
    "\n",
    "rvd = pd.DataFrame(rag_vs_direct_rows)\n",
    "\n",
    "if not rvd.empty:\n",
    "    # Apply Holm-Bonferroni correction\n",
    "    corrections = holm_bonferroni(rvd['p_boot'].tolist())\n",
    "    rvd['p_adjusted'] = [c['adjusted_p'] for c in corrections]\n",
    "    rvd['sig'] = [significance_stars(c['adjusted_p']) for c in corrections]\n",
    "\n",
    "    print(\"RAG vs Direct: Per Model x Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    display(rvd[['model', 'tier', 'dataset', 'direct_mean', 'rag_mean', 'diff',\n",
    "                 'ci_low', 'ci_high', 'p_adjusted', 'sig', 'cohens_d', 'effect']].round(4))\n",
    "\n",
    "    # Summary: how often does RAG significantly help?\n",
    "    n_sig_help = ((rvd['sig'] != 'ns') & (rvd['diff'] > 0)).sum()\n",
    "    n_sig_hurt = ((rvd['sig'] != 'ns') & (rvd['diff'] < 0)).sum()\n",
    "    n_ns = (rvd['sig'] == 'ns').sum()\n",
    "    print(f\"\\nRAG significantly helps: {n_sig_help}/{len(rvd)} combos\")\n",
    "    print(f\"RAG significantly hurts: {n_sig_hurt}/{len(rvd)} combos\")\n",
    "    print(f\"No significant difference: {n_ns}/{len(rvd)} combos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RAG vs Direct effect sizes\n",
    "if not rvd.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, max(4, len(rvd) * 0.4)))\n",
    "\n",
    "    rvd_sorted = rvd.sort_values('diff')\n",
    "    y_labels = rvd_sorted['model'] + ' / ' + rvd_sorted['dataset']\n",
    "    y_pos = range(len(rvd_sorted))\n",
    "\n",
    "    colors = ['#2ecc71' if d > 0 and s != 'ns' else '#e74c3c' if d < 0 and s != 'ns' else '#95a5a6'\n",
    "              for d, s in zip(rvd_sorted['diff'], rvd_sorted['sig'])]\n",
    "\n",
    "    ax.barh(y_pos, rvd_sorted['diff'], color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax.errorbar(rvd_sorted['diff'].values, y_pos,\n",
    "                xerr=[rvd_sorted['diff'].values - rvd_sorted['ci_low'].values,\n",
    "                      rvd_sorted['ci_high'].values - rvd_sorted['diff'].values],\n",
    "                fmt='none', color='black', capsize=3, linewidth=1)\n",
    "\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(y_labels, fontsize=9)\n",
    "    ax.set_xlabel(f'Mean {PRIMARY_METRIC.upper()} Difference (RAG - Direct)')\n",
    "    ax.set_title('RAG vs Direct: Effect Size with 95% Bootstrap CI\\n'\n",
    "                 '(green = significant positive, red = significant negative, grey = not significant)')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Reranker Impact\n",
    "\n",
    "Pairwise comparisons: none vs bge vs bge-v2, controlling for model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker pairwise comparisons\n",
    "reranker_levels = sorted(rag['reranker'].dropna().unique())\n",
    "reranker_rows = []\n",
    "\n",
    "for a_level, b_level in combinations(reranker_levels, 2):\n",
    "    a_vals = rag[rag['reranker'] == a_level][PRIMARY_METRIC].dropna().values\n",
    "    b_vals = rag[rag['reranker'] == b_level][PRIMARY_METRIC].dropna().values\n",
    "\n",
    "    if len(a_vals) < 3 or len(b_vals) < 3:\n",
    "        continue\n",
    "\n",
    "    result = unpaired_bootstrap_test(a_vals, b_vals)\n",
    "    d_cohen, _, interp = effect_size(a_vals, b_vals)\n",
    "\n",
    "    reranker_rows.append({\n",
    "        'baseline': a_level,\n",
    "        'treatment': b_level,\n",
    "        'baseline_mean': np.mean(a_vals),\n",
    "        'treatment_mean': np.mean(b_vals),\n",
    "        'diff': result['observed_diff'],\n",
    "        'ci_low': result['ci_low'],\n",
    "        'ci_high': result['ci_high'],\n",
    "        'p_boot': result['p_value'],\n",
    "        'cohens_d': d_cohen,\n",
    "        'effect': interp,\n",
    "        'n_baseline': len(a_vals),\n",
    "        'n_treatment': len(b_vals),\n",
    "    })\n",
    "\n",
    "rr_df = pd.DataFrame(reranker_rows)\n",
    "if not rr_df.empty:\n",
    "    corrections = holm_bonferroni(rr_df['p_boot'].tolist())\n",
    "    rr_df['p_adjusted'] = [c['adjusted_p'] for c in corrections]\n",
    "    rr_df['sig'] = [significance_stars(c['adjusted_p']) for c in corrections]\n",
    "\n",
    "    print(\"Reranker Pairwise Comparisons\")\n",
    "    print(\"=\" * 80)\n",
    "    display(rr_df[['baseline', 'treatment', 'baseline_mean', 'treatment_mean',\n",
    "                   'diff', 'ci_low', 'ci_high', 'p_adjusted', 'sig',\n",
    "                   'cohens_d', 'effect']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Agent Type Comparisons\n",
    "\n",
    "Pairwise: fixed_rag vs iterative_rag vs self_rag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_levels = sorted(rag['agent_type'].dropna().unique())\n",
    "agent_rows = []\n",
    "\n",
    "for a_level, b_level in combinations(agent_levels, 2):\n",
    "    a_vals = rag[rag['agent_type'] == a_level][PRIMARY_METRIC].dropna().values\n",
    "    b_vals = rag[rag['agent_type'] == b_level][PRIMARY_METRIC].dropna().values\n",
    "\n",
    "    if len(a_vals) < 3 or len(b_vals) < 3:\n",
    "        continue\n",
    "\n",
    "    result = unpaired_bootstrap_test(a_vals, b_vals)\n",
    "    d_cohen, _, interp = effect_size(a_vals, b_vals)\n",
    "\n",
    "    agent_rows.append({\n",
    "        'baseline': a_level,\n",
    "        'treatment': b_level,\n",
    "        'baseline_mean': np.mean(a_vals),\n",
    "        'treatment_mean': np.mean(b_vals),\n",
    "        'diff': result['observed_diff'],\n",
    "        'ci_low': result['ci_low'],\n",
    "        'ci_high': result['ci_high'],\n",
    "        'p_boot': result['p_value'],\n",
    "        'cohens_d': d_cohen,\n",
    "        'effect': interp,\n",
    "        'n_baseline': len(a_vals),\n",
    "        'n_treatment': len(b_vals),\n",
    "    })\n",
    "\n",
    "agent_df = pd.DataFrame(agent_rows)\n",
    "if not agent_df.empty:\n",
    "    corrections = holm_bonferroni(agent_df['p_boot'].tolist())\n",
    "    agent_df['p_adjusted'] = [c['adjusted_p'] for c in corrections]\n",
    "    agent_df['sig'] = [significance_stars(c['adjusted_p']) for c in corrections]\n",
    "\n",
    "    print(\"Agent Type Pairwise Comparisons\")\n",
    "    print(\"=\" * 80)\n",
    "    display(agent_df[['baseline', 'treatment', 'baseline_mean', 'treatment_mean',\n",
    "                      'diff', 'ci_low', 'ci_high', 'p_adjusted', 'sig',\n",
    "                      'cohens_d', 'effect']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Query Transform Impact\n",
    "\n",
    "Pairwise: none vs hyde vs multiquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_levels = sorted(rag['query_transform'].dropna().unique())\n",
    "qt_rows = []\n",
    "\n",
    "for a_level, b_level in combinations(qt_levels, 2):\n",
    "    a_vals = rag[rag['query_transform'] == a_level][PRIMARY_METRIC].dropna().values\n",
    "    b_vals = rag[rag['query_transform'] == b_level][PRIMARY_METRIC].dropna().values\n",
    "\n",
    "    if len(a_vals) < 3 or len(b_vals) < 3:\n",
    "        continue\n",
    "\n",
    "    result = unpaired_bootstrap_test(a_vals, b_vals)\n",
    "    d_cohen, _, interp = effect_size(a_vals, b_vals)\n",
    "\n",
    "    qt_rows.append({\n",
    "        'baseline': a_level,\n",
    "        'treatment': b_level,\n",
    "        'baseline_mean': np.mean(a_vals),\n",
    "        'treatment_mean': np.mean(b_vals),\n",
    "        'diff': result['observed_diff'],\n",
    "        'ci_low': result['ci_low'],\n",
    "        'ci_high': result['ci_high'],\n",
    "        'p_boot': result['p_value'],\n",
    "        'cohens_d': d_cohen,\n",
    "        'effect': interp,\n",
    "        'n_baseline': len(a_vals),\n",
    "        'n_treatment': len(b_vals),\n",
    "    })\n",
    "\n",
    "qt_df = pd.DataFrame(qt_rows)\n",
    "if not qt_df.empty:\n",
    "    corrections = holm_bonferroni(qt_df['p_boot'].tolist())\n",
    "    qt_df['p_adjusted'] = [c['adjusted_p'] for c in corrections]\n",
    "    qt_df['sig'] = [significance_stars(c['adjusted_p']) for c in corrections]\n",
    "\n",
    "    print(\"Query Transform Pairwise Comparisons\")\n",
    "    print(\"=\" * 80)\n",
    "    display(qt_df[['baseline', 'treatment', 'baseline_mean', 'treatment_mean',\n",
    "                   'diff', 'ci_low', 'ci_high', 'p_adjusted', 'sig',\n",
    "                   'cohens_d', 'effect']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Model Tier Comparisons\n",
    "\n",
    "Tiny (1-2B) vs Small (3B) vs Medium (7-9B) on RAG experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_tier = rag.dropna(subset=['tier']).copy()\n",
    "tier_levels = ['tiny', 'small', 'medium']\n",
    "tier_rows = []\n",
    "\n",
    "for a_tier, b_tier in combinations(tier_levels, 2):\n",
    "    a_vals = rag_with_tier[rag_with_tier['tier'] == a_tier][PRIMARY_METRIC].dropna().values\n",
    "    b_vals = rag_with_tier[rag_with_tier['tier'] == b_tier][PRIMARY_METRIC].dropna().values\n",
    "\n",
    "    if len(a_vals) < 3 or len(b_vals) < 3:\n",
    "        continue\n",
    "\n",
    "    result = unpaired_bootstrap_test(a_vals, b_vals)\n",
    "    d_cohen, _, interp = effect_size(a_vals, b_vals)\n",
    "\n",
    "    tier_rows.append({\n",
    "        'baseline': a_tier,\n",
    "        'treatment': b_tier,\n",
    "        'baseline_mean': np.mean(a_vals),\n",
    "        'treatment_mean': np.mean(b_vals),\n",
    "        'diff': result['observed_diff'],\n",
    "        'ci_low': result['ci_low'],\n",
    "        'ci_high': result['ci_high'],\n",
    "        'p_boot': result['p_value'],\n",
    "        'cohens_d': d_cohen,\n",
    "        'effect': interp,\n",
    "        'n_baseline': len(a_vals),\n",
    "        'n_treatment': len(b_vals),\n",
    "    })\n",
    "\n",
    "tier_df = pd.DataFrame(tier_rows)\n",
    "if not tier_df.empty:\n",
    "    corrections = holm_bonferroni(tier_df['p_boot'].tolist())\n",
    "    tier_df['p_adjusted'] = [c['adjusted_p'] for c in corrections]\n",
    "    tier_df['sig'] = [significance_stars(c['adjusted_p']) for c in corrections]\n",
    "\n",
    "    print(\"Model Tier Comparisons (RAG only)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(tier_df[['baseline', 'treatment', 'baseline_mean', 'treatment_mean',\n",
    "                     'diff', 'ci_low', 'ci_high', 'p_adjusted', 'sig',\n",
    "                     'cohens_d', 'effect']].round(4))\n",
    "\n",
    "    # Per-dataset breakdown\n",
    "    print(\"\\nTier comparisons per dataset:\")\n",
    "    for dataset in sorted(rag_with_tier['dataset'].unique()):\n",
    "        ds_data = rag_with_tier[rag_with_tier['dataset'] == dataset]\n",
    "        tier_means = ds_data.groupby('tier')[PRIMARY_METRIC].agg(['mean', 'count']).round(4)\n",
    "        print(f\"\\n  {dataset}:\")\n",
    "        for tier_name in tier_levels:\n",
    "            if tier_name in tier_means.index:\n",
    "                m = tier_means.loc[tier_name]\n",
    "                print(f\"    {tier_name:<8s}: {m['mean']:.4f} (n={int(m['count'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Best RAG vs Best Direct (Per Model x Dataset)\n",
    "\n",
    "The most relevant thesis comparison: does the *best* RAG config beat the *best* direct config?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comparison_rows = []\n",
    "\n",
    "for model in sorted(df['model_short'].unique()):\n",
    "    for dataset in sorted(df['dataset'].unique()):\n",
    "        d_sub = direct[(direct['model_short'] == model) & (direct['dataset'] == dataset)]\n",
    "        r_sub = rag[(rag['model_short'] == model) & (rag['dataset'] == dataset)]\n",
    "\n",
    "        d_vals = d_sub[PRIMARY_METRIC].dropna()\n",
    "        r_vals = r_sub[PRIMARY_METRIC].dropna()\n",
    "\n",
    "        if d_vals.empty or r_vals.empty:\n",
    "            continue\n",
    "\n",
    "        best_direct = d_vals.max()\n",
    "        best_rag = r_vals.max()\n",
    "\n",
    "        # Top-5 RAG mean as a more robust estimate\n",
    "        top5_rag = r_vals.nlargest(min(5, len(r_vals))).mean()\n",
    "\n",
    "        best_comparison_rows.append({\n",
    "            'model': model,\n",
    "            'tier': MODEL_TIER.get(model, 'unknown'),\n",
    "            'dataset': dataset,\n",
    "            'best_direct': best_direct,\n",
    "            'best_rag': best_rag,\n",
    "            'top5_rag_mean': top5_rag,\n",
    "            'rag_advantage': best_rag - best_direct,\n",
    "            'rag_advantage_pct': (best_rag - best_direct) / best_direct * 100 if best_direct > 0 else np.nan,\n",
    "            'n_rag_configs': len(r_vals),\n",
    "        })\n",
    "\n",
    "best_df = pd.DataFrame(best_comparison_rows)\n",
    "if not best_df.empty:\n",
    "    print(\"Best RAG vs Best Direct (per model x dataset)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(best_df.round(4))\n",
    "\n",
    "    n_rag_wins = (best_df['rag_advantage'] > 0).sum()\n",
    "    print(f\"\\nRAG beats direct: {n_rag_wins}/{len(best_df)} combos\")\n",
    "    print(f\"Mean RAG advantage: {best_df['rag_advantage'].mean():+.4f} F1\")\n",
    "    print(f\"Mean RAG advantage: {best_df['rag_advantage_pct'].mean():+.1f}%\")\n",
    "\n",
    "    # By tier\n",
    "    print(\"\\nBy model tier:\")\n",
    "    for tier in ['tiny', 'small', 'medium']:\n",
    "        t_df = best_df[best_df['tier'] == tier]\n",
    "        if not t_df.empty:\n",
    "            wins = (t_df['rag_advantage'] > 0).sum()\n",
    "            print(f\"  {tier}: RAG wins {wins}/{len(t_df)}, \"\n",
    "                  f\"mean advantage {t_df['rag_advantage'].mean():+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Consolidated Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a consolidated summary of all comparisons\n",
    "summary_rows = []\n",
    "\n",
    "# RAG vs Direct overall\n",
    "if not rvd.empty:\n",
    "    summary_rows.append({\n",
    "        'comparison': 'RAG vs Direct (all)',\n",
    "        'mean_diff': rvd['diff'].mean(),\n",
    "        'n_significant': (rvd['sig'] != 'ns').sum(),\n",
    "        'n_total': len(rvd),\n",
    "        'median_d': rvd['cohens_d'].median(),\n",
    "        'direction': 'RAG better' if rvd['diff'].mean() > 0 else 'Direct better',\n",
    "    })\n",
    "\n",
    "# Reranker: none vs bge-v2\n",
    "if not rr_df.empty:\n",
    "    for _, row in rr_df.iterrows():\n",
    "        summary_rows.append({\n",
    "            'comparison': f'Reranker: {row[\"treatment\"]} vs {row[\"baseline\"]}',\n",
    "            'mean_diff': row['diff'],\n",
    "            'n_significant': 1 if row['sig'] != 'ns' else 0,\n",
    "            'n_total': 1,\n",
    "            'median_d': row['cohens_d'],\n",
    "            'direction': f'{row[\"treatment\"]} better' if row['diff'] > 0 else f'{row[\"baseline\"]} better',\n",
    "        })\n",
    "\n",
    "# Agent types\n",
    "if not agent_df.empty:\n",
    "    for _, row in agent_df.iterrows():\n",
    "        summary_rows.append({\n",
    "            'comparison': f'Agent: {row[\"treatment\"]} vs {row[\"baseline\"]}',\n",
    "            'mean_diff': row['diff'],\n",
    "            'n_significant': 1 if row['sig'] != 'ns' else 0,\n",
    "            'n_total': 1,\n",
    "            'median_d': row['cohens_d'],\n",
    "            'direction': f'{row[\"treatment\"]} better' if row['diff'] > 0 else f'{row[\"baseline\"]} better',\n",
    "        })\n",
    "\n",
    "# Query transform\n",
    "if not qt_df.empty:\n",
    "    for _, row in qt_df.iterrows():\n",
    "        summary_rows.append({\n",
    "            'comparison': f'QT: {row[\"treatment\"]} vs {row[\"baseline\"]}',\n",
    "            'mean_diff': row['diff'],\n",
    "            'n_significant': 1 if row['sig'] != 'ns' else 0,\n",
    "            'n_total': 1,\n",
    "            'median_d': row['cohens_d'],\n",
    "            'direction': f'{row[\"treatment\"]} better' if row['diff'] > 0 else f'{row[\"baseline\"]} better',\n",
    "        })\n",
    "\n",
    "# Model tiers\n",
    "if not tier_df.empty:\n",
    "    for _, row in tier_df.iterrows():\n",
    "        summary_rows.append({\n",
    "            'comparison': f'Tier: {row[\"treatment\"]} vs {row[\"baseline\"]}',\n",
    "            'mean_diff': row['diff'],\n",
    "            'n_significant': 1 if row['sig'] != 'ns' else 0,\n",
    "            'n_total': 1,\n",
    "            'median_d': row['cohens_d'],\n",
    "            'direction': f'{row[\"treatment\"]} better' if row['diff'] > 0 else f'{row[\"baseline\"]} better',\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "if not summary_df.empty:\n",
    "    print(\"Consolidated Significance Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    display(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. LaTeX Tables\n",
    "\n",
    "Publication-ready tables for direct inclusion in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(df, columns, col_names=None, caption=\"\", label=\"\",\n",
    "                fmt=None, bold_col=None, bold_fn=None):\n",
    "    \"\"\"Convert a DataFrame to a LaTeX table string.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to convert.\n",
    "        columns: List of column names to include.\n",
    "        col_names: Display names for columns (same length as columns).\n",
    "        caption: LaTeX table caption.\n",
    "        label: LaTeX table label.\n",
    "        fmt: Dict mapping column names to format strings.\n",
    "        bold_col: Column to apply bold formatting to.\n",
    "        bold_fn: Function(value) -> bool; if True, bold that cell.\n",
    "    \"\"\"\n",
    "    if col_names is None:\n",
    "        col_names = columns\n",
    "    if fmt is None:\n",
    "        fmt = {}\n",
    "\n",
    "    n_cols = len(columns)\n",
    "    col_spec = 'l' + 'r' * (n_cols - 1)\n",
    "\n",
    "    lines = [\n",
    "        r'\\begin{table}[htbp]',\n",
    "        r'\\centering',\n",
    "        f'\\\\caption{{{caption}}}',\n",
    "        f'\\\\label{{{label}}}',\n",
    "        f'\\\\begin{{tabular}}{{{col_spec}}}',\n",
    "        r'\\toprule',\n",
    "        ' & '.join(f'\\\\textbf{{{n}}}' for n in col_names) + r' \\\\',\n",
    "        r'\\midrule',\n",
    "    ]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cells = []\n",
    "        for col in columns:\n",
    "            val = row[col]\n",
    "            if col in fmt and isinstance(val, (int, float)) and not pd.isna(val):\n",
    "                cell = fmt[col].format(val)\n",
    "            else:\n",
    "                cell = str(val)\n",
    "            # Escape underscores for LaTeX\n",
    "            cell = cell.replace('_', r'\\_')\n",
    "            if bold_col and col == bold_col and bold_fn and bold_fn(val):\n",
    "                cell = f'\\\\textbf{{{cell}}}'\n",
    "            cells.append(cell)\n",
    "        lines.append(' & '.join(cells) + r' \\\\')\n",
    "\n",
    "    lines.extend([\n",
    "        r'\\bottomrule',\n",
    "        r'\\end{tabular}',\n",
    "        r'\\end{table}',\n",
    "    ])\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Table 1: RAG vs Direct per Model x Dataset\n",
    "if not rvd.empty:\n",
    "    latex1 = df_to_latex(\n",
    "        rvd.sort_values(['tier', 'model', 'dataset']),\n",
    "        columns=['model', 'dataset', 'direct_mean', 'rag_mean', 'diff', 'p_adjusted', 'sig', 'effect'],\n",
    "        col_names=['Model', 'Dataset', 'Direct F1', 'RAG F1', '$\\\\Delta$', '$p_{adj}$', 'Sig.', 'Effect'],\n",
    "        caption='RAG vs Direct LLM performance per model and dataset. '\n",
    "                'P-values corrected with Holm-Bonferroni. Effect size: Cohen\\'s d.',\n",
    "        label='tab:rag-vs-direct',\n",
    "        fmt={'direct_mean': '{:.3f}', 'rag_mean': '{:.3f}', 'diff': '{:+.3f}',\n",
    "             'p_adjusted': '{:.4f}'},\n",
    "        bold_col='sig',\n",
    "        bold_fn=lambda v: v != 'ns',\n",
    "    )\n",
    "    print(\"% === Table 1: RAG vs Direct ===\")\n",
    "    print(latex1)\n",
    "    print()\n",
    "\n",
    "# Table 2: Component Impact Summary\n",
    "component_rows = []\n",
    "for label, comp_df in [('Reranker', rr_df), ('Agent Type', agent_df),\n",
    "                       ('Query Transform', qt_df), ('Model Tier', tier_df)]:\n",
    "    if comp_df is not None and not comp_df.empty:\n",
    "        for _, row in comp_df.iterrows():\n",
    "            component_rows.append({\n",
    "                'component': label,\n",
    "                'comparison': f\"{row['treatment']} vs {row['baseline']}\",\n",
    "                'diff': row['diff'],\n",
    "                'ci': f\"[{row['ci_low']:+.3f}, {row['ci_high']:+.3f}]\",\n",
    "                'p_adjusted': row['p_adjusted'],\n",
    "                'sig': row['sig'],\n",
    "                'cohens_d': row['cohens_d'],\n",
    "                'effect': row['effect'],\n",
    "            })\n",
    "\n",
    "comp_summary = pd.DataFrame(component_rows)\n",
    "if not comp_summary.empty:\n",
    "    latex2 = df_to_latex(\n",
    "        comp_summary,\n",
    "        columns=['component', 'comparison', 'diff', 'ci', 'p_adjusted', 'sig', 'effect'],\n",
    "        col_names=['Component', 'Comparison', '$\\\\Delta$ F1', '95\\\\% CI',\n",
    "                   '$p_{adj}$', 'Sig.', 'Effect'],\n",
    "        caption='Pairwise component comparisons with bootstrap significance tests. '\n",
    "                'All p-values Holm-Bonferroni corrected within each component group.',\n",
    "        label='tab:component-significance',\n",
    "        fmt={'diff': '{:+.3f}', 'p_adjusted': '{:.4f}', 'cohens_d': '{:.2f}'},\n",
    "        bold_col='sig',\n",
    "        bold_fn=lambda v: v != 'ns',\n",
    "    )\n",
    "    print(\"% === Table 2: Component Significance ===\")\n",
    "    print(latex2)\n",
    "    print()\n",
    "\n",
    "# Table 3: Best RAG vs Best Direct\n",
    "if not best_df.empty:\n",
    "    latex3 = df_to_latex(\n",
    "        best_df.sort_values(['tier', 'model', 'dataset']),\n",
    "        columns=['model', 'tier', 'dataset', 'best_direct', 'best_rag',\n",
    "                 'rag_advantage', 'rag_advantage_pct'],\n",
    "        col_names=['Model', 'Tier', 'Dataset', 'Best Direct', 'Best RAG',\n",
    "                   '$\\\\Delta$ F1', '$\\\\Delta$ \\\\%'],\n",
    "        caption='Best RAG configuration vs best direct LLM per model and dataset.',\n",
    "        label='tab:best-rag-vs-direct',\n",
    "        fmt={'best_direct': '{:.3f}', 'best_rag': '{:.3f}',\n",
    "             'rag_advantage': '{:+.3f}', 'rag_advantage_pct': '{:+.1f}'},\n",
    "        bold_col='rag_advantage',\n",
    "        bold_fn=lambda v: isinstance(v, (int, float)) and v > 0,\n",
    "    )\n",
    "    print(\"% === Table 3: Best RAG vs Best Direct ===\")\n",
    "    print(latex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Key findings from statistical tests:\n",
    "- Whether RAG significantly outperforms direct LLM (per model tier and dataset)\n",
    "- Which components have statistically significant effects (reranker, agent, query transform)\n",
    "- Effect sizes (Cohen's d) to quantify practical significance\n",
    "- LaTeX tables ready for thesis inclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}