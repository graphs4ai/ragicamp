{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: Study Overview\n",
    "\n",
    "**Question:** What data do we have? What's broken? What's the rough performance landscape?\n",
    "\n",
    "This notebook provides a high-level inventory of the `smart_retrieval_slm` study:\n",
    "- Experiment health and completeness\n",
    "- Coverage across models, datasets, agent types, and RAG components\n",
    "- Overall performance landscape\n",
    "- Metric correlation analysis (justifying F1 as primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as scipy_stats\n\nfrom analysis_utils import (\n    load_all_results, setup_plotting, get_experiment_health_summary,\n    print_search_space_summary, weighted_mean_with_ci,\n    load_failed_experiments, analyze_failure_patterns,\n    predict_context_length_issues,\n    METRICS, PRIMARY_METRIC, MODEL_PARAMS, MODEL_TIER, BROKEN_MODELS,\n)\n\nsetup_plotting()\nSTUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n\ndf = load_all_results(STUDY_PATH)\nprint(f\"Loaded {len(df)} experiments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Study Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = get_experiment_health_summary(STUDY_PATH)\n",
    "print(\"Study Health\")\n",
    "print(\"=\" * 50)\n",
    "for k, v in health.items():\n",
    "    print(f\"  {k:<22s}: {v}\")\n",
    "\n",
    "# Check for missing metrics\n",
    "print(\"\\nMetric availability:\")\n",
    "for m in METRICS:\n",
    "    if m in df.columns:\n",
    "        n_valid = df[m].notna().sum()\n",
    "        print(f\"  {m:<15s}: {n_valid:>4d}/{len(df)} ({n_valid/len(df)*100:.0f}%)\")\n",
    "\n",
    "# Flag broken models\n",
    "print(\"\\nBroken models (near-zero F1):\")\n",
    "for model in sorted(BROKEN_MODELS):\n",
    "    subset = df[df['model_short'] == model]\n",
    "    if len(subset) > 0 and PRIMARY_METRIC in subset.columns:\n",
    "        mean_f1 = subset[PRIMARY_METRIC].mean()\n",
    "        print(f\"  {model}: mean F1 = {mean_f1:.4f} ({len(subset)} experiments)\")\n",
    "    else:\n",
    "        print(f\"  {model}: no experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. Experiment Failure Analysis\n\nWhich configurations crash? Are failures random or systematic?\nUnderstanding failure patterns avoids wasting compute on doomed configs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load failed experiments\nfailed_df = load_failed_experiments(STUDY_PATH)\nn_failed = len(failed_df)\nn_total = len(df) + n_failed\nfail_rate = n_failed / n_total * 100 if n_total > 0 else 0\n\nprint(f\"Experiment Failure Analysis\")\nprint(\"=\" * 60)\nprint(f\"  Completed:  {len(df):>4d}\")\nprint(f\"  Failed:     {n_failed:>4d}\")\nprint(f\"  Total:      {n_total:>4d}\")\nprint(f\"  Fail rate:  {fail_rate:.1f}%\")\n\nif not failed_df.empty:\n    patterns = analyze_failure_patterns(failed_df)\n\n    # Error category breakdown\n    print(f\"\\nError Categories:\")\n    if 'by_error_type' in patterns:\n        for cat, count in patterns['by_error_type'].items():\n            pct = count / n_failed * 100\n            bar = '#' * int(pct / 2)\n            print(f\"  {cat:<25s}: {count:>3d} ({pct:.0f}%) {bar}\")\n\n    # Failure rate by model\n    print(f\"\\nFailure Rate by Model:\")\n    if 'by_model' in patterns:\n        model_counts = patterns['by_model']\n        for model, fail_count in sorted(model_counts.items(), key=lambda x: -x[1]):\n            total_model = len(df[df['model_short'] == model]) + fail_count\n            rate = fail_count / total_model * 100 if total_model > 0 else 0\n            print(f\"  {model:<20s}: {fail_count:>3d} failed / {total_model:>3d} total ({rate:.0f}%)\")\n\n    # Failure rate by retriever\n    print(f\"\\nFailure Rate by Retriever:\")\n    if 'by_retriever' in patterns:\n        retr_counts = patterns['by_retriever']\n        for retr, fail_count in sorted(retr_counts.items(), key=lambda x: -x[1]):\n            total_retr = len(df[df['retriever_type'] == retr]) + fail_count\n            rate = fail_count / total_retr * 100 if total_retr > 0 else 0\n            print(f\"  {retr:<20s}: {fail_count:>3d} failed / {total_retr:>3d} total ({rate:.0f}%)\")\n\n    # Visualization: failure heatmap (model x retriever)\n    if 'model_short' in failed_df.columns and 'retriever_type' in failed_df.columns:\n        fail_ct = pd.crosstab(failed_df['model_short'], failed_df['retriever_type'])\n        if fail_ct.size > 1:\n            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n            sns.heatmap(fail_ct, annot=True, fmt='d', cmap='OrRd', ax=axes[0],\n                        linewidths=0.5)\n            axes[0].set_title('Failure Count: Model x Retriever')\n\n            # Failure RATE heatmap (failures / total attempted)\n            # Build total attempted from successes + failures\n            success_ct = pd.crosstab(df['model_short'], df['retriever_type'])\n            total_ct = success_ct.add(fail_ct, fill_value=0)\n            rate_ct = fail_ct.div(total_ct).fillna(0) * 100\n            sns.heatmap(rate_ct, annot=True, fmt='.0f', cmap='OrRd', ax=axes[1],\n                        linewidths=0.5, vmin=0, vmax=100)\n            axes[1].set_title('Failure Rate (%): Model x Retriever')\n\n            plt.tight_layout()\n            plt.show()\n\n    # Context length risk prediction\n    risky = predict_context_length_issues(STUDY_PATH)\n    if isinstance(risky, pd.DataFrame) and not risky.empty:\n        print(f\"\\nContext-Length Risk Predictions:\")\n        print(f\"  {len(risky)} configurations flagged as high-risk\")\n        display(risky.head(10))\n    else:\n        print(f\"\\nContext-Length Risk: No high-risk configurations detected.\")\n\n    # Top-K and failure: do higher top_k values cause more failures?\n    if 'top_k' in failed_df.columns and failed_df['top_k'].notna().sum() > 0:\n        print(f\"\\nFailure Rate by Top-K:\")\n        for tk in sorted(failed_df['top_k'].dropna().unique()):\n            tk_fail = len(failed_df[failed_df['top_k'] == tk])\n            tk_success = len(df[df['top_k'] == tk]) if 'top_k' in df.columns else 0\n            tk_total = tk_fail + tk_success\n            rate = tk_fail / tk_total * 100 if tk_total > 0 else 0\n            print(f\"  top_k={int(tk):<4d}: {tk_fail:>3d} failed / {tk_total:>3d} total ({rate:.0f}%)\")\nelse:\n    print(\"\\nNo failed experiments found — all experiments completed successfully.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_search_space_summary(df)\n",
    "\n",
    "# Model x Dataset crosstab heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Model x Dataset\n",
    "ct_model_ds = pd.crosstab(df['model_short'], df['dataset'])\n",
    "sns.heatmap(ct_model_ds, annot=True, fmt='d', cmap='YlGn', ax=axes[0])\n",
    "axes[0].set_title('Experiments: Model x Dataset')\n",
    "\n",
    "# Agent type x Dataset\n",
    "ct_agent_ds = pd.crosstab(df['agent_type'], df['dataset'])\n",
    "sns.heatmap(ct_agent_ds, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1])\n",
    "axes[1].set_title('Experiments: Agent Type x Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out broken models for performance analysis\n",
    "df_clean = df[~df['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "print(f\"After filtering broken models: {len(df_clean)} experiments\")\n",
    "\n",
    "# Model leaderboard\n",
    "leaderboard = weighted_mean_with_ci(df_clean, 'model_short', PRIMARY_METRIC)\n",
    "print(\"\\nModel Leaderboard (F1):\")\n",
    "display(leaderboard)\n",
    "\n",
    "# Bar chart with CI\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = range(len(leaderboard))\n",
    "yerr_low = leaderboard['mean'] - leaderboard['ci_low']\n",
    "yerr_high = leaderboard['ci_high'] - leaderboard['mean']\n",
    "colors = [sns.color_palette('husl', len(leaderboard))[i] for i in range(len(leaderboard))]\n",
    "ax.bar(x, leaderboard['mean'], yerr=[yerr_low, yerr_high], capsize=5,\n",
    "       color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(leaderboard['model_short'], rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean F1')\n",
    "ax.set_title('Model Leaderboard — Mean F1 with 95% CI')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset violin plots of F1 distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "datasets = sorted(df_clean['dataset'].unique())\n",
    "data_for_violin = [df_clean[df_clean['dataset'] == ds][PRIMARY_METRIC].dropna().values\n",
    "                   for ds in datasets]\n",
    "parts = ax.violinplot(data_for_violin, showmeans=True, showmedians=True)\n",
    "ax.set_xticks(range(1, len(datasets) + 1))\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('F1 Distribution by Dataset')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metric Correlation\n",
    "\n",
    "Justification for using F1 as the primary metric: if F1, exact_match, bertscore, and bleurt are highly correlated, conclusions drawn from F1 generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spearman correlations across all available metrics\n",
    "available = [m for m in METRICS if m in df_clean.columns and df_clean[m].notna().sum() >= 10]\n",
    "print(f\"Metrics with sufficient data: {available}\")\n",
    "\n",
    "if len(available) >= 2:\n",
    "    metric_df = df_clean[available].dropna()\n",
    "    corr_matrix = metric_df.corr(method='spearman')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                vmin=-1, vmax=1, mask=mask, ax=ax, square=True)\n",
    "    ax.set_title('Spearman Correlation Between Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print pairwise correlations with F1\n",
    "    print(\"\\nCorrelation with F1:\")\n",
    "    for m in available:\n",
    "        if m != PRIMARY_METRIC:\n",
    "            rho, pval = scipy_stats.spearmanr(\n",
    "                metric_df[PRIMARY_METRIC], metric_df[m]\n",
    "            )\n",
    "            print(f\"  {m:<15s}: rho={rho:.3f}, p={pval:.2e}\")\n",
    "else:\n",
    "    print(\"Not enough metrics with data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Key takeaways from this overview — fill in after running on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STUDY OVERVIEW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments (loaded):   {len(df)}\")\n",
    "print(f\"After broken-model filter:    {len(df_clean)}\")\n",
    "print(f\"Models:                       {sorted(df['model_short'].unique())}\")\n",
    "print(f\"Datasets:                     {sorted(df['dataset'].unique())}\")\n",
    "print(f\"Agent types:                  {sorted(df['agent_type'].unique())}\")\n",
    "print(f\"Experiment types:             {df['exp_type'].value_counts().to_dict()}\")\n",
    "if PRIMARY_METRIC in df_clean.columns:\n",
    "    print(f\"\\nOverall F1 (clean):  mean={df_clean[PRIMARY_METRIC].mean():.4f}, \"\n",
    "          f\"std={df_clean[PRIMARY_METRIC].std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}