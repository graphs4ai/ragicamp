{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: Study Overview\n",
    "\n",
    "**Question:** What data do we have? What's broken? What's the rough performance landscape?\n",
    "\n",
    "This notebook provides a high-level inventory of the `smart_retrieval_slm` study:\n",
    "- Experiment health and completeness\n",
    "- Coverage across models, datasets, agent types, and RAG components\n",
    "- Overall performance landscape\n",
    "- Metric correlation analysis (justifying F1 as primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, get_experiment_health_summary,\n",
    "    print_search_space_summary, weighted_mean_with_ci,\n",
    "    METRICS, PRIMARY_METRIC, MODEL_PARAMS, MODEL_TIER, BROKEN_MODELS,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "df = load_all_results(STUDY_PATH)\n",
    "print(f\"Loaded {len(df)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Study Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = get_experiment_health_summary(STUDY_PATH)\n",
    "print(\"Study Health\")\n",
    "print(\"=\" * 50)\n",
    "for k, v in health.items():\n",
    "    print(f\"  {k:<22s}: {v}\")\n",
    "\n",
    "# Check for missing metrics\n",
    "print(\"\\nMetric availability:\")\n",
    "for m in METRICS:\n",
    "    if m in df.columns:\n",
    "        n_valid = df[m].notna().sum()\n",
    "        print(f\"  {m:<15s}: {n_valid:>4d}/{len(df)} ({n_valid/len(df)*100:.0f}%)\")\n",
    "\n",
    "# Flag broken models\n",
    "print(\"\\nBroken models (near-zero F1):\")\n",
    "for model in sorted(BROKEN_MODELS):\n",
    "    subset = df[df['model_short'] == model]\n",
    "    if len(subset) > 0 and PRIMARY_METRIC in subset.columns:\n",
    "        mean_f1 = subset[PRIMARY_METRIC].mean()\n",
    "        print(f\"  {model}: mean F1 = {mean_f1:.4f} ({len(subset)} experiments)\")\n",
    "    else:\n",
    "        print(f\"  {model}: no experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_search_space_summary(df)\n",
    "\n",
    "# Model x Dataset crosstab heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Model x Dataset\n",
    "ct_model_ds = pd.crosstab(df['model_short'], df['dataset'])\n",
    "sns.heatmap(ct_model_ds, annot=True, fmt='d', cmap='YlGn', ax=axes[0])\n",
    "axes[0].set_title('Experiments: Model x Dataset')\n",
    "\n",
    "# Agent type x Dataset\n",
    "ct_agent_ds = pd.crosstab(df['agent_type'], df['dataset'])\n",
    "sns.heatmap(ct_agent_ds, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1])\n",
    "axes[1].set_title('Experiments: Agent Type x Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out broken models for performance analysis\n",
    "df_clean = df[~df['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "print(f\"After filtering broken models: {len(df_clean)} experiments\")\n",
    "\n",
    "# Model leaderboard\n",
    "leaderboard = weighted_mean_with_ci(df_clean, 'model_short', PRIMARY_METRIC)\n",
    "print(\"\\nModel Leaderboard (F1):\")\n",
    "display(leaderboard)\n",
    "\n",
    "# Bar chart with CI\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = range(len(leaderboard))\n",
    "yerr_low = leaderboard['mean'] - leaderboard['ci_low']\n",
    "yerr_high = leaderboard['ci_high'] - leaderboard['mean']\n",
    "colors = [sns.color_palette('husl', len(leaderboard))[i] for i in range(len(leaderboard))]\n",
    "ax.bar(x, leaderboard['mean'], yerr=[yerr_low, yerr_high], capsize=5,\n",
    "       color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(leaderboard['model_short'], rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean F1')\n",
    "ax.set_title('Model Leaderboard — Mean F1 with 95% CI')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset violin plots of F1 distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "datasets = sorted(df_clean['dataset'].unique())\n",
    "data_for_violin = [df_clean[df_clean['dataset'] == ds][PRIMARY_METRIC].dropna().values\n",
    "                   for ds in datasets]\n",
    "parts = ax.violinplot(data_for_violin, showmeans=True, showmedians=True)\n",
    "ax.set_xticks(range(1, len(datasets) + 1))\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.set_ylabel('F1')\n",
    "ax.set_title('F1 Distribution by Dataset')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metric Correlation\n",
    "\n",
    "Justification for using F1 as the primary metric: if F1, exact_match, bertscore, and bleurt are highly correlated, conclusions drawn from F1 generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spearman correlations across all available metrics\n",
    "available = [m for m in METRICS if m in df_clean.columns and df_clean[m].notna().sum() >= 10]\n",
    "print(f\"Metrics with sufficient data: {available}\")\n",
    "\n",
    "if len(available) >= 2:\n",
    "    metric_df = df_clean[available].dropna()\n",
    "    corr_matrix = metric_df.corr(method='spearman')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                vmin=-1, vmax=1, mask=mask, ax=ax, square=True)\n",
    "    ax.set_title('Spearman Correlation Between Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print pairwise correlations with F1\n",
    "    print(\"\\nCorrelation with F1:\")\n",
    "    for m in available:\n",
    "        if m != PRIMARY_METRIC:\n",
    "            rho, pval = scipy_stats.spearmanr(\n",
    "                metric_df[PRIMARY_METRIC], metric_df[m]\n",
    "            )\n",
    "            print(f\"  {m:<15s}: rho={rho:.3f}, p={pval:.2e}\")\n",
    "else:\n",
    "    print(\"Not enough metrics with data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Key takeaways from this overview — fill in after running on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STUDY OVERVIEW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments (loaded):   {len(df)}\")\n",
    "print(f\"After broken-model filter:    {len(df_clean)}\")\n",
    "print(f\"Models:                       {sorted(df['model_short'].unique())}\")\n",
    "print(f\"Datasets:                     {sorted(df['dataset'].unique())}\")\n",
    "print(f\"Agent types:                  {sorted(df['agent_type'].unique())}\")\n",
    "print(f\"Experiment types:             {df['exp_type'].value_counts().to_dict()}\")\n",
    "if PRIMARY_METRIC in df_clean.columns:\n",
    "    print(f\"\\nOverall F1 (clean):  mean={df_clean[PRIMARY_METRIC].mean():.4f}, \"\n",
    "          f\"std={df_clean[PRIMARY_METRIC].std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
