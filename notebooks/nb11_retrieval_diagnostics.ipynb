{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB11: Retrieval Quality Diagnostics\n",
    "\n",
    "**Question:** Is performance bottlenecked by retrieval or generation?\n",
    "\n",
    "This notebook diagnoses retrieval quality using saved predictions:\n",
    "1. **Answer-in-context rates** by retriever, model, dataset\n",
    "2. **Retrieval vs Generation bottleneck** quadrant analysis\n",
    "3. **Reranking effectiveness** (rank changes, score distributions)\n",
    "4. **Correlation** between retrieval quality and F1\n",
    "5. **Per-retriever diagnostic** deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, parse_experiment_name,\n",
    "    _enrich_from_metadata, SKIP_DIRS,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "# Load experiment-level results for filtering\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "rag_df = df[df['exp_type'] == 'rag'].copy()\n",
    "print(f\"RAG experiments to analyze: {len(rag_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Retrieval analysis functions (adapted from scripts/analyze_retrieval.py) ----\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for fuzzy matching.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def answer_in_text(expected, text: str) -> bool:\n",
    "    \"\"\"Check if any expected answer appears in text.\"\"\"\n",
    "    if not text or not expected:\n",
    "        return False\n",
    "    text_norm = normalize_text(text)\n",
    "    answers = expected if isinstance(expected, list) else [expected]\n",
    "    for ans in answers:\n",
    "        ans_norm = normalize_text(str(ans))\n",
    "        if len(ans_norm) >= 2 and ans_norm in text_norm:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_correct(prediction: str, expected: list, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"Check if prediction matches any expected answer (fuzzy).\"\"\"\n",
    "    if not prediction or not expected:\n",
    "        return False\n",
    "    pred_norm = normalize_text(prediction)\n",
    "    for exp in expected:\n",
    "        exp_norm = normalize_text(str(exp))\n",
    "        if not exp_norm:\n",
    "            continue\n",
    "        if exp_norm in pred_norm or pred_norm in exp_norm:\n",
    "            return True\n",
    "        pred_words = set(pred_norm.split())\n",
    "        exp_words = set(exp_norm.split())\n",
    "        if exp_words and pred_words:\n",
    "            overlap = len(pred_words & exp_words) / len(exp_words)\n",
    "            if overlap >= threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"Analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load prediction-level retrieval data for all RAG experiments ----\n",
    "\n",
    "rag_experiment_names = set(rag_df['name'].values)\n",
    "retrieval_rows = []\n",
    "doc_rows = []  # Per-document data for reranking analysis\n",
    "\n",
    "for exp_dir in STUDY_PATH.iterdir():\n",
    "    if not exp_dir.is_dir() or exp_dir.name in SKIP_DIRS:\n",
    "        continue\n",
    "    if exp_dir.name.startswith('.') or exp_dir.name.startswith('_'):\n",
    "        continue\n",
    "\n",
    "    pred_file = exp_dir / \"predictions.json\"\n",
    "    meta_file = exp_dir / \"metadata.json\"\n",
    "    if not pred_file.exists():\n",
    "        continue\n",
    "\n",
    "    # Parse experiment config\n",
    "    config = parse_experiment_name(exp_dir.name)\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as f:\n",
    "            _enrich_from_metadata(config, json.load(f))\n",
    "\n",
    "    if config.get('exp_type') == 'direct':\n",
    "        continue\n",
    "    if config.get('model_short') in BROKEN_MODELS:\n",
    "        continue\n",
    "\n",
    "    with open(pred_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    preds = data.get('predictions', [])\n",
    "    if not preds:\n",
    "        continue\n",
    "\n",
    "    for p in preds:\n",
    "        expected = p.get('expected', [])\n",
    "        prediction = p.get('prediction', '')\n",
    "        prompt = p.get('prompt', '')\n",
    "        retrieved_docs = p.get('retrieved_docs', [])\n",
    "        metrics = p.get('metrics', {})\n",
    "\n",
    "        if not expected:\n",
    "            continue\n",
    "\n",
    "        # Check answer in context (using prompt text)\n",
    "        has_answer_in_prompt = answer_in_text(expected, prompt)\n",
    "\n",
    "        # Check answer in individual documents\n",
    "        answer_in_any_doc = False\n",
    "        answer_doc_rank = None\n",
    "        for doc in retrieved_docs:\n",
    "            content = doc.get('content', '')\n",
    "            if answer_in_text(expected, content):\n",
    "                answer_in_any_doc = True\n",
    "                rank = doc.get('rank', 999)\n",
    "                if answer_doc_rank is None or rank < answer_doc_rank:\n",
    "                    answer_doc_rank = rank\n",
    "\n",
    "        got_correct = is_correct(prediction, expected)\n",
    "        f1_score = metrics.get('f1', np.nan)\n",
    "\n",
    "        retrieval_rows.append({\n",
    "            'experiment': exp_dir.name,\n",
    "            'idx': p.get('idx'),\n",
    "            'model_short': config.get('model_short'),\n",
    "            'dataset': config.get('dataset'),\n",
    "            'retriever': config.get('retriever'),\n",
    "            'retriever_type': config.get('retriever_type'),\n",
    "            'reranker': config.get('reranker', 'none'),\n",
    "            'agent_type': config.get('agent_type'),\n",
    "            'top_k': config.get('top_k'),\n",
    "            'query_transform': config.get('query_transform', 'none'),\n",
    "            'has_answer_in_prompt': has_answer_in_prompt,\n",
    "            'answer_in_any_doc': answer_in_any_doc,\n",
    "            'answer_doc_rank': answer_doc_rank,\n",
    "            'got_correct': got_correct,\n",
    "            'f1': f1_score,\n",
    "            'n_docs': len(retrieved_docs),\n",
    "        })\n",
    "\n",
    "        # Per-document data for reranking analysis\n",
    "        for doc in retrieved_docs:\n",
    "            doc_rows.append({\n",
    "                'experiment': exp_dir.name,\n",
    "                'idx': p.get('idx'),\n",
    "                'reranker': config.get('reranker', 'none'),\n",
    "                'rank': doc.get('rank'),\n",
    "                'score': doc.get('score'),\n",
    "                'retrieval_score': doc.get('retrieval_score'),\n",
    "                'retrieval_rank': doc.get('retrieval_rank'),\n",
    "                'rerank_score': doc.get('rerank_score'),\n",
    "                'has_answer': answer_in_text(expected, doc.get('content', '')),\n",
    "            })\n",
    "\n",
    "ret_df = pd.DataFrame(retrieval_rows)\n",
    "doc_df = pd.DataFrame(doc_rows) if doc_rows else pd.DataFrame()\n",
    "\n",
    "print(f\"Loaded {len(ret_df)} question-level retrieval records from {ret_df['experiment'].nunique()} experiments\")\n",
    "if not doc_df.empty:\n",
    "    print(f\"Loaded {len(doc_df)} document-level records\")\n",
    "    print(f\"Documents with rerank_score: {doc_df['rerank_score'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Answer-in-Context Rates\n",
    "\n",
    "What fraction of questions have the answer in the retrieved context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ret_df.empty:\n",
    "    # Overall retrieval recall\n",
    "    overall_recall = ret_df['has_answer_in_prompt'].mean()\n",
    "    print(f\"Overall answer-in-context rate: {overall_recall:.1%}\")\n",
    "    print()\n",
    "\n",
    "    # By retriever\n",
    "    if 'retriever_type' in ret_df.columns:\n",
    "        by_retriever = ret_df.groupby('retriever_type').agg(\n",
    "            recall=('has_answer_in_prompt', 'mean'),\n",
    "            n=('has_answer_in_prompt', 'count'),\n",
    "        ).sort_values('recall', ascending=False)\n",
    "        print(\"By retriever type:\")\n",
    "        display(by_retriever.round(3))\n",
    "\n",
    "    # By dataset\n",
    "    by_dataset = ret_df.groupby('dataset').agg(\n",
    "        recall=('has_answer_in_prompt', 'mean'),\n",
    "        n=('has_answer_in_prompt', 'count'),\n",
    "    ).sort_values('recall', ascending=False)\n",
    "    print(\"\\nBy dataset:\")\n",
    "    display(by_dataset.round(3))\n",
    "\n",
    "    # By reranker\n",
    "    by_reranker = ret_df.groupby('reranker').agg(\n",
    "        recall=('has_answer_in_prompt', 'mean'),\n",
    "        n=('has_answer_in_prompt', 'count'),\n",
    "    ).sort_values('recall', ascending=False)\n",
    "    print(\"\\nBy reranker:\")\n",
    "    display(by_reranker.round(3))\n",
    "\n",
    "    # Heatmap: retriever_type x dataset\n",
    "    pivot = ret_df.groupby(['retriever_type', 'dataset'])['has_answer_in_prompt'].mean().unstack()\n",
    "    if not pivot.empty and pivot.shape[0] > 1:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        sns.heatmap(pivot, annot=True, fmt='.1%', cmap='RdYlGn', vmin=0, vmax=1, ax=ax)\n",
    "        ax.set_title('Answer-in-Context Rate by Retriever Type x Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Retrieval vs Generation Bottleneck\n",
    "\n",
    "Quadrant analysis:\n",
    "- **Correct + Context**: Retrieval and generation both succeeded\n",
    "- **Wrong + Context**: Had the answer but generation failed\n",
    "- **Correct + No Context**: Got lucky (or parametric knowledge)\n",
    "- **Wrong + No Context**: Retrieval failure (answer not retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ret_df.empty:\n",
    "    # Classify into quadrants\n",
    "    ret_df['quadrant'] = 'unknown'\n",
    "    mask_ctx = ret_df['has_answer_in_prompt']\n",
    "    mask_correct = ret_df['got_correct']\n",
    "\n",
    "    ret_df.loc[mask_ctx & mask_correct, 'quadrant'] = 'Correct + Context'\n",
    "    ret_df.loc[mask_ctx & ~mask_correct, 'quadrant'] = 'Wrong + Context'\n",
    "    ret_df.loc[~mask_ctx & mask_correct, 'quadrant'] = 'Correct + No Context'\n",
    "    ret_df.loc[~mask_ctx & ~mask_correct, 'quadrant'] = 'Wrong + No Context'\n",
    "\n",
    "    # Overall distribution\n",
    "    quadrant_counts = ret_df['quadrant'].value_counts()\n",
    "    quadrant_pcts = ret_df['quadrant'].value_counts(normalize=True)\n",
    "    print(\"Overall Quadrant Distribution:\")\n",
    "    for q in ['Correct + Context', 'Wrong + Context', 'Correct + No Context', 'Wrong + No Context']:\n",
    "        if q in quadrant_counts.index:\n",
    "            print(f\"  {q:<25s}: {quadrant_counts[q]:>7,} ({quadrant_pcts[q]:.1%})\")\n",
    "\n",
    "    # Bottleneck identification\n",
    "    retrieval_recall = mask_ctx.mean()\n",
    "    gen_given_ctx = ret_df.loc[mask_ctx, 'got_correct'].mean() if mask_ctx.sum() > 0 else 0\n",
    "    print(f\"\\nRetrieval Recall: {retrieval_recall:.1%}\")\n",
    "    print(f\"Generation|Context: {gen_given_ctx:.1%}\")\n",
    "    if retrieval_recall < 0.5:\n",
    "        print(\"Bottleneck: RETRIEVAL\")\n",
    "    elif gen_given_ctx < 0.5:\n",
    "        print(\"Bottleneck: GENERATION\")\n",
    "    else:\n",
    "        print(\"Bottleneck: BALANCED\")\n",
    "\n",
    "    # Per-model breakdown\n",
    "    print(\"\\nPer-model bottleneck:\")\n",
    "    for model in sorted(ret_df['model_short'].unique()):\n",
    "        m_df = ret_df[ret_df['model_short'] == model]\n",
    "        m_ctx = m_df['has_answer_in_prompt']\n",
    "        m_recall = m_ctx.mean()\n",
    "        m_gen = m_df.loc[m_ctx, 'got_correct'].mean() if m_ctx.sum() > 0 else 0\n",
    "        bottleneck = 'RETRIEVAL' if m_recall < 0.5 else 'GENERATION' if m_gen < 0.5 else 'BALANCED'\n",
    "        print(f\"  {model:<16s}: recall={m_recall:.1%}, gen|ctx={m_gen:.1%}  -> {bottleneck}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrant breakdown by dataset and retriever\n",
    "if not ret_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # By dataset\n",
    "    ds_quad = ret_df.groupby('dataset')['quadrant'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    quad_order = ['Correct + Context', 'Correct + No Context', 'Wrong + Context', 'Wrong + No Context']\n",
    "    quad_order = [q for q in quad_order if q in ds_quad.columns]\n",
    "    colors = ['#2ecc71', '#27ae60', '#e67e22', '#e74c3c']\n",
    "    ds_quad[quad_order].plot(kind='bar', stacked=True, ax=axes[0],\n",
    "                             color=colors[:len(quad_order)])\n",
    "    axes[0].set_title('Outcome Quadrants by Dataset')\n",
    "    axes[0].set_ylabel('Proportion')\n",
    "    axes[0].legend(fontsize=8, loc='upper right')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    # By retriever type\n",
    "    if 'retriever_type' in ret_df.columns and ret_df['retriever_type'].nunique() > 1:\n",
    "        rt_quad = ret_df.groupby('retriever_type')['quadrant'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "        rt_quad[quad_order].plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                                 color=colors[:len(quad_order)])\n",
    "        axes[1].set_title('Outcome Quadrants by Retriever Type')\n",
    "        axes[1].set_ylabel('Proportion')\n",
    "        axes[1].legend(fontsize=8, loc='upper right')\n",
    "        axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Reranking Effectiveness\n",
    "\n",
    "How much does reranking improve the position of answer-bearing documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not doc_df.empty and doc_df['retrieval_rank'].notna().sum() > 0:\n",
    "    # Filter to documents that have both original and final rank\n",
    "    reranked = doc_df[doc_df['retrieval_rank'].notna() & doc_df['rank'].notna()].copy()\n",
    "    reranked['rank_change'] = reranked['retrieval_rank'] - reranked['rank']  # positive = improved\n",
    "\n",
    "    if not reranked.empty:\n",
    "        # Answer-bearing docs: rank change by reranker\n",
    "        answer_docs = reranked[reranked['has_answer']]\n",
    "        non_answer_docs = reranked[~reranked['has_answer']]\n",
    "\n",
    "        print(f\"Documents with rank data: {len(reranked):,}\")\n",
    "        print(f\"  Answer-bearing: {len(answer_docs):,}\")\n",
    "        print(f\"  Non-answer: {len(non_answer_docs):,}\")\n",
    "        print()\n",
    "\n",
    "        if not answer_docs.empty:\n",
    "            by_reranker = answer_docs.groupby('reranker').agg(\n",
    "                mean_rank_change=('rank_change', 'mean'),\n",
    "                median_rank_change=('rank_change', 'median'),\n",
    "                pct_improved=('rank_change', lambda x: (x > 0).mean()),\n",
    "                mean_final_rank=('rank', 'mean'),\n",
    "                n=('rank_change', 'count'),\n",
    "            ).round(3)\n",
    "\n",
    "            print(\"Reranking impact on ANSWER-BEARING documents:\")\n",
    "            display(by_reranker)\n",
    "\n",
    "            # Distribution of rank changes\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "            for rr in sorted(answer_docs['reranker'].unique()):\n",
    "                sub = answer_docs[answer_docs['reranker'] == rr]\n",
    "                axes[0].hist(sub['rank_change'], bins=20, alpha=0.5, label=rr, edgecolor='black')\n",
    "            axes[0].axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "            axes[0].set_xlabel('Rank Change (positive = improved)')\n",
    "            axes[0].set_ylabel('Count')\n",
    "            axes[0].set_title('Rank Change for Answer-Bearing Documents')\n",
    "            axes[0].legend()\n",
    "\n",
    "            # Before vs After rank scatter\n",
    "            sample = answer_docs.sample(min(2000, len(answer_docs)), random_state=42)\n",
    "            for rr in sorted(sample['reranker'].unique()):\n",
    "                sub = sample[sample['reranker'] == rr]\n",
    "                axes[1].scatter(sub['retrieval_rank'], sub['rank'],\n",
    "                               s=10, alpha=0.3, label=rr)\n",
    "            axes[1].plot([0, 25], [0, 25], 'k--', alpha=0.5, label='No change')\n",
    "            axes[1].set_xlabel('Original Rank (before reranking)')\n",
    "            axes[1].set_ylabel('Final Rank (after reranking)')\n",
    "            axes[1].set_title('Reranking Effect on Answer Documents')\n",
    "            axes[1].legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No document-level rank data available for reranking analysis.\")\n",
    "    print(\"This requires retrieved_docs with retrieval_rank and rank fields in predictions.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Retrieval Quality vs F1 Correlation\n",
    "\n",
    "Does higher retrieval recall correlate with higher F1 at the experiment level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ret_df.empty:\n",
    "    # Aggregate per experiment: retrieval recall and mean F1\n",
    "    exp_stats = ret_df.groupby('experiment').agg(\n",
    "        retrieval_recall=('has_answer_in_prompt', 'mean'),\n",
    "        gen_given_ctx=('got_correct', lambda x: x[ret_df.loc[x.index, 'has_answer_in_prompt']].mean()\n",
    "                        if ret_df.loc[x.index, 'has_answer_in_prompt'].sum() > 0 else np.nan),\n",
    "        mean_f1=('f1', 'mean'),\n",
    "        accuracy=('got_correct', 'mean'),\n",
    "        n_questions=('idx', 'count'),\n",
    "        model_short=('model_short', 'first'),\n",
    "        dataset=('dataset', 'first'),\n",
    "        retriever_type=('retriever_type', 'first'),\n",
    "        reranker=('reranker', 'first'),\n",
    "    ).dropna(subset=['mean_f1'])\n",
    "\n",
    "    if len(exp_stats) >= 5:\n",
    "        # Correlation\n",
    "        r, p = scipy_stats.pearsonr(exp_stats['retrieval_recall'], exp_stats['mean_f1'])\n",
    "        print(f\"Pearson correlation (retrieval recall vs F1): r={r:.3f}, p={p:.4f}\")\n",
    "\n",
    "        rho, p_rho = scipy_stats.spearmanr(exp_stats['retrieval_recall'], exp_stats['mean_f1'])\n",
    "        print(f\"Spearman correlation: rho={rho:.3f}, p={p_rho:.4f}\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Scatter: retrieval recall vs F1\n",
    "        for ds in sorted(exp_stats['dataset'].unique()):\n",
    "            sub = exp_stats[exp_stats['dataset'] == ds]\n",
    "            axes[0].scatter(sub['retrieval_recall'], sub['mean_f1'],\n",
    "                           s=30, alpha=0.5, label=ds)\n",
    "\n",
    "        # Regression line\n",
    "        x = exp_stats['retrieval_recall'].values\n",
    "        y = exp_stats['mean_f1'].values\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p_line = np.poly1d(z)\n",
    "        x_sorted = np.sort(x)\n",
    "        axes[0].plot(x_sorted, p_line(x_sorted), 'k--', alpha=0.5,\n",
    "                     label=f'r={r:.3f}')\n",
    "\n",
    "        axes[0].set_xlabel('Retrieval Recall (answer-in-context rate)')\n",
    "        axes[0].set_ylabel('Mean F1')\n",
    "        axes[0].set_title('Retrieval Quality vs Generation Quality')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "\n",
    "        # Scatter: retrieval recall vs gen|ctx\n",
    "        gen_stats = exp_stats.dropna(subset=['gen_given_ctx'])\n",
    "        if not gen_stats.empty:\n",
    "            for ds in sorted(gen_stats['dataset'].unique()):\n",
    "                sub = gen_stats[gen_stats['dataset'] == ds]\n",
    "                axes[1].scatter(sub['retrieval_recall'], sub['gen_given_ctx'],\n",
    "                               s=30, alpha=0.5, label=ds)\n",
    "            axes[1].set_xlabel('Retrieval Recall')\n",
    "            axes[1].set_ylabel('Generation|Context (accuracy given answer in context)')\n",
    "            axes[1].set_title('Retrieval vs Generation Quality')\n",
    "            axes[1].axhline(y=0.5, color='red', linestyle='--', alpha=0.3)\n",
    "            axes[1].axvline(x=0.5, color='red', linestyle='--', alpha=0.3)\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Per-Retriever Deep Dive\n",
    "\n",
    "Detailed retrieval diagnostics for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ret_df.empty:\n",
    "    # Comprehensive per-retriever stats\n",
    "    retriever_stats = ret_df.groupby('retriever_type').agg(\n",
    "        retrieval_recall=('has_answer_in_prompt', 'mean'),\n",
    "        accuracy=('got_correct', 'mean'),\n",
    "        mean_f1=('f1', lambda x: x.dropna().mean()),\n",
    "        n_questions=('idx', 'count'),\n",
    "        n_experiments=('experiment', 'nunique'),\n",
    "    ).round(4)\n",
    "\n",
    "    # Add generation|context rate\n",
    "    for rt in retriever_stats.index:\n",
    "        rt_data = ret_df[ret_df['retriever_type'] == rt]\n",
    "        ctx_mask = rt_data['has_answer_in_prompt']\n",
    "        gen_rate = rt_data.loc[ctx_mask, 'got_correct'].mean() if ctx_mask.sum() > 0 else np.nan\n",
    "        retriever_stats.loc[rt, 'gen_given_ctx'] = gen_rate\n",
    "\n",
    "    print(\"Per-Retriever Diagnostics:\")\n",
    "    display(retriever_stats.round(3))\n",
    "\n",
    "    # Answer rank distribution (where in the top-K is the answer?)\n",
    "    answer_rank_data = ret_df[ret_df['answer_doc_rank'].notna()].copy()\n",
    "    if not answer_rank_data.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        for rt in sorted(answer_rank_data['retriever_type'].dropna().unique()):\n",
    "            sub = answer_rank_data[answer_rank_data['retriever_type'] == rt]\n",
    "            ranks = sub['answer_doc_rank']\n",
    "            ax.hist(ranks, bins=range(1, int(ranks.max()) + 2), alpha=0.5,\n",
    "                    label=f\"{rt} (n={len(sub)}, median={ranks.median():.0f})\",\n",
    "                    edgecolor='black')\n",
    "\n",
    "        ax.set_xlabel('Rank of First Answer-Bearing Document')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Where Does the Answer Appear in Retrieved Documents?')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K sensitivity for retrieval recall\n",
    "if not ret_df.empty and 'top_k' in ret_df.columns and ret_df['top_k'].nunique() > 1:\n",
    "    topk_recall = ret_df.groupby(['top_k', 'retriever_type']).agg(\n",
    "        recall=('has_answer_in_prompt', 'mean'),\n",
    "        mean_f1=('f1', lambda x: x.dropna().mean()),\n",
    "        n=('idx', 'count'),\n",
    "    ).reset_index()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for rt in sorted(topk_recall['retriever_type'].unique()):\n",
    "        sub = topk_recall[topk_recall['retriever_type'] == rt].sort_values('top_k')\n",
    "        axes[0].plot(sub['top_k'], sub['recall'], 'o-', label=rt)\n",
    "        axes[1].plot(sub['top_k'], sub['mean_f1'], 'o-', label=rt)\n",
    "\n",
    "    axes[0].set_xlabel('Top-K')\n",
    "    axes[0].set_ylabel('Retrieval Recall')\n",
    "    axes[0].set_title('Retrieval Recall vs Top-K')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].set_xlabel('Top-K')\n",
    "    axes[1].set_ylabel('Mean F1')\n",
    "    axes[1].set_title('F1 vs Top-K (diminishing returns?)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key retrieval diagnostics:\n",
    "- **Answer-in-context rate**: What fraction of questions have the answer in retrieved docs\n",
    "- **Bottleneck analysis**: Is performance limited by retrieval or generation\n",
    "- **Reranking value**: How much do rerankers improve answer document positioning\n",
    "- **Retrieval-F1 correlation**: How tightly does retrieval quality predict final F1\n",
    "- **Per-retriever profiles**: Which retrievers work best for which datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}