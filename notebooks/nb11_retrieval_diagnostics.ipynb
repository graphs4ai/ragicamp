{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB11: Retrieval Quality Diagnostics\n",
    "\n",
    "**Question:** Is performance bottlenecked by retrieval or generation?\n",
    "\n",
    "This notebook diagnoses retrieval quality using saved predictions:\n",
    "1. **Answer-in-context rates** by retriever, model, dataset\n",
    "2. **Retrieval vs Generation bottleneck** quadrant analysis\n",
    "3. **Reranking effectiveness** (rank changes, score distributions)\n",
    "4. **Correlation** between retrieval quality and F1\n",
    "5. **Per-retriever diagnostic** deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_all_results, setup_plotting, parse_experiment_name,\n",
    "    _enrich_from_metadata, SKIP_DIRS,\n",
    "    PRIMARY_METRIC, BROKEN_MODELS, MODEL_TIER,\n",
    ")\n",
    "\n",
    "setup_plotting()\n",
    "STUDY_PATH = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "# Load experiment-level results for filtering\n",
    "df_all = load_all_results(STUDY_PATH)\n",
    "df = df_all[~df_all['model_short'].isin(BROKEN_MODELS)].copy()\n",
    "rag_df = df[df['exp_type'] == 'rag'].copy()\n",
    "print(f\"RAG experiments to analyze: {len(rag_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Retrieval analysis functions (adapted from scripts/analyze_retrieval.py) ----\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for fuzzy matching.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def answer_in_text(expected, text: str) -> bool:\n",
    "    \"\"\"Check if any expected answer appears in text.\"\"\"\n",
    "    if not text or not expected:\n",
    "        return False\n",
    "    text_norm = normalize_text(text)\n",
    "    answers = expected if isinstance(expected, list) else [expected]\n",
    "    for ans in answers:\n",
    "        ans_norm = normalize_text(str(ans))\n",
    "        if len(ans_norm) >= 2 and ans_norm in text_norm:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_correct(prediction: str, expected: list, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"Check if prediction matches any expected answer (fuzzy).\"\"\"\n",
    "    if not prediction or not expected:\n",
    "        return False\n",
    "    pred_norm = normalize_text(prediction)\n",
    "    for exp in expected:\n",
    "        exp_norm = normalize_text(str(exp))\n",
    "        if not exp_norm:\n",
    "            continue\n",
    "        if exp_norm in pred_norm or pred_norm in exp_norm:\n",
    "            return True\n",
    "        pred_words = set(pred_norm.split())\n",
    "        exp_words = set(exp_norm.split())\n",
    "        if exp_words and pred_words:\n",
    "            overlap = len(pred_words & exp_words) / len(exp_words)\n",
    "            if overlap >= threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"Analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load prediction-level retrieval data for all RAG experiments ----\n",
    "\n",
    "rag_experiment_names = set(rag_df['name'].values)\n",
    "retrieval_rows = []\n",
    "doc_rows = []  # Per-document data for reranking analysis\n",
    "\n",
    "for exp_dir in STUDY_PATH.iterdir():\n",
    "    if not exp_dir.is_dir() or exp_dir.name in SKIP_DIRS:\n",
    "        continue\n",
    "    if exp_dir.name.startswith('.') or exp_dir.name.startswith('_'):\n",
    "        continue\n",
    "\n",
    "    pred_file = exp_dir / \"predictions.json\"\n",
    "    meta_file = exp_dir / \"metadata.json\"\n",
    "    if not pred_file.exists():\n",
    "        continue\n",
    "\n",
    "    # Parse experiment config\n",
    "    config = parse_experiment_name(exp_dir.name)\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as f:\n",
    "            _enrich_from_metadata(config, json.load(f))\n",
    "\n",
    "    if config.get('exp_type') == 'direct':\n",
    "        continue\n",
    "    if config.get('model_short') in BROKEN_MODELS:\n",
    "        continue\n",
    "\n",
    "    with open(pred_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    preds = data.get('predictions', [])\n",
    "    if not preds:\n",
    "        continue\n",
    "\n",
    "    for p in preds:\n",
    "        expected = p.get('expected', [])\n",
    "        prediction = p.get('prediction', '')\n",
    "        prompt = p.get('prompt', '')\n",
    "        retrieved_docs = p.get('retrieved_docs', [])\n",
    "        metrics = p.get('metrics', {})\n",
    "\n",
    "        if not expected:\n",
    "            continue\n",
    "\n",
    "        # Check answer in context (using prompt text)\n",
    "        has_answer_in_prompt = answer_in_text(expected, prompt)\n",
    "\n",
    "        # Check answer in individual documents\n",
    "        answer_in_any_doc = False\n",
    "        answer_doc_rank = None\n",
    "        for doc in retrieved_docs:\n",
    "            content = doc.get('content', '')\n",
    "            if answer_in_text(expected, content):\n",
    "                answer_in_any_doc = True\n",
    "                rank = doc.get('rank', 999)\n",
    "                if answer_doc_rank is None or rank < answer_doc_rank:\n",
    "                    answer_doc_rank = rank\n",
    "\n",
    "        got_correct = is_correct(prediction, expected)\n",
    "        f1_score = metrics.get('f1', np.nan)\n",
    "\n",
    "        retrieval_rows.append({\n",
    "            'experiment': exp_dir.name,\n",
    "            'idx': p.get('idx'),\n",
    "            'model_short': config.get('model_short'),\n",
    "            'dataset': config.get('dataset'),\n",
    "            'retriever': config.get('retriever'),\n",
    "            'retriever_type': config.get('retriever_type'),\n",
    "            'reranker': config.get('reranker', 'none'),\n",
    "            'agent_type': config.get('agent_type'),\n",
    "            'top_k': config.get('top_k'),\n",
    "            'query_transform': config.get('query_transform', 'none'),\n",
    "            'has_answer_in_prompt': has_answer_in_prompt,\n",
    "            'answer_in_any_doc': answer_in_any_doc,\n",
    "            'answer_doc_rank': answer_doc_rank,\n",
    "            'got_correct': got_correct,\n",
    "            'f1': f1_score,\n",
    "            'n_docs': len(retrieved_docs),\n",
    "        })\n",
    "\n",
    "        # Per-document data for reranking analysis\n",
    "        for doc in retrieved_docs:\n",
    "            doc_rows.append({\n",
    "                'experiment': exp_dir.name,\n",
    "                'idx': p.get('idx'),\n",
    "                'reranker': config.get('reranker', 'none'),\n",
    "                'rank': doc.get('rank'),\n",
    "                'score': doc.get('score'),\n",
    "                'retrieval_score': doc.get('retrieval_score'),\n",
    "                'retrieval_rank': doc.get('retrieval_rank'),\n",
    "                'rerank_score': doc.get('rerank_score'),\n",
    "                'has_answer': answer_in_text(expected, doc.get('content', '')),\n",
    "            })\n",
    "\n",
    "ret_df = pd.DataFrame(retrieval_rows)\n",
    "doc_df = pd.DataFrame(doc_rows) if doc_rows else pd.DataFrame()\n",
    "\n",
    "print(f\"Loaded {len(ret_df)} question-level retrieval records from {ret_df['experiment'].nunique()} experiments\")\n",
    "if not doc_df.empty:\n",
    "    print(f\"Loaded {len(doc_df)} document-level records\")\n",
    "    print(f\"Documents with rerank_score: {doc_df['rerank_score'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Answer-in-Context Rates\n",
    "\n",
    "What fraction of questions have the answer in the retrieved context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "if not ret_df.empty:\n    # Overall retrieval recall (dataset-stratified)\n    if ret_df['dataset'].nunique() > 1:\n        overall_recall = ret_df.groupby('dataset')['has_answer_in_prompt'].mean().mean()\n        print(f\"Overall answer-in-context rate (dataset-stratified): {overall_recall:.1%}\")\n    else:\n        overall_recall = ret_df['has_answer_in_prompt'].mean()\n        print(f\"Overall answer-in-context rate: {overall_recall:.1%}\")\n    print()\n\n    # By retriever (dataset-stratified)\n    if 'retriever_type' in ret_df.columns:\n        if ret_df['dataset'].nunique() > 1:\n            by_retriever = (\n                ret_df.groupby(['retriever_type', 'dataset'])\n                .agg(recall=('has_answer_in_prompt', 'mean'), n=('has_answer_in_prompt', 'count'))\n                .reset_index()\n                .groupby('retriever_type')\n                .agg(recall=('recall', 'mean'), n=('n', 'sum'))\n                .sort_values('recall', ascending=False)\n            )\n        else:\n            by_retriever = ret_df.groupby('retriever_type').agg(\n                recall=('has_answer_in_prompt', 'mean'),\n                n=('has_answer_in_prompt', 'count'),\n            ).sort_values('recall', ascending=False)\n        print(\"By retriever type (dataset-stratified):\")\n        display(by_retriever.round(3))\n\n    # By dataset (this is naturally per-dataset)\n    by_dataset = ret_df.groupby('dataset').agg(\n        recall=('has_answer_in_prompt', 'mean'),\n        n=('has_answer_in_prompt', 'count'),\n    ).sort_values('recall', ascending=False)\n    print(\"\\nBy dataset:\")\n    display(by_dataset.round(3))\n\n    # By reranker (dataset-stratified)\n    if ret_df['dataset'].nunique() > 1:\n        by_reranker = (\n            ret_df.groupby(['reranker', 'dataset'])\n            .agg(recall=('has_answer_in_prompt', 'mean'), n=('has_answer_in_prompt', 'count'))\n            .reset_index()\n            .groupby('reranker')\n            .agg(recall=('recall', 'mean'), n=('n', 'sum'))\n            .sort_values('recall', ascending=False)\n        )\n    else:\n        by_reranker = ret_df.groupby('reranker').agg(\n            recall=('has_answer_in_prompt', 'mean'),\n            n=('has_answer_in_prompt', 'count'),\n        ).sort_values('recall', ascending=False)\n    print(\"\\nBy reranker (dataset-stratified):\")\n    display(by_reranker.round(3))\n\n    # Heatmap: retriever_type x dataset (already naturally stratified)\n    pivot = ret_df.groupby(['retriever_type', 'dataset'])['has_answer_in_prompt'].mean().unstack()\n    if not pivot.empty and pivot.shape[0] > 1:\n        fig, ax = plt.subplots(figsize=(8, 5))\n        sns.heatmap(pivot, annot=True, fmt='.1%', cmap='RdYlGn', vmin=0, vmax=1, ax=ax)\n        ax.set_title('Answer-in-Context Rate by Retriever Type x Dataset')\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Retrieval vs Generation Bottleneck\n",
    "\n",
    "Quadrant analysis:\n",
    "- **Correct + Context**: Retrieval and generation both succeeded\n",
    "- **Wrong + Context**: Had the answer but generation failed\n",
    "- **Correct + No Context**: Got lucky (or parametric knowledge)\n",
    "- **Wrong + No Context**: Retrieval failure (answer not retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "if not ret_df.empty:\n    # Classify into quadrants\n    ret_df['quadrant'] = 'unknown'\n    mask_ctx = ret_df['has_answer_in_prompt']\n    mask_correct = ret_df['got_correct']\n\n    ret_df.loc[mask_ctx & mask_correct, 'quadrant'] = 'Correct + Context'\n    ret_df.loc[mask_ctx & ~mask_correct, 'quadrant'] = 'Wrong + Context'\n    ret_df.loc[~mask_ctx & mask_correct, 'quadrant'] = 'Correct + No Context'\n    ret_df.loc[~mask_ctx & ~mask_correct, 'quadrant'] = 'Wrong + No Context'\n\n    # Dataset-stratified overall distribution\n    if ret_df['dataset'].nunique() > 1:\n        print(\"Overall Quadrant Distribution (dataset-stratified):\")\n        per_ds_quad = (\n            ret_df.groupby('dataset')['quadrant']\n            .value_counts(normalize=True)\n            .rename('pct')\n            .reset_index()\n        )\n        strat_quad = per_ds_quad.groupby('quadrant')['pct'].mean()\n        for q in ['Correct + Context', 'Wrong + Context', 'Correct + No Context', 'Wrong + No Context']:\n            if q in strat_quad.index:\n                print(f\"  {q:<25s}: {strat_quad[q]:.1%}\")\n    else:\n        quadrant_pcts = ret_df['quadrant'].value_counts(normalize=True)\n        print(\"Overall Quadrant Distribution:\")\n        for q in ['Correct + Context', 'Wrong + Context', 'Correct + No Context', 'Wrong + No Context']:\n            if q in quadrant_pcts.index:\n                print(f\"  {q:<25s}: {quadrant_pcts[q]:.1%}\")\n\n    # Bottleneck identification (dataset-stratified)\n    if ret_df['dataset'].nunique() > 1:\n        ds_recalls = ret_df.groupby('dataset')['has_answer_in_prompt'].mean()\n        retrieval_recall = ds_recalls.mean()\n        ds_gen = []\n        for ds in ret_df['dataset'].unique():\n            ds_mask = (ret_df['dataset'] == ds) & ret_df['has_answer_in_prompt']\n            if ds_mask.sum() > 0:\n                ds_gen.append(ret_df.loc[ds_mask, 'got_correct'].mean())\n        gen_given_ctx = np.mean(ds_gen) if ds_gen else 0\n    else:\n        retrieval_recall = mask_ctx.mean()\n        gen_given_ctx = ret_df.loc[mask_ctx, 'got_correct'].mean() if mask_ctx.sum() > 0 else 0\n\n    print(f\"\\nRetrieval Recall (dataset-stratified): {retrieval_recall:.1%}\")\n    print(f\"Generation|Context (dataset-stratified): {gen_given_ctx:.1%}\")\n    if retrieval_recall < 0.5:\n        print(\"Bottleneck: RETRIEVAL\")\n    elif gen_given_ctx < 0.5:\n        print(\"Bottleneck: GENERATION\")\n    else:\n        print(\"Bottleneck: BALANCED\")\n\n    # Per-model breakdown (dataset-stratified)\n    print(\"\\nPer-model bottleneck (dataset-stratified):\")\n    for model in sorted(ret_df['model_short'].unique()):\n        m_df = ret_df[ret_df['model_short'] == model]\n        if m_df['dataset'].nunique() > 1:\n            m_recall = m_df.groupby('dataset')['has_answer_in_prompt'].mean().mean()\n            ds_gen_rates = []\n            for ds in m_df['dataset'].unique():\n                ds_m = m_df[(m_df['dataset'] == ds) & m_df['has_answer_in_prompt']]\n                if len(ds_m) > 0:\n                    ds_gen_rates.append(ds_m['got_correct'].mean())\n            m_gen = np.mean(ds_gen_rates) if ds_gen_rates else 0\n        else:\n            m_ctx = m_df['has_answer_in_prompt']\n            m_recall = m_ctx.mean()\n            m_gen = m_df.loc[m_ctx, 'got_correct'].mean() if m_ctx.sum() > 0 else 0\n        bottleneck = 'RETRIEVAL' if m_recall < 0.5 else 'GENERATION' if m_gen < 0.5 else 'BALANCED'\n        print(f\"  {model:<16s}: recall={m_recall:.1%}, gen|ctx={m_gen:.1%}  -> {bottleneck}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrant breakdown by dataset and retriever\n",
    "if not ret_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # By dataset\n",
    "    ds_quad = ret_df.groupby('dataset')['quadrant'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    quad_order = ['Correct + Context', 'Correct + No Context', 'Wrong + Context', 'Wrong + No Context']\n",
    "    quad_order = [q for q in quad_order if q in ds_quad.columns]\n",
    "    colors = ['#2ecc71', '#27ae60', '#e67e22', '#e74c3c']\n",
    "    ds_quad[quad_order].plot(kind='bar', stacked=True, ax=axes[0],\n",
    "                             color=colors[:len(quad_order)])\n",
    "    axes[0].set_title('Outcome Quadrants by Dataset')\n",
    "    axes[0].set_ylabel('Proportion')\n",
    "    axes[0].legend(fontsize=8, loc='upper right')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    # By retriever type\n",
    "    if 'retriever_type' in ret_df.columns and ret_df['retriever_type'].nunique() > 1:\n",
    "        rt_quad = ret_df.groupby('retriever_type')['quadrant'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "        rt_quad[quad_order].plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                                 color=colors[:len(quad_order)])\n",
    "        axes[1].set_title('Outcome Quadrants by Retriever Type')\n",
    "        axes[1].set_ylabel('Proportion')\n",
    "        axes[1].legend(fontsize=8, loc='upper right')\n",
    "        axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Reranking Effectiveness\n",
    "\n",
    "How much does reranking improve the position of answer-bearing documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "if not doc_df.empty and doc_df['retrieval_rank'].notna().sum() > 0:\n    # Filter to documents that have both original and final rank\n    reranked = doc_df[doc_df['retrieval_rank'].notna() & doc_df['rank'].notna()].copy()\n    reranked['rank_change'] = reranked['retrieval_rank'] - reranked['rank']  # positive = improved\n\n    # Add dataset info from ret_df for stratification\n    if 'dataset' not in reranked.columns:\n        exp_to_ds = ret_df.groupby('experiment')['dataset'].first()\n        reranked['dataset'] = reranked['experiment'].map(exp_to_ds)\n\n    if not reranked.empty:\n        answer_docs = reranked[reranked['has_answer']]\n        non_answer_docs = reranked[~reranked['has_answer']]\n\n        print(f\"Documents with rank data: {len(reranked):,}\")\n        print(f\"  Answer-bearing: {len(answer_docs):,}\")\n        print(f\"  Non-answer: {len(non_answer_docs):,}\")\n        print()\n\n        if not answer_docs.empty:\n            # Dataset-stratified reranker stats\n            if 'dataset' in answer_docs.columns and answer_docs['dataset'].nunique() > 1:\n                per_ds = (\n                    answer_docs.groupby(['reranker', 'dataset'])\n                    .agg(\n                        mean_rank_change=('rank_change', 'mean'),\n                        median_rank_change=('rank_change', 'median'),\n                        pct_improved=('rank_change', lambda x: (x > 0).mean()),\n                        mean_final_rank=('rank', 'mean'),\n                        n=('rank_change', 'count'),\n                    )\n                    .reset_index()\n                )\n                by_reranker = (\n                    per_ds.groupby('reranker')\n                    .agg(\n                        mean_rank_change=('mean_rank_change', 'mean'),\n                        median_rank_change=('median_rank_change', 'mean'),\n                        pct_improved=('pct_improved', 'mean'),\n                        mean_final_rank=('mean_final_rank', 'mean'),\n                        n=('n', 'sum'),\n                    )\n                    .round(3)\n                )\n            else:\n                by_reranker = answer_docs.groupby('reranker').agg(\n                    mean_rank_change=('rank_change', 'mean'),\n                    median_rank_change=('rank_change', 'median'),\n                    pct_improved=('rank_change', lambda x: (x > 0).mean()),\n                    mean_final_rank=('rank', 'mean'),\n                    n=('rank_change', 'count'),\n                ).round(3)\n\n            print(\"Reranking impact on ANSWER-BEARING documents (dataset-stratified):\")\n            display(by_reranker)\n\n            # Distribution of rank changes\n            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n            for rr in sorted(answer_docs['reranker'].unique()):\n                sub = answer_docs[answer_docs['reranker'] == rr]\n                axes[0].hist(sub['rank_change'], bins=20, alpha=0.5, label=rr, edgecolor='black')\n            axes[0].axvline(x=0, color='red', linestyle='--', linewidth=1)\n            axes[0].set_xlabel('Rank Change (positive = improved)')\n            axes[0].set_ylabel('Count')\n            axes[0].set_title('Rank Change for Answer-Bearing Documents')\n            axes[0].legend()\n\n            # Before vs After rank scatter\n            sample = answer_docs.sample(min(2000, len(answer_docs)), random_state=42)\n            for rr in sorted(sample['reranker'].unique()):\n                sub = sample[sample['reranker'] == rr]\n                axes[1].scatter(sub['retrieval_rank'], sub['rank'],\n                               s=10, alpha=0.3, label=rr)\n            axes[1].plot([0, 25], [0, 25], 'k--', alpha=0.5, label='No change')\n            axes[1].set_xlabel('Original Rank (before reranking)')\n            axes[1].set_ylabel('Final Rank (after reranking)')\n            axes[1].set_title('Reranking Effect on Answer Documents')\n            axes[1].legend()\n\n            plt.tight_layout()\n            plt.show()\nelse:\n    print(\"No document-level rank data available for reranking analysis.\")\n    print(\"This requires retrieved_docs with retrieval_rank and rank fields in predictions.json.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Retrieval Quality vs F1 Correlation\n",
    "\n",
    "Does higher retrieval recall correlate with higher F1 at the experiment level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "if not ret_df.empty:\n    # Aggregate per experiment: retrieval recall and mean F1\n    exp_stats = ret_df.groupby('experiment').agg(\n        retrieval_recall=('has_answer_in_prompt', 'mean'),\n        gen_given_ctx=('got_correct', lambda x: x[ret_df.loc[x.index, 'has_answer_in_prompt']].mean()\n                        if ret_df.loc[x.index, 'has_answer_in_prompt'].sum() > 0 else np.nan),\n        mean_f1=('f1', 'mean'),\n        accuracy=('got_correct', 'mean'),\n        n_questions=('idx', 'count'),\n        model_short=('model_short', 'first'),\n        dataset=('dataset', 'first'),\n        retriever_type=('retriever_type', 'first'),\n        reranker=('reranker', 'first'),\n    ).dropna(subset=['mean_f1'])\n\n    if len(exp_stats) >= 5:\n        # Per-dataset correlation, then report both per-dataset and combined\n        print(\"Retrieval Recall vs F1 Correlation (per-dataset):\")\n        per_ds_corrs = []\n        for ds in sorted(exp_stats['dataset'].unique()):\n            sub = exp_stats[exp_stats['dataset'] == ds]\n            if len(sub) >= 5:\n                r_ds, p_ds = scipy_stats.pearsonr(sub['retrieval_recall'], sub['mean_f1'])\n                rho_ds, prho_ds = scipy_stats.spearmanr(sub['retrieval_recall'], sub['mean_f1'])\n                per_ds_corrs.append({'dataset': ds, 'pearson_r': r_ds, 'spearman_rho': rho_ds, 'n': len(sub)})\n                print(f\"  {ds}: Pearson r={r_ds:.3f}, Spearman rho={rho_ds:.3f} (n={len(sub)})\")\n\n        # Overall (pooled â€” shown for reference but dataset scatter colors make it interpretable)\n        r, p = scipy_stats.pearsonr(exp_stats['retrieval_recall'], exp_stats['mean_f1'])\n        rho, p_rho = scipy_stats.spearmanr(exp_stats['retrieval_recall'], exp_stats['mean_f1'])\n        print(f\"\\n  Pooled (all datasets): Pearson r={r:.3f}, Spearman rho={rho:.3f}\")\n        if per_ds_corrs:\n            mean_r = np.mean([c['pearson_r'] for c in per_ds_corrs])\n            mean_rho = np.mean([c['spearman_rho'] for c in per_ds_corrs])\n            print(f\"  Mean across datasets: Pearson r={mean_r:.3f}, Spearman rho={mean_rho:.3f}\")\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n        # Scatter: retrieval recall vs F1 (colored by dataset)\n        for ds in sorted(exp_stats['dataset'].unique()):\n            sub = exp_stats[exp_stats['dataset'] == ds]\n            axes[0].scatter(sub['retrieval_recall'], sub['mean_f1'],\n                           s=30, alpha=0.5, label=ds)\n\n        # Per-dataset regression lines\n        for ds in sorted(exp_stats['dataset'].unique()):\n            sub = exp_stats[exp_stats['dataset'] == ds]\n            if len(sub) >= 5:\n                x = sub['retrieval_recall'].values\n                y = sub['mean_f1'].values\n                z = np.polyfit(x, y, 1)\n                p_line = np.poly1d(z)\n                x_sorted = np.sort(x)\n                axes[0].plot(x_sorted, p_line(x_sorted), '--', alpha=0.4)\n\n        axes[0].set_xlabel('Retrieval Recall (answer-in-context rate)')\n        axes[0].set_ylabel('Mean F1')\n        axes[0].set_title('Retrieval Quality vs Generation Quality\\n(per-dataset regression lines)')\n        axes[0].legend()\n        axes[0].grid(alpha=0.3)\n\n        # Scatter: retrieval recall vs gen|ctx\n        gen_stats = exp_stats.dropna(subset=['gen_given_ctx'])\n        if not gen_stats.empty:\n            for ds in sorted(gen_stats['dataset'].unique()):\n                sub = gen_stats[gen_stats['dataset'] == ds]\n                axes[1].scatter(sub['retrieval_recall'], sub['gen_given_ctx'],\n                               s=30, alpha=0.5, label=ds)\n            axes[1].set_xlabel('Retrieval Recall')\n            axes[1].set_ylabel('Generation|Context (accuracy given answer in context)')\n            axes[1].set_title('Retrieval vs Generation Quality')\n            axes[1].axhline(y=0.5, color='red', linestyle='--', alpha=0.3)\n            axes[1].axvline(x=0.5, color='red', linestyle='--', alpha=0.3)\n            axes[1].legend()\n            axes[1].grid(alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Per-Retriever Deep Dive\n",
    "\n",
    "Detailed retrieval diagnostics for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "if not ret_df.empty:\n    # Dataset-stratified per-retriever stats\n    if ret_df['dataset'].nunique() > 1:\n        per_ds_stats = (\n            ret_df.groupby(['retriever_type', 'dataset'])\n            .agg(\n                retrieval_recall=('has_answer_in_prompt', 'mean'),\n                accuracy=('got_correct', 'mean'),\n                mean_f1=('f1', lambda x: x.dropna().mean()),\n                n_questions=('idx', 'count'),\n                n_experiments=('experiment', 'nunique'),\n            )\n            .reset_index()\n        )\n        retriever_stats = (\n            per_ds_stats.groupby('retriever_type')\n            .agg(\n                retrieval_recall=('retrieval_recall', 'mean'),\n                accuracy=('accuracy', 'mean'),\n                mean_f1=('mean_f1', 'mean'),\n                n_questions=('n_questions', 'sum'),\n                n_experiments=('n_experiments', 'sum'),\n            )\n            .round(4)\n        )\n    else:\n        retriever_stats = ret_df.groupby('retriever_type').agg(\n            retrieval_recall=('has_answer_in_prompt', 'mean'),\n            accuracy=('got_correct', 'mean'),\n            mean_f1=('f1', lambda x: x.dropna().mean()),\n            n_questions=('idx', 'count'),\n            n_experiments=('experiment', 'nunique'),\n        ).round(4)\n\n    # Add generation|context rate (dataset-stratified)\n    for rt in retriever_stats.index:\n        rt_data = ret_df[ret_df['retriever_type'] == rt]\n        if rt_data['dataset'].nunique() > 1:\n            ds_gen_rates = []\n            for ds in rt_data['dataset'].unique():\n                ds_data = rt_data[(rt_data['dataset'] == ds) & rt_data['has_answer_in_prompt']]\n                if len(ds_data) > 0:\n                    ds_gen_rates.append(ds_data['got_correct'].mean())\n            gen_rate = np.mean(ds_gen_rates) if ds_gen_rates else np.nan\n        else:\n            ctx_mask = rt_data['has_answer_in_prompt']\n            gen_rate = rt_data.loc[ctx_mask, 'got_correct'].mean() if ctx_mask.sum() > 0 else np.nan\n        retriever_stats.loc[rt, 'gen_given_ctx'] = gen_rate\n\n    print(\"Per-Retriever Diagnostics (dataset-stratified):\")\n    display(retriever_stats.round(3))\n\n    # Answer rank distribution (where in the top-K is the answer?)\n    answer_rank_data = ret_df[ret_df['answer_doc_rank'].notna()].copy()\n    if not answer_rank_data.empty:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        for rt in sorted(answer_rank_data['retriever_type'].dropna().unique()):\n            sub = answer_rank_data[answer_rank_data['retriever_type'] == rt]\n            ranks = sub['answer_doc_rank']\n            ax.hist(ranks, bins=range(1, int(ranks.max()) + 2), alpha=0.5,\n                    label=f\"{rt} (n={len(sub)}, median={ranks.median():.0f})\",\n                    edgecolor='black')\n\n        ax.set_xlabel('Rank of First Answer-Bearing Document')\n        ax.set_ylabel('Count')\n        ax.set_title('Where Does the Answer Appear in Retrieved Documents?')\n        ax.legend()\n        ax.grid(alpha=0.3)\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Top-K sensitivity for retrieval recall (dataset-stratified)\nif not ret_df.empty and 'top_k' in ret_df.columns and ret_df['top_k'].nunique() > 1:\n    if ret_df['dataset'].nunique() > 1:\n        # Per-(top_k, retriever_type, dataset) stats, then average across datasets\n        per_ds = (\n            ret_df.groupby(['top_k', 'retriever_type', 'dataset'])\n            .agg(\n                recall=('has_answer_in_prompt', 'mean'),\n                mean_f1=('f1', lambda x: x.dropna().mean()),\n                n=('idx', 'count'),\n            )\n            .reset_index()\n        )\n        topk_recall = (\n            per_ds.groupby(['top_k', 'retriever_type'])\n            .agg(recall=('recall', 'mean'), mean_f1=('mean_f1', 'mean'), n=('n', 'sum'))\n            .reset_index()\n        )\n    else:\n        topk_recall = ret_df.groupby(['top_k', 'retriever_type']).agg(\n            recall=('has_answer_in_prompt', 'mean'),\n            mean_f1=('f1', lambda x: x.dropna().mean()),\n            n=('idx', 'count'),\n        ).reset_index()\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    for rt in sorted(topk_recall['retriever_type'].unique()):\n        sub = topk_recall[topk_recall['retriever_type'] == rt].sort_values('top_k')\n        axes[0].plot(sub['top_k'], sub['recall'], 'o-', label=rt)\n        axes[1].plot(sub['top_k'], sub['mean_f1'], 'o-', label=rt)\n\n    axes[0].set_xlabel('Top-K')\n    axes[0].set_ylabel('Retrieval Recall (dataset-stratified)')\n    axes[0].set_title('Retrieval Recall vs Top-K')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n\n    axes[1].set_xlabel('Top-K')\n    axes[1].set_ylabel('Mean F1 (dataset-stratified)')\n    axes[1].set_title('F1 vs Top-K (diminishing returns?)')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key retrieval diagnostics:\n",
    "- **Answer-in-context rate**: What fraction of questions have the answer in retrieved docs\n",
    "- **Bottleneck analysis**: Is performance limited by retrieval or generation\n",
    "- **Reranking value**: How much do rerankers improve answer document positioning\n",
    "- **Retrieval-F1 correlation**: How tightly does retrieval quality predict final F1\n",
    "- **Per-retriever profiles**: Which retrievers work best for which datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}