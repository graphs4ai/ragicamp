{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Profiling\n",
    "\n",
    "Analyze where time is spent in the experiment pipeline.\n",
    "\n",
    "Data sources:\n",
    "- `results.json` → `metadata.timing` (phase + metric durations)\n",
    "- `predictions.json` → `metric_timings` (per-metric durations)\n",
    "- `experiment.log` → parsed for model load/inference split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "STUDY_DIR = Path(\"../outputs/smart_retrieval_slm\")\n",
    "\n",
    "SKIP_DIRS = {\"_archived_fake_reranked\", \"_archived_old_names\", \"_tainted\",\n",
    "             \"_collisions\", \"_incomplete\", \"_quarantined\", \"analysis\",\n",
    "             \"__pycache__\", \".ipynb_checkpoints\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load timing data from results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for exp_dir in sorted(STUDY_DIR.iterdir()):\n",
    "    if not exp_dir.is_dir() or exp_dir.name in SKIP_DIRS or exp_dir.name.startswith((\".\", \"_\")):\n",
    "        continue\n",
    "\n",
    "    results_path = exp_dir / \"results.json\"\n",
    "    if not results_path.exists():\n",
    "        continue\n",
    "\n",
    "    with open(results_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    meta = data.get(\"metadata\", {})\n",
    "    timing = meta.get(\"timing\", {})\n",
    "    phases = timing.get(\"phases\", {})\n",
    "    metric_t = timing.get(\"metrics\", {})\n",
    "\n",
    "    # Also try predictions.json for metric_timings (works for older experiments)\n",
    "    if not metric_t:\n",
    "        preds_path = exp_dir / \"predictions.json\"\n",
    "        if preds_path.exists():\n",
    "            try:\n",
    "                with open(preds_path) as f:\n",
    "                    pdata = json.load(f)\n",
    "                metric_t = pdata.get(\"metric_timings\", {})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    rec = {\n",
    "        \"name\": exp_dir.name,\n",
    "        \"exp_type\": meta.get(\"exp_type\", meta.get(\"type\", \"unknown\")),\n",
    "        \"model\": (meta.get(\"model\", \"\").split(\"/\")[-1] if \"/\" in meta.get(\"model\", \"\") \n",
    "                  else meta.get(\"model\", \"unknown\")),\n",
    "        \"dataset\": meta.get(\"dataset\", \"unknown\"),\n",
    "        \"retriever\": meta.get(\"retriever\", \"none\"),\n",
    "        \"agent_type\": meta.get(\"agent_type\", \"fixed_rag\"),\n",
    "        \"total_s\": data.get(\"duration_seconds\", 0),\n",
    "        \"f1\": data.get(\"metrics\", {}).get(\"f1\", 0),\n",
    "        # Phase timings\n",
    "        \"phase_init\": phases.get(\"init\", 0),\n",
    "        \"phase_generating\": phases.get(\"generating\", 0),\n",
    "        \"phase_generated\": phases.get(\"generated\", 0),\n",
    "        \"phase_metrics\": phases.get(\"computing_metrics\", 0),\n",
    "        \"phase_complete\": phases.get(\"complete\", 0),\n",
    "        # Metric timings\n",
    "        \"metric_f1\": metric_t.get(\"f1\", 0),\n",
    "        \"metric_exact_match\": metric_t.get(\"exact_match\", 0),\n",
    "        \"metric_bertscore\": metric_t.get(\"bertscore\", 0),\n",
    "        \"metric_bleurt\": metric_t.get(\"bleurt\", 0),\n",
    "    }\n",
    "    records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "print(f\"  With timing data: {(df['phase_generating'] > 0).sum()}\")\n",
    "print(f\"  Without timing data: {(df['phase_generating'] == 0).sum()} (pre-profiling runs)\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse experiment.log for model load vs inference split\n",
    "\n",
    "For experiments without structured timing data, fall back to parsing logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_experiment_log(log_path: Path) -> dict:\n",
    "    \"\"\"Extract timing info from experiment.log.\"\"\"\n",
    "    result = {\"model_load_s\": 0, \"inference_s\": 0, \"phase_timings\": {}, \"metric_timings\": {}}\n",
    "    if not log_path.exists():\n",
    "        return result\n",
    "\n",
    "    text = log_path.read_text(errors=\"replace\")\n",
    "\n",
    "    # Generator loaded in 28.0s\n",
    "    m = re.search(r\"Generator loaded in ([\\d.]+)s\", text)\n",
    "    if m:\n",
    "        result[\"model_load_s\"] = float(m.group(1))\n",
    "\n",
    "    # Step [batch_generate] ... completed in 2.5s\n",
    "    m = re.search(r\"Step \\[batch_generate\\].*completed in ([\\d.]+)s\", text)\n",
    "    if m:\n",
    "        result[\"inference_s\"] = float(m.group(1))\n",
    "\n",
    "    # Phase timings: Phase [generating] completed in 32.2s\n",
    "    for m in re.finditer(r\"Phase \\[(\\w+)\\] completed in ([\\d.]+)s\", text):\n",
    "        result[\"phase_timings\"][m.group(1)] = float(m.group(2))\n",
    "\n",
    "    # Metric timings: bertscore done in 15.3s  (new format)\n",
    "    for m in re.finditer(r\"(\\w+) done in ([\\d.]+)s\", text):\n",
    "        result[\"metric_timings\"][m.group(1)] = float(m.group(2))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Enrich dataframe with log-parsed timings for experiments missing structured data\n",
    "log_records = []\n",
    "for _, row in df.iterrows():\n",
    "    log_path = STUDY_DIR / row[\"name\"] / \"experiment.log\"\n",
    "    parsed = parse_experiment_log(log_path)\n",
    "    log_records.append({\n",
    "        \"name\": row[\"name\"],\n",
    "        \"model_load_s\": parsed[\"model_load_s\"],\n",
    "        \"inference_s\": parsed[\"inference_s\"],\n",
    "    })\n",
    "\n",
    "    # Backfill phase timings from log if structured data is missing\n",
    "    if row[\"phase_generating\"] == 0 and parsed[\"phase_timings\"]:\n",
    "        idx = df.index[df[\"name\"] == row[\"name\"]]\n",
    "        for phase, t in parsed[\"phase_timings\"].items():\n",
    "            col = f\"phase_{phase}\"\n",
    "            if col in df.columns:\n",
    "                df.loc[idx, col] = t\n",
    "\n",
    "    # Backfill metric timings from log if missing\n",
    "    if row[\"metric_bertscore\"] == 0 and parsed[\"metric_timings\"]:\n",
    "        idx = df.index[df[\"name\"] == row[\"name\"]]\n",
    "        for metric, t in parsed[\"metric_timings\"].items():\n",
    "            col = f\"metric_{metric}\"\n",
    "            if col in df.columns:\n",
    "                df.loc[idx, col] = t\n",
    "\n",
    "df_logs = pd.DataFrame(log_records)\n",
    "df = df.merge(df_logs, on=\"name\", how=\"left\")\n",
    "\n",
    "print(f\"Log-parsed: {(df['model_load_s'] > 0).sum()} experiments with model load times\")\n",
    "df[[\"name\", \"total_s\", \"model_load_s\", \"inference_s\", \"phase_generating\", \"phase_metrics\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to experiments with timing data\n",
    "timed = df[df[\"phase_generating\"] > 0].copy()\n",
    "\n",
    "if len(timed) == 0:\n",
    "    print(\"No experiments with structured timing data yet.\")\n",
    "    print(\"Run experiments with the updated code to collect timing data.\")\n",
    "    print(\"\\nFalling back to log-parsed data...\")\n",
    "    timed = df[df[\"model_load_s\"] > 0].copy()\n",
    "    if len(timed) > 0:\n",
    "        # Use log-parsed phase timings\n",
    "        timed[\"phase_other\"] = timed[\"total_s\"] - timed[\"phase_generating\"] - timed[\"phase_metrics\"]\n",
    "        timed[\"phase_other\"] = timed[\"phase_other\"].clip(lower=0)\n",
    "\n",
    "if len(timed) > 0:\n",
    "    # Average phase breakdown by experiment type\n",
    "    phase_cols = [\"phase_generating\", \"phase_metrics\", \"phase_init\", \"phase_generated\", \"phase_complete\"]\n",
    "    available_cols = [c for c in phase_cols if c in timed.columns and timed[c].sum() > 0]\n",
    "\n",
    "    by_type = timed.groupby(\"exp_type\")[available_cols + [\"total_s\"]].mean()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar chart: average time per phase\n",
    "    by_type[available_cols].plot.bar(stacked=True, ax=axes[0], colormap=\"Set2\")\n",
    "    axes[0].set_title(\"Average phase breakdown by experiment type\")\n",
    "    axes[0].set_ylabel(\"Time (seconds)\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=0)\n",
    "    axes[0].legend(loc=\"upper left\", fontsize=8)\n",
    "\n",
    "    # Pie chart: overall time allocation\n",
    "    totals = timed[available_cols].sum()\n",
    "    labels = [c.replace(\"phase_\", \"\") for c in available_cols]\n",
    "    axes[1].pie(totals, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "    axes[1].set_title(f\"Overall time allocation ({len(timed)} experiments)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No timing data available. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metric computation breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [c for c in df.columns if c.startswith(\"metric_\") and df[c].sum() > 0]\n",
    "\n",
    "if metric_cols:\n",
    "    m_timed = df[df[metric_cols].sum(axis=1) > 0].copy()\n",
    "    avg_metric = m_timed[metric_cols].mean().sort_values(ascending=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar: average metric computation time\n",
    "    labels = [c.replace(\"metric_\", \"\") for c in avg_metric.index]\n",
    "    axes[0].barh(labels, avg_metric.values, color=\"steelblue\")\n",
    "    axes[0].set_xlabel(\"Time (seconds)\")\n",
    "    axes[0].set_title(f\"Average metric computation time ({len(m_timed)} experiments)\")\n",
    "    for i, v in enumerate(avg_metric.values):\n",
    "        axes[0].text(v + 0.2, i, f\"{v:.1f}s\", va=\"center\", fontsize=9)\n",
    "\n",
    "    # Proportion pie\n",
    "    totals = m_timed[metric_cols].sum()\n",
    "    labels_pie = [c.replace(\"metric_\", \"\") for c in totals.index]\n",
    "    axes[1].pie(totals, labels=labels_pie, autopct=\"%1.1f%%\", startangle=90)\n",
    "    axes[1].set_title(\"Metric time share\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No per-metric timing data yet. Run experiments with updated code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model load vs inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_load = df[(df[\"model_load_s\"] > 0) & (df[\"inference_s\"] > 0)].copy()\n",
    "\n",
    "if len(with_load) > 0:\n",
    "    with_load[\"overhead_s\"] = (\n",
    "        with_load[\"phase_generating\"] - with_load[\"model_load_s\"] - with_load[\"inference_s\"]\n",
    "    ).clip(lower=0)\n",
    "\n",
    "    by_model = with_load.groupby(\"model\")[[\"model_load_s\", \"inference_s\", \"overhead_s\"]].mean()\n",
    "    by_model = by_model.sort_values(\"model_load_s\", ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(4, len(by_model) * 0.6)))\n",
    "    by_model.plot.barh(stacked=True, ax=ax, color=[\"#e74c3c\", \"#2ecc71\", \"#95a5a6\"])\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_title(\"Generation phase breakdown by model\")\n",
    "    ax.legend([\"Model load\", \"Inference\", \"Other overhead\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary table\n",
    "    summary = with_load.groupby(\"model\").agg(\n",
    "        count=(\"name\", \"count\"),\n",
    "        avg_load=(\"model_load_s\", \"mean\"),\n",
    "        avg_infer=(\"inference_s\", \"mean\"),\n",
    "        avg_total=(\"total_s\", \"mean\"),\n",
    "    ).round(1)\n",
    "    summary[\"load_pct\"] = (summary[\"avg_load\"] / summary[\"avg_total\"] * 100).round(1)\n",
    "    print(\"\\nModel load as % of total time:\")\n",
    "    display(summary.sort_values(\"avg_load\", ascending=False))\n",
    "else:\n",
    "    print(\"No model load timing data found in experiment logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time per experiment type and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # By experiment type\n",
    "    by_type = df.groupby(\"exp_type\")[\"total_s\"].agg([\"mean\", \"count\"])\n",
    "    by_type[\"mean\"].plot.bar(ax=axes[0], color=\"steelblue\")\n",
    "    axes[0].set_title(\"Average total time by experiment type\")\n",
    "    axes[0].set_ylabel(\"Time (seconds)\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=0)\n",
    "    for i, (_, row) in enumerate(by_type.iterrows()):\n",
    "        axes[0].text(i, row[\"mean\"] + 1, f\"n={int(row['count'])}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "    # By model\n",
    "    by_model = df.groupby(\"model\")[\"total_s\"].agg([\"mean\", \"count\"]).sort_values(\"mean\")\n",
    "    by_model[\"mean\"].plot.barh(ax=axes[1], color=\"coral\")\n",
    "    axes[1].set_title(\"Average total time by model\")\n",
    "    axes[1].set_xlabel(\"Time (seconds)\")\n",
    "    for i, (_, row) in enumerate(by_model.iterrows()):\n",
    "        axes[1].text(row[\"mean\"] + 1, i, f\"n={int(row['count'])}\", va=\"center\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bottleneck summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BOTTLENECK SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    total_hours = df[\"total_s\"].sum() / 3600\n",
    "    print(f\"\\nTotal compute time: {total_hours:.1f} hours across {len(df)} experiments\")\n",
    "    print(f\"Average per experiment: {df['total_s'].mean():.0f}s\")\n",
    "    print(f\"Median per experiment: {df['total_s'].median():.0f}s\")\n",
    "\n",
    "    if df[\"phase_generating\"].sum() > 0:\n",
    "        gen_total = df[\"phase_generating\"].sum()\n",
    "        metrics_total = df[\"phase_metrics\"].sum()\n",
    "        total = gen_total + metrics_total\n",
    "        if total > 0:\n",
    "            print(f\"\\nPhase split (of generation + metrics):\")\n",
    "            print(f\"  Generation: {gen_total/3600:.1f}h ({gen_total/total*100:.0f}%)\")\n",
    "            print(f\"  Metrics:    {metrics_total/3600:.1f}h ({metrics_total/total*100:.0f}%)\")\n",
    "\n",
    "    if df[\"model_load_s\"].sum() > 0:\n",
    "        load_total = df[\"model_load_s\"].sum()\n",
    "        infer_total = df[\"inference_s\"].sum()\n",
    "        print(f\"\\nGeneration breakdown (from logs):\")\n",
    "        print(f\"  Model loading: {load_total/3600:.1f}h ({load_total/df['total_s'].sum()*100:.0f}% of total)\")\n",
    "        print(f\"  Inference:     {infer_total/3600:.1f}h ({infer_total/df['total_s'].sum()*100:.0f}% of total)\")\n",
    "\n",
    "    if metric_cols:\n",
    "        m_timed = df[df[metric_cols].sum(axis=1) > 0]\n",
    "        if len(m_timed) > 0:\n",
    "            print(f\"\\nMetric breakdown (avg per experiment):\")\n",
    "            for col in metric_cols:\n",
    "                avg = m_timed[col].mean()\n",
    "                if avg > 0:\n",
    "                    name = col.replace(\"metric_\", \"\")\n",
    "                    print(f\"  {name:20s} {avg:6.1f}s\")\n",
    "\n",
    "    # Slowest experiments\n",
    "    print(f\"\\nSlowest 5 experiments:\")\n",
    "    for _, row in df.nlargest(5, \"total_s\").iterrows():\n",
    "        print(f\"  {row['name']:45s} {row['total_s']:6.0f}s ({row['exp_type']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
