{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Failure Analysis\n",
    "\n",
    "This notebook analyzes failed experiments to identify patterns and root causes.\n",
    "\n",
    "Common failure modes:\n",
    "- **Context length exceeded**: Hierarchical retrieval + high top_k + small context models\n",
    "- **OOM errors**: GPU memory exhaustion during generation\n",
    "- **Partial completions**: Experiments that started but didn't finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_failed_experiments,\n",
    "    analyze_failure_patterns,\n",
    "    get_experiment_health_summary,\n",
    "    predict_context_length_issues,\n",
    "    DEFAULT_STUDY_PATH,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f\"Study path: {DEFAULT_STUDY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Health Summary\n",
    "\n",
    "Quick overview of experiment status across the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = get_experiment_health_summary()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERIMENT HEALTH SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in health.items():\n",
    "    print(f\"{key:20}: {value}\")\n",
    "\n",
    "# Calculate percentages\n",
    "if health.get('total_experiments', 0) > 0:\n",
    "    total = health['total_experiments']\n",
    "    print(\"\\n--- Percentages ---\")\n",
    "    print(f\"Complete:    {health.get('complete', 0) / total * 100:.1f}%\")\n",
    "    print(f\"Failed:      {health.get('failed', 0) / total * 100:.1f}%\")\n",
    "    print(f\"In Progress: {health.get('in_progress', 0) / total * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Failed Experiments\n",
    "\n",
    "Load all experiments that have `phase: failed` in their state.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = load_failed_experiments()\n",
    "\n",
    "print(f\"Total failed experiments: {len(failed_df)}\")\n",
    "\n",
    "if not failed_df.empty:\n",
    "    display(failed_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Failure Pattern Analysis\n",
    "\n",
    "Categorize failures by type and identify systematic issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    patterns = analyze_failure_patterns(failed_df)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"FAILURE PATTERNS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total failed: {patterns['total_failed']}\")\n",
    "    print(f\"Context length issues: {patterns['context_length_issues']}\")\n",
    "    print(f\"OOM issues: {patterns['oom_issues']}\")\n",
    "    print(f\"Partial completions: {len(patterns['partial_completions'])}\")\n",
    "else:\n",
    "    print(\"No failed experiments found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Failures by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    model_failures = failed_df['model_short'].value_counts()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    model_failures.plot(kind='bar', ax=ax, color='coral')\n",
    "    ax.set_title('Failures by Model')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFailures by model:\")\n",
    "    print(model_failures.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Failures by Retriever Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    retriever_failures = failed_df['retriever_type'].value_counts()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    retriever_failures.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_title('Failures by Retriever Type')\n",
    "    ax.set_xlabel('Retriever Type')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFailures by retriever:\")\n",
    "    print(retriever_failures.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Failures by Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    topk_failures = failed_df['top_k'].value_counts().sort_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    topk_failures.plot(kind='bar', ax=ax, color='forestgreen')\n",
    "    ax.set_title('Failures by Top-K')\n",
    "    ax.set_xlabel('Top-K')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFailures by top_k:\")\n",
    "    print(topk_failures.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cross-tabulation: Model × Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty and len(failed_df) > 5:\n",
    "    cross_tab = pd.crosstab(failed_df['model_short'], failed_df['retriever_type'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(cross_tab, annot=True, fmt='d', cmap='Reds', ax=ax)\n",
    "    ax.set_title('Failure Heatmap: Model × Retriever')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCross-tabulation:\")\n",
    "    display(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Messages\n",
    "\n",
    "Examine the actual error messages to understand root causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    # Group by error message (truncated for readability)\n",
    "    failed_df['error_short'] = failed_df['error'].str[:80]\n",
    "    error_counts = failed_df['error_short'].value_counts()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ERROR MESSAGE FREQUENCY\")\n",
    "    print(\"=\" * 50)\n",
    "    for error, count in error_counts.head(10).items():\n",
    "        print(f\"\\n[{count}x] {error}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    # Show full error messages for unique errors\n",
    "    print(\"\\nUnique error messages:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, error in enumerate(failed_df['error'].unique()[:5]):\n",
    "        print(f\"\\n[Error {i+1}]\")\n",
    "        print(error[:500] if len(error) > 500 else error)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Length Risk Prediction\n",
    "\n",
    "Identify experiments that are likely to fail due to context length limits.\n",
    "\n",
    "**Estimation:**\n",
    "- Dense/Hybrid: ~512 tokens/doc\n",
    "- Hierarchical: ~2048 tokens/doc (parent chunks)\n",
    "- Prompt overhead: ~200 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risky_df = predict_context_length_issues()\n",
    "\n",
    "print(f\"Experiments at risk of context length failure: {len(risky_df)}\")\n",
    "\n",
    "if not risky_df.empty:\n",
    "    display(risky_df.sort_values('headroom_pct'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not risky_df.empty:\n",
    "    # Group by model to see which models are most affected\n",
    "    model_risk = risky_df.groupby('model').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nRisky experiments by model:\")\n",
    "    print(model_risk.to_string())\n",
    "    \n",
    "    # Group by retriever\n",
    "    retriever_risk = risky_df.groupby('retriever_type').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nRisky experiments by retriever:\")\n",
    "    print(retriever_risk.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partial Completions\n",
    "\n",
    "Experiments that started but didn't finish - may be recoverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    partial = failed_df[failed_df['predictions_complete'] > 0].copy()\n",
    "    \n",
    "    if not partial.empty:\n",
    "        partial['completion_pct'] = partial['predictions_complete'] / partial['total_questions'] * 100\n",
    "        partial = partial.sort_values('completion_pct', ascending=False)\n",
    "        \n",
    "        print(f\"Partial completions: {len(partial)}\")\n",
    "        display(partial[['name', 'model_short', 'predictions_complete', 'total_questions', 'completion_pct', 'error']].head(20))\n",
    "    else:\n",
    "        print(\"No partial completions found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations\n",
    "\n",
    "Based on the failure analysis, here are recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not failed_df.empty:\n",
    "    # Check for context length issues\n",
    "    context_fails = failed_df[failed_df['error'].str.contains('context|length|token|truncat', case=False, na=False)]\n",
    "    if len(context_fails) > 0:\n",
    "        print(\"\\n1. CONTEXT LENGTH ISSUES DETECTED\")\n",
    "        print(f\"   {len(context_fails)} experiments failed due to context length.\")\n",
    "        print(\"   Recommended actions:\")\n",
    "        print(\"   - Exclude hierarchical retriever for Phi-3-mini-4k (4K context)\")\n",
    "        print(\"   - Reduce top_k for hierarchical retrieval\")\n",
    "        print(\"   - Use Phi-3-mini-128k variant if available\")\n",
    "    \n",
    "    # Check for OOM issues\n",
    "    oom_fails = failed_df[failed_df['error'].str.contains('OOM|out of memory|CUDA', case=False, na=False)]\n",
    "    if len(oom_fails) > 0:\n",
    "        print(\"\\n2. GPU MEMORY ISSUES DETECTED\")\n",
    "        print(f\"   {len(oom_fails)} experiments failed due to OOM.\")\n",
    "        print(\"   Recommended actions:\")\n",
    "        print(\"   - Reduce batch_size in config\")\n",
    "        print(\"   - Use quantization (AWQ, GPTQ)\")\n",
    "        print(\"   - Increase GPU memory or use tensor parallelism\")\n",
    "    \n",
    "    # Check for model-specific failures\n",
    "    model_fail_pct = failed_df['model_short'].value_counts() / len(failed_df) * 100\n",
    "    high_fail_models = model_fail_pct[model_fail_pct > 30]\n",
    "    if len(high_fail_models) > 0:\n",
    "        print(\"\\n3. HIGH-FAILURE MODELS\")\n",
    "        for model, pct in high_fail_models.items():\n",
    "            print(f\"   - {model}: {pct:.1f}% of all failures\")\n",
    "        print(\"   Consider investigating these models specifically.\")\n",
    "else:\n",
    "    print(\"\\nNo failed experiments - all experiments completed successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Failed Experiments\n",
    "\n",
    "Export the list of failed experiments for further investigation or re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not failed_df.empty:\n",
    "    output_path = DEFAULT_STUDY_PATH / 'failed_experiments.csv'\n",
    "    failed_df.to_csv(output_path, index=False)\n",
    "    print(f\"Exported failed experiments to: {output_path}\")\n",
    "    \n",
    "    # Also export just the names for easy re-running\n",
    "    names_path = DEFAULT_STUDY_PATH / 'failed_experiment_names.txt'\n",
    "    with open(names_path, 'w') as f:\n",
    "        for name in failed_df['name']:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    print(f\"Exported experiment names to: {names_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
