[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "ragicamp"
version = "0.4.0"
description = "A modular framework for experimenting with RAG approaches"
readme = "README.md"
requires-python = ">=3.9"
authors = [
    {name = "RAGiCamp Team"}
]
dependencies = [
    "numpy>=1.21.0,<2.0.0", # Pin to <2.0 for TensorFlow/Transformers compatibility
    "pandas>=1.3.0",
    "torch>=2.0.0",
    "transformers>=4.30.0",
    "datasets>=2.10.0",
    "faiss-cpu>=1.7.4",
    "sentence-transformers>=2.2.0",
    "openai>=1.0.0",
    "tiktoken>=0.5.0",
    "pydantic>=2.0.0",
    "pyyaml>=6.0",
    "tqdm>=4.65.0",
    "scikit-learn>=1.2.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0", # For 8-bit quantization
    "bert-score>=0.3.13",
    "bleurt-pytorch>=0.0.1",  # PyTorch BLEURT (no TensorFlow dependency)
    # Experiment tracking and optimization
    "mlflow>=2.8.0", # Experiment tracking and model registry
    "ragas>=0.1.0", # RAG-specific evaluation metrics
    "optuna>=3.0.0", # Hyperparameter optimization
    # Configuration management
    "hydra-core>=1.3.0", # Composable config system
    "omegaconf>=2.3.0", # Config management for Hydra
    "ipykernel>=6.31.0",
    "seaborn>=0.13.2",
    "ruff>=0.4.0",
    "pytest>=8.4.2",
    "boto3>=1.42.39",
    "awscli>=1.30.0",  # For B2 backup download script
]

[project.scripts]
ragicamp = "ragicamp.cli.main:main"

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "ruff>=0.4.0",
    "mypy>=1.0.0",
]
viz = [
    "matplotlib>=3.5.0",
    "seaborn>=0.12.0",
    "wandb>=0.15.0",
]
# vLLM backend for efficient inference with PagedAttention
# Recommended for long context without weight quantization
vllm = [
    "vllm>=0.6.0",
]
# FlashAttention-2 for memory-efficient attention (O(n) instead of O(nÂ²))
# NOTE: flash-attn has complex build requirements (CUDA toolkit with nvcc >= 11.7)
# Install manually if needed: pip install flash-attn --no-build-isolation
# The code gracefully falls back to standard attention if not installed.
#
# All GPU optimizations (vLLM only - flash-attn excluded due to build complexity)
# NOTE: For GPU FAISS (10-100x faster similarity search), replace faiss-cpu:
#   uv pip uninstall faiss-cpu && uv pip install faiss-gpu-cu12>=1.7.4
# (or faiss-gpu-cu11>=1.7.4 for CUDA 11.x)
gpu = [
    "vllm>=0.6.0",
]
# Advanced RAG features (reranking, BM25)
# Note: Cross-encoders are included in sentence-transformers
rag = [
    "rank-bm25>=0.2.2",  # Proper BM25 implementation for hybrid retrieval
]

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["src/ragicamp"]

[tool.ruff]
line-length = 100
target-version = "py39"

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # Pyflakes
    "I",      # isort
    "UP",     # pyupgrade
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
]
ignore = [
    "E501",   # line too long (handled by formatter)
    "B008",   # do not perform function calls in argument defaults
]

[tool.ruff.lint.isort]
known-first-party = ["ragicamp"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false

[tool.uv.sources]
# bleurt-pytorch is from PyPI, no custom source needed
