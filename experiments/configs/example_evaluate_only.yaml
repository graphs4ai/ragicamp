# Example: Evaluate existing predictions (metrics only)
# Use this to compute metrics on predictions that were generated earlier
# Useful for retrying after API failures or adding new metrics

# Note: agent/model/dataset not needed for evaluate mode
# They're inferred from the predictions file

evaluation:
  mode: evaluate  # Only compute metrics on existing predictions
  predictions_file: "outputs/gemma_2b_baseline_generate_predictions_raw.json"

# Judge model for LLM-based metrics (optional)
judge_model:
  type: openai
  model_name: "gpt-4o"  # or "gpt-4o-mini" for budget option

metrics:
  - exact_match
  - f1
  - bertscore
  - bleurt
  - llm_judge_qa  # Requires judge_model above

output:
  save_predictions: true
  output_path: "outputs/nq_baseline_with_metrics.json"

