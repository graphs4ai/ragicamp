# Configuration for MDP-based RAG
# Uses Q-learning to learn optimal sequential RAG actions
# EXPERIMENTAL: For research/development use

agent:
  type: mdp_rag
  name: "mdp_qlearning_rag"
  max_steps: 5
  system_prompt: "You are a helpful assistant. Use the provided context to answer questions accurately."

model:
  type: huggingface
  model_name: "google/flan-t5-base"
  device: "cuda"
  max_tokens: 128
  temperature: 0.7

retriever:
  type: dense
  name: "wikipedia_dense"
  embedding_model: "all-MiniLM-L6-v2"
  index_type: "flat"

policy:
  type: qlearning
  name: "qlearning_policy"
  learning_rate: 0.1
  discount_factor: 0.95
  epsilon: 0.1
  action_types:
    - retrieve
    - reformulate
    - generate

dataset:
  name: natural_questions
  split: train
  num_examples: 1000

training:
  num_epochs: 1
  eval_interval: 100

metrics:
  - exact_match
  - f1

output:
  save_predictions: true
  save_policy: true
  output_path: "outputs/mdp_rag_results.json"
  policy_path: "outputs/mdp_policy.json"
