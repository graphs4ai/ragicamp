# Natural Questions - Baseline with LLM Judge (Ternary Classification)
# Evaluates answers using GPT-4 for ternary classification (correct/partial/incorrect)
# ~5 minutes on GPU (with batch processing)

agent:
  type: direct_llm
  name: "gemma_2b_with_llm_judge_ternary"
  system_prompt: "You are a helpful AI assistant. Answer questions accurately and concisely based on your knowledge."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"
  device: "cuda"
  load_in_8bit: true

dataset:
  name: natural_questions
  split: validation
  num_examples: 20  # Start small due to API costs
  filter_no_answer: true

# Batch processing for faster evaluation
evaluation:
  batch_size: 8

# LLM Judge Model Configuration
judge_model:
  type: openai
  model_name: "gpt-4o"  # or "gpt-4o-mini" for lower cost
  temperature: 0.0  # Deterministic judgments

metrics:
  - exact_match
  - f1
  - name: llm_judge_qa
    params:
      judgment_type: "ternary"  # ternary: correct/partial/incorrect
      batch_size: 16  # Process 16 judgments in parallel for speed

output:
  save_predictions: true
  output_path: "outputs/nq_baseline_with_llm_judge_ternary.json"
