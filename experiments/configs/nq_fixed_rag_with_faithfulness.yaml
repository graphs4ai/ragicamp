# Natural Questions - Fixed RAG with Faithfulness Metrics
# Evaluates both correctness AND groundedness (hallucination detection)
# REQUIRES: Pre-indexed retriever artifact (run: make index-wiki-small)
# ~30 minutes on GPU

agent:
  type: fixed_rag
  name: "gemma_2b_fixed_rag_faithful"
  top_k: 5
  system_prompt: "You are a helpful assistant. Use ONLY the provided context to answer questions. Do not use outside knowledge."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"
  device: "cuda"
  load_in_8bit: true
  max_tokens: 128
  temperature: 0.3  # Lower temp for more grounded answers

retriever:
  type: dense
  name: "wikipedia_retriever"
  embedding_model: "all-MiniLM-L6-v2"
  index_type: "flat"
  # Load pre-indexed artifact with:
  # retriever = DenseRetriever.load_index("wikipedia_small")

dataset:
  name: natural_questions
  split: validation
  num_examples: 50  # 50 examples for RAG eval
  filter_no_answer: true

# Batch processing for faster evaluation
evaluation:
  batch_size: 8

metrics:
  # Correctness metrics
  - exact_match
  - f1
  
  # Semantic similarity
  - name: bertscore
    params:
      model_type: "microsoft/deberta-base-mnli"
  
  # RAG-specific: Groundedness/Faithfulness
  - name: faithfulness
    params:
      method: "nli"  # Options: "nli", "token_overlap", "llm"
      nli_model: "microsoft/deberta-base-mnli"
      threshold: 0.5
  
  # RAG-specific: Hallucination detection
  - name: hallucination
    params:
      method: "nli"  # Options: "nli", "simple"
      nli_model: "microsoft/deberta-base-mnli"
      threshold: 0.5

output:
  save_predictions: true
  output_path: "outputs/nq_fixed_rag_faithful.json"

# Notes:
# - Faithfulness: Measures if answer is supported by retrieved docs (0-1, higher=better)
# - Hallucination: Measures unsupported claims (0-1, lower=better)
# - These metrics only make sense with RAG (require context)
# - NLI method uses transformer models, slower but accurate
# - For faster eval, use method: "simple" or "token_overlap"
