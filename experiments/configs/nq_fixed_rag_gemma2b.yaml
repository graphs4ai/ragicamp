# Natural Questions - Fixed RAG with Gemma 2B
# Uses pre-indexed CHUNKED Wikipedia corpus for better retrieval
# Memory-optimized: 8-bit quantization, lightweight metrics
# Run: make index-wiki-small-chunked (first time only)
# ~5-10 minutes on GPU for 100 examples

agent:
  type: fixed_rag
  name: "gemma_2b_fixed_rag"
  top_k: 5                    # Reduced to save memory (fewer docs = shorter prompt)
  system_prompt: "You are a helpful assistant. Use the provided context to answer questions accurately and concisely."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"  # Small but powerful!
  device: "cuda"
  load_in_8bit: true          # 8-bit is fine for smaller model
  max_tokens: 128
  trust_remote_code: true     # Required for Gemma 2B

# Pre-indexed Wikipedia corpus (run: make index-wiki-small-chunked)
retriever:
  type: dense
  name: "wikipedia_retriever"
  embedding_model: "all-MiniLM-L6-v2"
  index_type: "flat"
  artifact_path: "wikipedia_small_chunked"  # Chunked corpus for better retrieval
  # Chunking config (used during indexing, stored here for reference)
  chunking:
    strategy: "recursive"      # Best overall (LangChain-style hierarchical splitting)
    chunk_size: 512            # Target chunk size in characters
    chunk_overlap: 50          # Overlap to preserve context between chunks

dataset:
  name: natural_questions
  split: validation
  num_examples: 100            # Reduced for testing
  filter_no_answer: true

# Evaluation settings
evaluation:
  mode: both
  batch_size: 1               # Sequential processing to avoid OOM
  checkpoint_every: 10        # Save checkpoint every 10 questions
  resume_from_checkpoint: true  # Resume from checkpoint if exists
  retry_failures: true        # Retry failed questions on resume

# LLM Judge Model Configuration
judge_model:
  type: openai
  model_name: "gpt-4o-mini"  # Budget-friendly option
  temperature: 0.0


# Metrics: Using lightweight metrics only (BERTScore/BLEURT cause OOM)
# To add heavy metrics later, use the two-phase pipeline approach
metrics:
  - exact_match
  - f1
  # LLM judge uses OpenAI API (no GPU needed)
  - name: llm_judge_qa
    params:
      judgment_type: "binary"
      batch_size: 16

output:
  save_predictions: true
  output_path: "outputs/nq_fixed_rag_gemma2b.json"