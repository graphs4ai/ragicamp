# Natural Questions - Fixed RAG with Gemma 2B
# REQUIRES: Pre-indexed retriever artifact (run: make index-wiki-small)
# ~20 minutes on GPU

agent:
  type: fixed_rag
  name: "gemma_2b_fixed_rag"
  top_k: 5
  system_prompt: "You are a helpful assistant. Use the provided context to answer questions accurately."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"
  device: "cuda"
  load_in_8bit: true

retriever:
  type: dense
  name: "wikipedia_retriever"
  embedding_model: "all-MiniLM-L6-v2"
  index_type: "flat"
  # Note: Load pre-indexed artifact with:
  # retriever = DenseRetriever.load_index("wikipedia_small")

dataset:
  name: natural_questions
  split: validation
  num_examples: 100
  filter_no_answer: true

# Batch processing for faster evaluation
evaluation:
  batch_size: 8

metrics:
  - exact_match
  - f1
  - name: bertscore
    params:
      model_type: "microsoft/deberta-base-mnli"
  - name: bleurt
    params:
      checkpoint: "BLEURT-20-D3"

output:
  save_predictions: true
  output_path: "outputs/nq_fixed_rag_gemma2b.json"
