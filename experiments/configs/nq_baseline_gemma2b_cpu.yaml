# Natural Questions - Baseline (DirectLLM) with Gemma 2B - CPU Version
# Quick test on CPU: 10 examples with basic metrics
# ~10-15 minutes on CPU (slower than GPU)

agent:
  type: direct_llm
  name: "gemma_2b_baseline_cpu"
  system_prompt: "You are a helpful AI assistant. Answer questions accurately and concisely based on your knowledge."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"
  device: "cpu"
  load_in_8bit: false  # 8-bit quantization not supported on CPU

dataset:
  name: natural_questions
  split: validation
  num_examples: 10  # Keep small for CPU (slower)
  filter_no_answer: true

# Evaluation settings
evaluation:
  mode: both      # generate (predictions only), evaluate (metrics only), or both
  batch_size: 2   # Small batch for CPU
  checkpoint_every: 10        # Save checkpoint every 10 questions (CPU slower)
  resume_from_checkpoint: true  # Resume from checkpoint if exists
  retry_failures: true        # Retry failed questions on resume

metrics:
  - exact_match
  - f1

output:
  save_predictions: true
  output_path: "outputs/nq_baseline_gemma2b_cpu.json"
