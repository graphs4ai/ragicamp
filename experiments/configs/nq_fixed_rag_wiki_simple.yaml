# Natural Questions - Fixed RAG with Wikipedia Simple Corpus
# Uses full Simple Wikipedia (~200k articles) for comprehensive coverage
# 
# PREREQUISITES:
#   1. Index corpus first: make index-wiki-simple
#      (Takes 30-60 minutes, but only needed once)
#   2. Ensure you have GPU with sufficient memory (~16GB recommended)
#
# Run: uv run python experiments/scripts/run_experiment.py --config experiments/configs/nq_fixed_rag_wiki_simple.yaml

agent:
  type: fixed_rag
  name: "gemma_2b_fixed_rag_wiki_simple"
  top_k: 10                     # Retrieve top 5 most relevant documents (reduced for memory)
  system_prompt: "You are a helpful assistant. Use the provided context to answer questions accurately and concisely. If the context doesn't contain the answer, say you don't know."

model:
  type: huggingface
  model_name: "google/gemma-2-2b-it"  # Efficient 2B parameter model
  device: "cuda"              # Use GPU (change to "cpu" if no GPU available)
  load_in_4bit: true          # 4-bit quantization to reduce memory usage
  max_tokens: 64              # Maximum tokens in generated answer (shorter = less memory)
  temperature: 0.0            # Deterministic generation for evaluation
  trust_remote_code: true     # Required for Gemma models

# Dense retriever with Wikipedia Simple corpus
retriever:
  type: dense                 # Use dense embeddings (vs sparse/BM25)
  name: "wikipedia_simple_retriever"
  embedding_model: "all-MiniLM-L6-v2"  # Fast, lightweight sentence embeddings
  index_type: "flat"          # FAISS flat index (exact search)
  artifact_path: "wikipedia_simple_chunked_1024_overlap_128"  # Pre-indexed Simple Wikipedia corpus
  # Note: Index must exist before running. Create with: make index-wiki-simple

dataset:
  name: natural_questions     # Natural Questions from Google
  split: validation           # Use validation split for evaluation
  num_examples: 100           # Number of questions to evaluate
  filter_no_answer: true      # Filter out questions with no answer

# Evaluation settings
evaluation:
  mode: both                  # Generate predictions AND compute metrics
  batch_size: 1               # Process 1 question at a time (sequential to avoid OOM)
  checkpoint_every: 10        # Save checkpoint every 10 questions (enables resume)
  resume_from_checkpoint: true  # Auto-resume from last checkpoint if it exists
  retry_failures: true        # Retry previously failed questions on resume
  # Alternative modes:
  #   generate: Only generate predictions (safe for large evaluations)
  #   evaluate: Only compute metrics on existing predictions

# Metrics for evaluation
metrics:
  - exact_match               # Binary match (case-insensitive)
  - f1                        # Token-level F1 score
  # bertscore removed - uses ~2-3GB GPU memory (can compute later with two-phase eval)
  # Add more if needed:
  # - bleurt                  # Learned metric (slow but high quality)
  - name: llm_judge_qa      # GPT-4 evaluation (requires OPENAI_API_KEY)
    params:
      judgment_type: "binary"
      batch_size: 16

# Optional: LLM Judge (uncomment to enable GPT-4 evaluation)
# Requires: export OPENAI_API_KEY='your-key-here'
judge_model:
  type: openai
  model_name: "gpt-4o-mini"  # Budget-friendly: $0.05-0.10 per 100 questions
  temperature: 0.0

output:
  save_predictions: true
  output_path: "outputs/nq_fixed_rag_wiki_simple.json"

# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# 1. First time - Index the corpus (only once):
#    make index-wiki-simple
#    (Takes 30-60 minutes, creates artifacts/retrievers/wikipedia_simple_full/)
#
# 2. Run the experiment:
#    uv run python experiments/scripts/run_experiment.py \
#      --config experiments/configs/nq_fixed_rag_wiki_simple.yaml
#
# 3. Check results:
#    cat outputs/nq_fixed_rag_wiki_simple_summary.json
#
# 4. For CPU (slower but no GPU needed):
#    Change: device: "cpu" and load_in_8bit: false
#
# 5. For faster testing:
#    Change: num_examples: 10
#
# ============================================================================
# EXPECTED PERFORMANCE
# ============================================================================
#
# Time: ~15-20 minutes for 100 examples on GPU
# Memory: ~8-12GB GPU memory with 8-bit quantization
# Metrics: EM ~0.35-0.45, F1 ~0.45-0.55 (depends on corpus quality)
#
# ============================================================================

