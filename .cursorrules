# RAGiCamp - Cursor AI Rules

## Project Overview

RAGiCamp is a modular, production-ready framework for Retrieval-Augmented Generation (RAG) experimentation. It supports multiple RAG strategies from simple baselines to complex RL-based adaptive agents.

**Tech Stack:** Python 3.9+, PyTorch, HuggingFace Transformers, FAISS, Sentence Transformers

**Key Goal:** Clean abstractions, reusable components, easy experimentation

---

## Architecture Principles

### 1. Clean Abstractions (SOLID)

- **Base classes** define interfaces via ABC
- **Implementations** are fully substitutable (LSP)
- **Single responsibility** - each component does one thing well
- **Dependency injection** - agents receive models/retrievers, not create them

### 2. Type Safety

- **Always use type hints**
- **Proper dataclasses** - Use `Document`, not `Dict[str, Any]`
- **List[Document]** not `List[dict]` for retrieved docs
- **Dict[str, float]** for all metric returns (standardized)

### 3. Separation of Concerns

```
agents/      - Decision logic (answer questions)
models/      - LLM interfaces (generate text)
retrievers/  - Document retrieval (find relevant docs)
corpus/      - Document sources (NO answers! prevents data leakage)
datasets/    - QA datasets (questions + answers for evaluation)
policies/    - Action selection (for adaptive agents)
metrics/     - Evaluation (compute scores)
training/    - Training loops
evaluation/  - Evaluation orchestration
config/      - Experiment configuration (dataclasses + YAML)
output/      - Output management (organized results)
utils/       - Shared utilities
```

**Critical Distinction:**
- `corpus/` = Documents for retrieval (Wikipedia, PubMed) - NO ANSWER INFORMATION
- `datasets/` = QA pairs for evaluation (Natural Questions, TriviaQA) - WITH ANSWERS

---

## Code Patterns

### DocumentCorpus Pattern (NEW)

**Purpose:** Provide documents for retrieval WITHOUT answer information (prevents data leakage)

```python
from ragicamp.corpus import DocumentCorpus, CorpusConfig

# Always use configuration
config = CorpusConfig(
    name="wikipedia_simple",
    source="wikimedia/wikipedia",
    version="20231101.simple",
    max_docs=10000  # Optional limit
)

# Load corpus
corpus = WikipediaCorpus(config)

# Iterate documents - NO ANSWERS!
for doc in corpus.load():
    # doc.text contains article text
    # doc.metadata may have title, url, etc
    # doc.metadata MUST NOT have answers
    pass
```

**Rules:**
- âŒ NEVER include answer information in corpus documents
- âœ… Use for indexing retrievers
- âœ… Separate from QA datasets
- âœ… One corpus can serve multiple datasets

### ExperimentConfig Pattern (NEW)

**Purpose:** Single source of truth for experiment configuration

```python
from ragicamp.config import ExperimentConfig

# Load from YAML
config = ExperimentConfig.from_yaml("configs/my_experiment.yaml")

# Access typed configuration
print(config.corpus.name)
print(config.retriever.top_k)
print(config.model.load_in_8bit)

# Save configuration
config.to_yaml("outputs/experiment_v1/config.yaml")

# Or create programmatically
from ragicamp.config import create_fixed_rag_config
config = create_fixed_rag_config(dataset="natural_questions")
```

**Rules:**
- âœ… Use dataclasses, not dictionaries
- âœ… All experiments have a config file
- âœ… Config saved with results for reproducibility
- âŒ Don't hard-code values in scripts

### OutputManager Pattern (NEW)

**Purpose:** Organize experiment results cleanly

```python
from ragicamp.output import OutputManager

mgr = OutputManager()

# Create experiment directory
exp_dir = mgr.create_experiment_dir("my_experiment_v1")

# Save experiment
mgr.save_experiment(config, results, exp_dir)

# List experiments
experiments = mgr.list_experiments(dataset="natural_questions")

# Compare experiments
mgr.compare_experiments(["exp1", "exp2", "exp3"])
mgr.print_comparison(["exp1", "exp2"], metrics=["exact_match", "f1"])
```

**Directory Structure:**
```
outputs/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ experiment_name/
â”‚   â”‚   â”œâ”€â”€ config.yaml        # Full config
â”‚   â”‚   â”œâ”€â”€ metadata.json      # Timestamp, git hash, etc
â”‚   â”‚   â”œâ”€â”€ contexts.json      # Retrieved contexts (if RAG)
â”‚   â”‚   â”œâ”€â”€ predictions.json   # All predictions
â”‚   â”‚   â””â”€â”€ results.json       # Metrics summary
â””â”€â”€ comparisons/
    â””â”€â”€ nq_comparison.json
```

### Agent Pattern

```python
class MyAgent(RAGAgent):
    def __init__(self, name: str, model: LanguageModel, **kwargs):
        super().__init__(name, **kwargs)
        self.model = model
        self.prompt_builder = PromptBuilder.create_default()
    
    def answer(self, query: str, **kwargs: Any) -> RAGResponse:
        # 1. Create context
        context = RAGContext(query=query)
        
        # 2. Do work (retrieve, format, generate)
        answer = self.model.generate(query)
        
        # 3. Return proper response
        return RAGResponse(answer=answer, context=context)
```

### Retriever Pattern

```python
class MyRetriever(Retriever):
    def retrieve(self, query: str, top_k: int = 5, **kwargs) -> List[Document]:
        # Return List[Document], not List[dict]
        return [Document(id=..., text=..., metadata={}, score=...)]
    
    def index_documents(self, documents: List[Document]) -> None:
        # Build index
        pass
```

### Metric Pattern

```python
class MyMetric(Metric):
    def compute(
        self, 
        predictions: List[str], 
        references: Union[List[str], List[List[str]]]
    ) -> Dict[str, float]:  # Always return Dict[str, float]
        # Compute score
        score = ...
        return {"my_metric": score}  # Not just `score`
```

---

## DO's and DON'Ts

### âœ… DO

**Use utilities instead of duplicating:**
```python
from ragicamp.utils.formatting import ContextFormatter
from ragicamp.utils.prompts import PromptBuilder
from ragicamp.utils.paths import ensure_dir, safe_write_json

context_text = ContextFormatter.format_numbered(docs)
prompt = PromptBuilder.create_default().build_prompt(query, context_text)

# Ensure directories exist before writing
ensure_dir("outputs/results.json")
# Or use safe_write_json to do both
safe_write_json(data, "outputs/results.json", indent=2)
```

**Use proper types:**
```python
def answer(self, query: str, **kwargs: Any) -> RAGResponse:
    docs: List[Document] = self.retriever.retrieve(query)
    context = RAGContext(query=query, retrieved_docs=docs)
    return RAGResponse(answer=answer, context=context)
```

**Use dataclasses:**
```python
from ragicamp.retrievers.base import Document

doc = Document(id="1", text="...", metadata={}, score=0.9)
print(doc.text)  # Clean access
```

**Provide save/load for artifacts:**
```python
def save(self, artifact_name: str) -> str:
    manager = get_artifact_manager()
    path = manager.get_agent_path(artifact_name)
    manager.save_json(config, path / "config.json")
    return str(path)

@classmethod
def load(cls, artifact_name: str, model) -> 'MyClass':
    manager = get_artifact_manager()
    config = manager.load_json(...)
    return cls(...)
```

### âŒ DON'T

**Don't duplicate formatting logic:**
```python
# âŒ BAD - This is in utils now
formatted = []
for i, doc in enumerate(docs, 1):
    formatted.append(f"[{i}] {doc.text}")
context_text = "\n\n".join(formatted)

# âœ… GOOD
context_text = ContextFormatter.format_numbered(docs)
```

**Don't use dict for documents:**
```python
# âŒ BAD
def retrieve(self, query: str) -> List[Dict[str, Any]]:
    return [{"text": "...", "score": 0.9}]

# âœ… GOOD
def retrieve(self, query: str) -> List[Document]:
    return [Document(id="1", text="...", metadata={}, score=0.9)]
```

**Don't return inconsistent types:**
```python
# âŒ BAD - Sometimes float, sometimes dict
def compute(self, preds, refs) -> Union[float, Dict[str, float]]:
    if self.name == "f1":
        return 0.85
    else:
        return {"precision": 0.9, "recall": 0.8}

# âœ… GOOD - Always dict
def compute(self, preds, refs) -> Dict[str, float]:
    return {"f1": 0.85}
```

**Don't check for internal methods:**
```python
# âŒ BAD
if hasattr(self.agent, "update_policy"):
    self.agent.update_policy(...)

# âœ… GOOD - Define proper interface
from abc import ABC, abstractmethod

class TrainableAgent(RAGAgent):
    @abstractmethod
    def update_policy(self, ...):
        pass
```

**Don't save models in artifacts:**
```python
# âŒ BAD - Models are huge
artifact = {
    "model": self.model,  # Don't save this!
    "config": {...}
}

# âœ… GOOD - Save config, provide model at load time
artifact = {
    "model_name": self.model.model_name,  # Just the name
    "config": {...}
}

@classmethod
def load(cls, artifact_name, model: LanguageModel):  # Model provided
    ...
```

---

## File Organization

### Adding New Components

**New agent:**
```
src/ragicamp/agents/
â””â”€â”€ my_agent.py          # Inherit from RAGAgent
```

**New retriever:**
```
src/ragicamp/retrievers/
â””â”€â”€ my_retriever.py      # Inherit from Retriever
```

**New metric:**
```
src/ragicamp/metrics/
â””â”€â”€ my_metric.py         # Inherit from Metric
```

**New script:**
```
experiments/scripts/
â””â”€â”€ my_script.py         # Use argparse, add to Makefile
```

**New guide:**
```
docs/guides/
â””â”€â”€ my_guide.md
```

**Important:** Do NOT create standalone `.md` files in the root directory. Always place documentation in the appropriate location:
- New feature guides â†’ `docs/guides/`
- Architecture changes â†’ Update `docs/ARCHITECTURE.md`
- Implementation details â†’ Add to relevant guide
- Quick references â†’ Update `QUICK_REFERENCE.md`
- Temporary summaries â†’ Integrate into existing docs, then delete

### Import Structure

```python
# Standard library
import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

# Third party
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

# Local - absolute imports from src/ragicamp
from ragicamp.agents.base import RAGAgent
from ragicamp.models.base import LanguageModel
from ragicamp.utils.formatting import ContextFormatter
```

---

## Common Tasks

### Adding a New Agent

1. Create `src/ragicamp/agents/my_agent.py`
2. Inherit from `RAGAgent`
3. Implement `answer()` method
4. Use `PromptBuilder` and `ContextFormatter`
5. Return `RAGResponse` with `RAGContext`
6. Optional: Add `save()` and `load()` methods
7. Add tests in `tests/test_agents.py`
8. Update `docs/AGENTS.md` with example

### Adding a New Metric

1. Create `src/ragicamp/metrics/my_metric.py`
2. Inherit from `Metric`
3. Implement `compute()` returning `Dict[str, float]`
4. Add to `__init__.py`
5. Test with example predictions/references
6. Update `docs/guides/METRICS_GUIDE.md`

### Adding a New Retriever

1. Create `src/ragicamp/retrievers/my_retriever.py`
2. Inherit from `Retriever`
3. Implement `retrieve()` returning `List[Document]`
4. Implement `index_documents()`
5. Add `save_index()` and `load_index()` class method
6. Add to `__init__.py`

### Adding Training Support

1. Ensure agent has `update_policy()` or similar
2. Create trainer that calls it with rewards
3. Save trained policy/params with `save()`
4. Document training process
5. Add Makefile shortcut

---

## Testing Approach

### Unit Tests

```python
# tests/test_agents.py
def test_direct_llm_agent():
    mock_model = MockLanguageModel()
    agent = DirectLLMAgent("test", mock_model)
    response = agent.answer("test query")
    assert isinstance(response, RAGResponse)
    assert response.answer is not None
```

### Integration Tests

```python
# Test full pipeline
def test_fixed_rag_pipeline():
    # Create all components
    model = MockModel()
    retriever = MockRetriever()
    agent = FixedRAGAgent("test", model, retriever)
    
    # Test answer generation
    response = agent.answer("query")
    assert len(response.context.retrieved_docs) > 0
```

### Run Tests

```bash
make test           # Run all tests
pytest tests/       # Direct pytest
```

---

## Documentation Standards

### Location Guidelines

**âœ… DO:**
- Place feature docs in `docs/guides/`
- Update existing docs when adding features
- Keep root directory clean (only README, CHANGELOG, etc.)
- Integrate implementation details into relevant guides
- Create examples in `examples/` directory

**âŒ DON'T:**
- Create standalone `.md` files in root (except standard ones)
- Scatter documentation across the repo
- Create "improvements" or "changes" docs (use CHANGELOG or integrate)
- Leave temporary documentation files

### Documentation Structure

```
docs/
â”œâ”€â”€ README.md                    # Project overview
â”œâ”€â”€ ARCHITECTURE.md              # System design
â”œâ”€â”€ GETTING_STARTED.md           # Quick start
â”œâ”€â”€ USAGE.md                     # General usage
â”œâ”€â”€ TROUBLESHOOTING.md           # Common issues
â””â”€â”€ guides/
    â”œâ”€â”€ METRICS.md               # Metrics system
    â”œâ”€â”€ DATASET_MANAGEMENT.md    # Dataset handling
    â”œâ”€â”€ CONFIG_BASED_EVALUATION.md
    â””â”€â”€ ...

examples/                        # Working code examples
â”œâ”€â”€ dataset_download_example.py
â””â”€â”€ ...
```

### Docstrings

```python
def answer(self, query: str, **kwargs: Any) -> RAGResponse:
    """Generate an answer for the given query.
    
    Args:
        query: The input question
        **kwargs: Additional generation parameters (temperature, max_tokens, etc.)
        
    Returns:
        RAGResponse containing the answer, context, and metadata
        
    Example:
        >>> agent = DirectLLMAgent("test", model)
        >>> response = agent.answer("What is Python?")
        >>> print(response.answer)
    """
```

### Comments

```python
# Good comments explain WHY, not WHAT
# âœ… GOOD
# Use normalized format for fair comparison across metrics
text = normalize_answer(text)

# âŒ BAD
# Normalize the text
text = normalize_answer(text)
```

### When Adding New Features

1. **Code first** - Implement with proper docstrings
2. **Update existing guide** - Add to relevant `docs/guides/` file
3. **Add example** - Create working example in `examples/`
4. **Update main docs** - Add links in `docs/README.md` if major feature
5. **Makefile** - Add commands for common operations
6. **No loose docs** - Don't leave `.md` files scattered in root

---

## Artifact Management

### Structure

```
artifacts/
â”œâ”€â”€ retrievers/
â”‚   â””â”€â”€ name_v1/
â”‚       â”œâ”€â”€ index.faiss      # FAISS index
â”‚       â”œâ”€â”€ documents.pkl    # Documents
â”‚       â””â”€â”€ config.json      # Metadata
â””â”€â”€ agents/
    â””â”€â”€ name_v1/
        â””â”€â”€ config.json      # Config (references retriever)
```

### Naming Convention

- Use descriptive names: `wikipedia_nq_v1`, not `index1`
- Include version: `_v1`, `_v2`, etc.
- Be specific: `fixed_rag_nq_v1`, not `agent1`

### What to Save

âœ… **DO save:**
- Configurations (JSON)
- FAISS indices
- Document stores (pickle)
- Trained policies (JSON/pickle)
- Metadata

âŒ **DON'T save:**
- Language models (too large, provide at runtime)
- Entire datasets (reference by name)
- Temporary computations

---

## Performance Considerations

### Indexing

- Use `flat` index for < 1M documents
- Use `ivf` index for larger datasets
- Batch document encoding when possible
- Show progress bars for long operations (tqdm)

### Generation

- Use batching when available: `model.batch_generate()`
- Cache retrieval results if queries repeat
- Use 8-bit quantization for memory: `load_in_8bit=True`

### Memory

- Don't load entire dataset if not needed
- Use generators for large datasets
- Clean up indices after use if RAM-constrained

---

## Error Handling

### Graceful Failures

```python
try:
    retriever = DenseRetriever.load_index("artifact_name")
except FileNotFoundError:
    print("âŒ Artifact not found. Run 'make train-fixed-rag' first.")
    sys.exit(1)
```

### User-Friendly Messages

```python
# âœ… GOOD
print("âœ“ Model loaded successfully")
print(f"  - {len(documents)} documents indexed")
print(f"  - Artifact saved to: {path}")

# âŒ BAD
print("done")
```

---

## Makefile Integration

When adding new scripts, add to Makefile:

```makefile
my-command:
	@echo "ğŸš€ Running my command..."
	uv run python experiments/scripts/my_script.py \
		--arg1 value1 \
		--arg2 value2
```

And update `make help`:

```makefile
help:
	@echo "ğŸ†• MY SECTION"
	@echo "  make my-command           - Description here"
```

---

## Commit Messages

Use conventional commits:

```
feat: Add BanditRAGAgent with epsilon-greedy policy
fix: Correct Document type in retriever return
refactor: Extract formatting logic to utils
docs: Update AGENTS.md with save/load examples
test: Add tests for FixedRAGAgent
chore: Update dependencies in pyproject.toml
```

---

## When Helping Users

1. **Check existing patterns first** - Look at similar components
2. **Use utilities** - Don't duplicate formatting/prompt logic
3. **Follow type system** - Use Document, not dict
4. **Keep it simple** - Don't over-engineer
5. **Add documentation properly** - Update relevant files in `docs/guides/`, don't scatter `.md` files
6. **Test your changes** - At least smoke test
7. **Update Makefile** - If adding scripts
8. **Follow LSP** - New implementations should be substitutable
9. **Clean up** - Remove temporary files, integrate docs into proper locations
10. **Examples over docs** - Prefer working code examples when possible

---

## Quick Reference

```python
# Agent
from ragicamp.agents.fixed_rag import FixedRAGAgent
agent = FixedRAGAgent.load("fixed_rag_v1", model)
response = agent.answer("query")

# Retriever
from ragicamp.retrievers.dense import DenseRetriever
retriever = DenseRetriever.load_index("wikipedia_v1")
docs = retriever.retrieve("query", top_k=5)

# Formatting
from ragicamp.utils.formatting import ContextFormatter
text = ContextFormatter.format_numbered(docs)

# Prompts
from ragicamp.utils.prompts import PromptBuilder
builder = PromptBuilder.create_default()
prompt = builder.build_prompt(query, context)

# Artifacts
from ragicamp.utils.artifacts import get_artifact_manager
manager = get_artifact_manager()
path = manager.get_agent_path("agent_v1")
```

---

## Project Status

**Current State:**
- âœ… Clean type-safe abstractions
- âœ… DirectLLM, FixedRAG, BanditRAG, MDPRAG agents
- âœ… Dense & Sparse retrievers with save/load
- âœ… EM, F1, BERTScore, BLEURT, LLM-judge metrics
- âœ… Training infrastructure for adaptive agents
- âœ… Artifact management system
- âœ… Comprehensive documentation

**Next Priorities:**
- Advanced RL policies (PPO, A2C)
- ResultStore abstraction for better viz
- Multi-objective reward shaping
- More comprehensive tests

---

## Resources

- **Main docs:** `docs/README.md`
- **Architecture:** `docs/ARCHITECTURE.md`
- **Agent guide:** `docs/AGENTS.md`
- **Quick start:** `docs/GETTING_STARTED.md`
- **API patterns:** Look at existing implementations

**Remember:** Keep it simple, type-safe, and reusable! ğŸš€

