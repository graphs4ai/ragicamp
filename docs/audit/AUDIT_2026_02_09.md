# Codebase Audit — 2026-02-09

Comprehensive audit of RAGiCamp codebase. Tracks all findings, fixes, and migration
needs for the ongoing `smart_retrieval_slm` study (~350+ Optuna trials completed).

## Status Key

- `[ ]` Not started
- `[~]` In progress
- `[x]` Fixed
- `[-]` Won't fix (with reason)
- `[M]` Needs migration script for existing data

---

## A. Critical — Experiment Accuracy

### A1. Checkpoint resume crashes with KeyError
- **Status:** `[x]` Fixed
- **File:** `agents/base.py`
- **Bug:** `_save_checkpoint` serializes query text as `"question"` (via `to_dict()`), but
  `_load_checkpoint` reads `r["query"]`. Any crash-recovery attempt raises `KeyError`.
- **Fix:** `_load_checkpoint` now reads `"question"` key (matching `to_dict()`), accepts
  both `"question"` and `"query"` for robustness, and restores steps from checkpoint.
- **Migration:** `[M]` Stale `agent_checkpoint.json` files cleaned up by migration script.

### A2. OpenAI API errors become empty-string predictions
- **Status:** `[x]` Fixed
- **File:** `models/openai.py`
- **Bug:** When OpenAI API call fails, exception is caught and `""` is stored as prediction.
- **Fix:** Error marker `"[ERROR: ExceptionType: message]"` stored instead. Both sync
  and async paths fixed. Also added `or ""` for None content responses.
  Fixed logger import to use `get_logger(__name__)`.
- **Migration:** Existing data uses vLLM for generation (unaffected). LLM judge (gpt-4o-mini)
  may have silent failures in judge scores.

### A3. FaithfulnessMetric / HallucinationMetric violate Metric interface
- **Status:** `[x]` Fixed
- **File:** `metrics/faithfulness.py`, `metrics/hallucination.py`
- **Bug:** `compute()` took singular strings, returned `float`. Incompatible with
  `compute_metrics_batched()`.
- **Fix:** Rewrote both to match base class interface: `compute(predictions, references)`
  returns `dict[str, float]`, populates `_last_per_item`.

### A4. NLI metrics use literal `[SEP]` text
- **Status:** `[x]` Fixed (with A3)
- **File:** `metrics/faithfulness.py`, `metrics/hallucination.py`
- **Bug:** `f"{passage} [SEP] {prediction}"` tokenizes `[SEP]` as literal chars.
- **Fix:** Changed to `{"text": passage, "text_pair": prediction}` dict format.

### A5. `compare()` CLI imports non-existent function
- **Status:** `[x]` Fixed
- **File:** `cli/study.py`
- **Bug:** Imports `compare_experiments` which doesn't exist.
- **Fix:** Changed to import `compare_results` from `analysis.comparison` and
  `ResultsLoader` from `analysis.loader`.

### A6. HNSW indexes use L2 instead of inner product
- **Status:** `[x]` Fixed
- **Files:** `index_builder.py`, `vector_index.py`, `embedding_builder.py`
- **Bug:** `IndexHNSWFlat(dim, 32)` defaults to L2 metric.
- **Fix:** Added `faiss.METRIC_INNER_PRODUCT` to all three HNSW index constructors.
  Also fixed IVFFlat missing metric in `index_builder.py` and `embedding_builder.py`.
- **Migration:** Existing indexes have correct rankings (L2 and IP are monotonically
  related for normalized vectors). New indexes will use IP metric for correct scores.

### A7. IterativeRAG off-by-one: `max_iterations+1` retrieval rounds
- **Status:** `[x]` Fixed
- **File:** `iterative_rag.py`
- **Bug:** `range(self.max_iterations + 1)` performs one extra retrieval.
- **Fix:** Changed to `range(self.max_iterations)` with break at last iteration.
- **Migration:** Pre-fix iterative_rag experiments did N+1 retrievals. Results valid
  but used more retrieval than labeled.

---

## B. High — Bugs Affecting Correctness

### B1. Reranker mutates input document scores in-place
- **Status:** `[x]` Fixed
- **File:** `models/providers/reranker.py`
- **Bug:** `doc.score = float(score)` overwrites original retrieval scores.
- **Fix:** Both `rerank()` and `batch_rerank()` now `copy.copy()` documents before
  assigning scores.

### B2. Reranking steps not traced in IterativeRAG / SelfRAG
- **Status:** `[x]` Fixed
- **Files:** `iterative_rag.py`, `self_rag.py`
- **Bug:** `_apply_reranking` returns only reranked results, no Step object.
- **Fix:** Both methods now return `tuple[list[list], Step]` using StepTimer.
  Ranks changed to 1-indexed (also C10).

### B3. `run_generation()` returns "failed" but state.json says COMPLETE
- **Status:** `[x]` Fixed
- **File:** `execution/runner.py`
- **Bug:** Runner detects error predictions after experiment saves COMPLETE state.
- **Fix:** When errors detected, loads state, calls `set_error()`, saves as FAILED.
- **Migration:** `[M]` Migration script scans for failed-but-complete experiments.

### B4. Duplicate predictions on resume
- **Status:** `[x]` Fixed
- **File:** `execution/phases/generation.py`
- **Bug:** `on_result` appends without checking for duplicate idx.
- **Fix:** Added `completed_idx` dedup guard — skips if `result.query.idx` already seen.
- **Migration:** `[M]` Migration script deduplicates by keeping last per idx.

### B5. BERTScore/BLEURT `_last_per_item` never populated
- **Status:** `[x]` Fixed
- **Files:** `metrics/bertscore.py`, `metrics/bleurt.py`
- **Bug:** Per-item scores stored as `_last_scores`, base class reads `_last_per_item`.
- **Fix:** Added `self._last_per_item = ...` in both compute() methods.

### B6. BERTScore key mismatch breaks multi-reference aggregation
- **Status:** `[x]` Fixed
- **File:** `metrics/bertscore.py`
- **Bug:** Returns `bertscore_f1` but aggregation looks for `bertscore`.
- **Fix:** Added `"bertscore": F1.mean().item()` as primary key matching `metric.name`.
  Keeps `bertscore_precision/recall/f1` for detailed access.
- **Migration:** `[M]` Migration script adds `"bertscore"` key from `"bertscore_f1"`
  in existing predictions and results.

### B7. Init phase writes not atomic
- **Status:** `[x]` Fixed
- **File:** `execution/phases/init_phase.py`
- **Bug:** `questions.json` and `metadata.json` written with direct `json.dump()`.
- **Fix:** Added `_atomic_write()` helper using temp-then-rename pattern.

### B8. `results.json` written non-atomically
- **Status:** `[x]` Fixed
- **File:** `experiment.py`
- **Bug:** Direct `json.dump()` write for results.json.
- **Fix:** Changed to temp-then-rename pattern.

### B9. Pydantic validators depend on field not yet validated
- **Status:** `[-]` Won't fix
- **File:** `config/schemas.py:276-298`
- **Reason:** These validators are for a Pydantic config path that isn't used by the
  YAML-based study runner. Low impact, risky to refactor.

---

## C. Medium — Quality & Performance

### C1. O(n^2) prediction serialization
- **Status:** `[x]` Fixed
- **File:** `execution/phases/generation.py`
- **Fix:** Save every 10 results instead of every single one. Final save still happens
  after all predictions complete.

### C2. Sufficiency check ignores later-iteration docs
- **Status:** `[x]` Fixed
- **File:** `iterative_rag.py`
- **Fix:** Sort docs by score (descending) before slicing top_k*2 for sufficiency check.

### C3. `random.seed()` sets global state
- **Status:** `[x]` Fixed
- **Files:** `spec/builder.py`, `datasets/base.py`
- **Fix:** Use `random.Random(seed)` local instance in both locations.

### C4. Dedup by 200-char prefix can falsely merge documents
- **Status:** `[x]` Fixed
- **Files:** `agents/base.py`, `iterative_rag.py`
- **Fix:** Use `hash(doc.text)` instead of `doc.text[:200]` for dedup key.

### C5. vLLM embedder division by zero on zero-norm embeddings
- **Status:** `[x]` Fixed
- **File:** `models/vllm_embedder.py`
- **Fix:** Clamp norms to `1e-12` before dividing.

### C6. HyDE `batch_transform` ignores `num_hypothetical > 1`
- **Status:** `[-]` Won't fix
- **File:** `rag/query_transform/hyde.py`
- **Reason:** Feature gap, not a bug. `num_hypothetical > 1` is not used in current
  study configs. Would require significant changes to batch embedding logic.

### C7. RetrievalStore `_mem` cache declared but never used
- **Status:** `[x]` Fixed
- **File:** `cache/retrieval_store.py`
- **Fix:** Removed dead `_mem` dict declaration.

### C8. Optuna returns 0.0 for failed experiments
- **Status:** `[x]` Fixed
- **File:** `optimization/optuna_search.py`
- **Fix:** Raise `optuna.TrialPruned()` instead of returning 0.0 for failed trials.

### C9. `cmd_run` missing `--limit` and `--force` CLI args
- **Status:** `[x]` Fixed
- **Files:** `cli/main.py`, `cli/commands.py`
- **Fix:** Added `--limit` and `--force` args to CLI parser, wired through to `run_study()`.

### C10. Reranked rank is 0-indexed vs 1-indexed convention
- **Status:** `[x]` Fixed (with B2)
- **Fix:** Changed to `rank + 1` in both iterative_rag and self_rag `_apply_reranking`.

### C11. `HierarchicalIndex` references non-existent constant
- **Status:** `[x]` Fixed
- **File:** `indexes/hierarchical.py`
- **Fix:** Changed `VLLM_EMBEDDER_GPU_MEMORY_FRACTION` to
  `VLLM_EMBEDDER_GPU_MEMORY_FRACTION_SHARED`.

### C12. `gc.collect()` ordering wrong in unload methods
- **Status:** `[x]` Fixed
- **Files:** `resource_manager.py`, `huggingface.py`, `vllm.py`, `bertscore.py`, `bleurt.py`
- **Fix:** `gc.collect()` now runs before `torch.cuda.empty_cache()` in all five locations.

### C13. Loader skips experiments in `computing_metrics` phase
- **Status:** `[x]` Fixed
- **File:** `analysis/loader.py`
- **Fix:** Removed `"computing_metrics"` from skip list — these experiments have valid
  predictions that should be loadable.

### C14. Study summary only shows baseline counts, not Optuna trials
- **Status:** `[x]` Fixed
- **File:** `cli/study.py`
- **Fix:** Capture return value from `run_optuna_study()`, display trial count,
  pruned count, and best trial value in summary.

---

## Migration Script

**Script:** `scripts/migrate_audit_fixes.py`

Scans and fixes existing experiment data:

1. **Duplicate predictions (B4):** Dedup by idx, keep last occurrence
2. **Failed-but-complete (B3):** Set state to FAILED if predictions have errors
3. **Stale checkpoints (A1):** Remove `agent_checkpoint.json` files
4. **BERTScore key (B6):** Add `"bertscore"` key from `"bertscore_f1"` in predictions
   and results files

Usage:
```bash
# Dry run (report only)
python scripts/migrate_audit_fixes.py outputs/smart_retrieval_slm

# Apply fixes
python scripts/migrate_audit_fixes.py outputs/smart_retrieval_slm --apply
```

---

## Summary

| Severity | Total | Fixed | Won't Fix |
|----------|-------|-------|-----------|
| A (Critical) | 7 | 7 | 0 |
| B (High) | 9 | 8 | 1 |
| C (Medium) | 14 | 12 | 1 |
| **Total** | **30** | **27** | **2** |

Won't fix:
- B9: Pydantic validators (unused code path)
- C6: HyDE multi-hypothetical in batch (feature gap, not used)
